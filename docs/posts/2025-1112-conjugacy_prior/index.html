<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tai-Ning Liao">
<meta name="dcterms.date" content="2025-11-12">

<title>LTN’s Blog - Conjugate Prior</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">LTN’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/TaiNing1237/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Conjugate Prior</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">statistic</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Tai-Ning Liao </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 12, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="什麼是共軛分佈從丟硬幣開始說起" class="level3">
<h3 class="anchored" data-anchor-id="什麼是共軛分佈從丟硬幣開始說起">什麼是共軛分佈？從丟硬幣開始說起</h3>
<p>在貝氏統計 (Bayesian statistics) 中，我們經常需要估計一個未知參數的機率分佈。</p>
<p>讓我們從一個最經典的問題開始：<strong>丟硬幣</strong>。</p>
<p>假設我們有一枚硬幣，它有 <span class="math inline">\(p\)</span> 的機率正面朝上。我們不知道 <span class="math inline">\(p\)</span> 究竟是多少（可能是一枚完美的硬幣，<span class="math inline">\(p=0.5\)</span>，也可能是一枚被動過手腳的硬幣）。</p>
<p>為了估計 <span class="math inline">\(p\)</span>，我們開始做實驗：連續丟這枚硬幣。 假設我們總共丟了 <span class="math inline">\(N\)</span> 次，結果是 <strong><span class="math inline">\(\alpha\)</span> 次正面</strong>和 <strong><span class="math inline">\(\beta\)</span> 次反面</strong>（其中 <span class="math inline">\(\alpha + \beta = N\)</span>）。</p>
<p>現在，問題來了：我們對 <span class="math inline">\(p\)</span> 的<strong>最佳</strong>估計是什麼？</p>
<hr>
</section>
<section id="什麼才算最好的估計" class="level3">
<h3 class="anchored" data-anchor-id="什麼才算最好的估計">什麼才算「最好」的估計？</h3>
<p>要回答這個問題，我們必須先釐清「最好」是什麼意思。在統計學上，至少有兩種截然不同的思考流派。</p>
<p><strong>觀點一：最大似然估計 (Maximum Likelihood Estimation, MLE)</strong></p>
<p>這是「頻率學派」的觀點。他們會問：</p>
<blockquote class="blockquote">
<p>哪一個 <span class="math inline">\(p\)</span> 值，最有可能產生我們<strong>觀測到的實驗結果</strong>（<span class="math inline">\(\alpha\)</span> 次正面，<span class="math inline">\(\beta\)</span> 次反面）？</p>
</blockquote>
<p>我們把觀測到的數據 (Data) 稱為 <span class="math inline">\(D\)</span>。這個觀點的目標是找到一個 <span class="math inline">\(p\)</span>，來最大化「給定 <span class="math inline">\(p\)</span> 之後，觀測到 <span class="math inline">\(D\)</span>」的機率。這個機率在統計上稱為<strong>似然 (Likelihood)</strong>。</p>
<p>寫成數學式，我們要找的就是： <span class="math display">\[
\text{arg}\max_{p} { \mathbb{P}(D \mid p) }
\]</span></p>
<p>（在這個硬幣問題中，<span class="math inline">\(\mathbb{P}(D \mid p) = p^\alpha (1-p)^\beta\)</span>。懂微積分的話，你會發現答案是 <span class="math inline">\(p = \frac{\alpha}{\alpha+\beta}\)</span>，這非常直觀。）</p>
<p><strong>觀點二：貝氏推論 (Bayesian Inference)</strong></p>
<p>這是「貝氏學派」的觀點。他們會說：</p>
<blockquote class="blockquote">
<p>在我們開始丟硬幣<strong>之前</strong>，我們對 <span class="math inline">\(p\)</span> 就已經有一些<strong>初步的信念</strong>（比如，它可能是一枚公平硬幣，所以 <span class="math inline">\(p\)</span> 應該在 0.5 附近）。</p>
<p>然後，我們利用實驗數據 <span class="math inline">\(D\)</span> 來<strong>更新</strong>這個信念。</p>
</blockquote>
<p>在這個框架下，<span class="math inline">\(p\)</span> 不再是一個固定的未知數，而是<strong>一個隨機變數</strong>，它自己也有一個機率分佈。</p>
<p>這裡有三個核心概念：</p>
<ol type="1">
<li><strong>事前分佈 (Prior Distribution) <span class="math inline">\(\mathbb{P}(p)\)</span>：</strong> 在觀測到任何數據<strong>之前</strong>，我們對 <span class="math inline">\(p\)</span> 的信念分佈。</li>
<li><strong>似然 (Likelihood) <span class="math inline">\(\mathbb{P}(D \mid p)\)</span>：</strong> 同 MLE，代表「在某個 <span class="math inline">\(p\)</span> 之下，觀測到數據 <span class="math inline">\(D\)</span>」的機率。</li>
<li><strong>事後分佈 (Posterior Distribution) <span class="math inline">\(\mathbb{P}(p \mid D)\)</span>：</strong> 在觀測到數據 <span class="math inline">\(D\)</span> <strong>之後</strong>，我們對 <span class="math inline">\(p\)</span> <strong>更新後的信念</strong>分佈。</li>
</ol>
<hr>
<p><strong>貝氏定理：更新信念的關鍵</strong></p>
<p>貝氏學派的精髓，就是透過「貝氏定理」來完成這個信念的更新：</p>
<p><span class="math display">\[
\mathbb{P}(p \mid D) = \frac{\mathbb{P}(D \mid p) \cdot \mathbb{P}(p)}{\mathbb{P}(D)}
\]</span></p>
<p>這個公式可能看起來有點嚇人，但它的核心思想很簡單：</p>
<blockquote class="blockquote">
<p><strong>事後分佈 <span class="math inline">\(\propto\)</span> 似然 <span class="math inline">\(\times\)</span> 事前分佈</strong> (Posterior <span class="math inline">\(\propto\)</span> Likelihood <span class="math inline">\(\times\)</span> Prior)</p>
</blockquote>
</section>
<section id="為什麼我們需要共軛分佈" class="level3">
<h3 class="anchored" data-anchor-id="為什麼我們需要共軛分佈">為什麼我們需要「共軛分佈」？</h3>
<p>貝氏推論非常強大，但它有個很現實的數學問題：</p>
<p>要計算事後分佈 <span class="math inline">\(\mathbb{P}(p \mid D)\)</span>，我們需要計算分母 <span class="math inline">\(\mathbb{P}(D)\)</span>。 <span class="math inline">\(\mathbb{P}(D) = \int \mathbb{P}(D \mid p) \mathbb{P}(p) \text{d}p\)</span>。這個積分（或離散情況下的加總）常常<strong>非常複雜</strong>，甚至算不出來。</p>
<p>這就讓早期的統計學家很頭痛。直到他們發現了一個「捷徑」。</p>
<p><strong>想像一下：</strong></p>
<p>如果我們<strong>精心挑選</strong>一個「事前分佈」<span class="math inline">\(\mathbb{P}(p)\)</span>，使得它在乘上「似然函數」<span class="math inline">\(\mathbb{P}(D \mid p)\)</span> 之後，得到的「事後分佈」<span class="math inline">\(\mathbb{P}(p \mid D)\)</span> …</p>
<p>…<strong>竟然跟原來的「事前分佈」長得一模一樣</strong>（只是參數不同了）！</p>
<p>這就太神奇了！這代表：</p>
<ol type="1">
<li><strong>計算超級簡單：</strong> 我們不需要去算那個可怕的積分，只需要套用簡單的「參數更新規則」就好。</li>
<li><strong>迭代更新：</strong> 這次得到的「事後分佈」可以當作下一次實驗的「事前分佈」，形成一個不斷學習的循環。</li>
</ol>
<p>這種「事前分佈」與「事後分佈」<strong>同屬於一個機率分佈家族</strong>的特性，就稱為<strong>共軛 (Conjugacy)</strong>。</p>
<p>我們稱：</p>
<blockquote class="blockquote">
<p><strong>這個「事前分佈」(Prior) 是該「似然函數」(Likelihood) 的「共軛事前分佈」(Conjugate Prior)。</strong></p>
</blockquote>
<p>回到我們最初的硬幣問題。</p>
<p>我們的「似然函數」<span class="math inline">\(\mathbb{P}(D \mid p)\)</span> 是一個<strong>二項分佈 (Binomial Distribution)</strong> 的形式。</p>
<p>那麼，是否存在一個機率分佈，是二項分佈的「共軛事前分佈」呢？</p>
<p>答案是：<strong>有！</strong></p>
<hr>
</section>
<section id="數學推導為什麼-beta-是二項分佈的共軛" class="level3">
<h3 class="anchored" data-anchor-id="數學推導為什麼-beta-是二項分佈的共軛">數學推導：為什麼 Beta 是二項分佈的共軛？</h3>
<p><strong>第 1 步：定義我們的「似然」 (Likelihood)</strong></p>
<p>我們的實驗是丟硬幣，得到了 <span class="math inline">\(\alpha\)</span> 次正面和 <span class="math inline">\(\beta\)</span> 次反面。 給定一個特定的 <span class="math inline">\(p\)</span>（正面機率），發生這件事的機率（即「似然」）是一個<strong>二項分佈 (Binomial Distribution)</strong>：</p>
<p><span class="math display">\[
\mathbb{P}(D \mid p) = \binom{\alpha+\beta}{\alpha} p^\alpha (1-p)^\beta
\]</span></p>
<p>在貝氏推導中，我們只關心這個公式「作為 <span class="math inline">\(p\)</span> 的函數」長什麼樣子。那個 <span class="math inline">\(\binom{\alpha+\beta}{\alpha}\)</span> 是一個常數（跟 <span class="math inline">\(p\)</span> 無關），所以我們可以把它合併到 <span class="math inline">\(\propto\)</span> (正比於) 符號中：</p>
<blockquote class="blockquote">
<p><strong>Likelihood: <span class="math inline">\(\mathbb{P}(D \mid p) \propto p^\alpha (1-p)^\beta\)</span></strong></p>
</blockquote>
<p><strong>第 2 步：介紹主角「Beta 分佈」</strong></p>
<p>現在，我們需要尋找一個「事前分佈」<span class="math inline">\(\mathbb{P}(p)\)</span>，它乘上 <span class="math inline">\(p^\alpha (1-p)^\beta\)</span> 之後，長得會跟自己很像。</p>
<p>仔細看 <span class="math inline">\(p^\alpha (1-p)^\beta\)</span> 這個形式… 如果我們的「事前分佈」也長成「<span class="math inline">\(p\)</span> 的幾次方」乘上「<span class="math inline">\((1-p)\)</span> 的幾次方」，那它們相乘時，不就可以很漂亮地合併指數嗎？</p>
<p>這正是 <strong>Beta 分佈</strong>登場的時刻！</p>
<p>Beta 分佈的機率密度函數 (PDF) 定義在 <span class="math inline">\(p \in [0, 1]\)</span> 之間（這剛好就是機率 <span class="math inline">\(p\)</span> 的合理範圍），它有兩個<strong>超參數 (hyperparameters)</strong>，我們叫它們 <span class="math inline">\(a\)</span> 和 <span class="math inline">\(b\)</span>：</p>
<blockquote class="blockquote">
<p><strong>Prior: <span class="math inline">\(\mathbb{P}(p) = \text{Beta}(p \mid a, b) = \frac{1}{B(a, b)} p^{a-1} (1-p)^{b-1}\)</span></strong></p>
</blockquote>
<ul>
<li><span class="math inline">\(a\)</span> 和 <span class="math inline">\(b\)</span> 必須大於 0。</li>
<li><span class="math inline">\(B(a, b)\)</span> 是一個常數（Beta 函數，<span class="math inline">\(\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\)</span>），用來確保整個分佈的總機率積分為 1。</li>
<li><strong>直觀上</strong>，你可以把 <span class="math inline">\(a\)</span> 想像成我們在實驗前「信念中」的正面次數， <span class="math inline">\(b\)</span> 想像成「信念中」的反面次數。</li>
</ul>
<p>和似然函數一樣，在 <span class="math inline">\(\propto\)</span> 的世界裡，我們可以暫時忽略常數 <span class="math inline">\(B(a, b)\)</span>：</p>
<blockquote class="blockquote">
<p><strong>Prior: <span class="math inline">\(\mathbb{P}(p) \propto p^{a-1} (1-p)^{b-1}\)</span></strong></p>
</blockquote>
<p><strong>第 3 步：施展貝氏魔法！(推導事後分佈)</strong></p>
<p>我們把 Likelihood 和 Prior 相乘：</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}(p \mid D) &amp;\propto \mathbb{P}(D \mid p) \times \mathbb{P}(p)  \\
&amp;\propto [p^\alpha (1-p)^\beta] \times [p^{a-1} (1-p)^{b-1}]  \\
&amp;\propto p^{(\alpha + a) - 1} \times (1-p)^{(\beta + b) - 1}
\end{aligned}
\]</span></p>
<p><strong>第 4 步：揭曉答案</strong></p>
<p>請停下來，仔細看看我們得到的結果：</p>
<p><strong>事後分佈: <span class="math inline">\(\mathbb{P}(p \mid D) \propto p^{(a + \alpha) - 1} \cdot (1-p)^{(b + \beta) - 1}\)</span></strong></p>
<p>再回頭看看我們的事前分佈：</p>
<p><strong>事前分佈: <span class="math inline">\(\mathbb{P}(p) \propto p^{a - 1} \cdot (1-p)^{b - 1}\)</span></strong></p>
<p><strong>它們的數學形式一模一樣！</strong></p>
<p>這證明了事後分佈<strong>仍然是一個 Beta 分佈</strong>。只不過，它的參數從 <span class="math inline">\((a, b)\)</span> 更新成了 <span class="math inline">\((a + \alpha, b + \beta)\)</span>。</p>
<hr>
</section>
<section id="結論" class="level3">
<h3 class="anchored" data-anchor-id="結論">結論：</h3>
<p>這個推導告訴我們一個美妙的更新規則：</p>
<blockquote class="blockquote">
<p>如果你對 <span class="math inline">\(p\)</span> 的「事前信念」是 <span class="math inline">\(\text{Beta}(a, b)\)</span>，</p>
<p>接著你觀測到了 <span class="math inline">\(\alpha\)</span> 次正面和 <span class="math inline">\(\beta\)</span> 次反面，</p>
<p>那麼你對 <span class="math inline">\(p\)</span> 的「事後信念」<strong>就是</strong> <span class="math inline">\(\text{Beta}(a + \alpha, b + \beta)\)</span>。</p>
</blockquote>
<p><strong>這就是共軛的魔力！</strong></p>
<ul>
<li><strong>無需計算複雜積分：</strong> 我們完全繞過了 <span class="math inline">\(\mathbb{P}(D) = \int \mathbb{P}(D \mid p) \mathbb{P}(p) \text{d}p\)</span> 這個大魔王。</li>
<li><strong>直觀的更新：</strong> 我們的信念更新規則變成了<strong>簡單的加法</strong>。
<ul>
<li>新的 “正面” 參數 = 舊的 “正面” 信念 + 觀測到的正面次數</li>
<li>新的 “反面” 參數 = 舊的 “反面” 信念 + 觀測到的反面次數</li>
</ul></li>
<li><strong>迭代學習：</strong> 這個新的 <span class="math inline">\(\text{Beta}(a+\alpha, b+\beta)\)</span> 分佈可以立刻作為下一次實驗的「事前分佈」，讓我們不斷地、無縫地用新數據更新我們的信念。</li>
</ul>
<p>這完美地展示了貝氏統計如何將「先驗知識」與「觀測數據」優雅地結合起來。</p>
</section>
<section id="共軛分佈二估計事件的發生率-gamma-與-poisson-的共舞" class="level2">
<h2 class="anchored" data-anchor-id="共軛分佈二估計事件的發生率-gamma-與-poisson-的共舞">共軛分佈（二）：估計事件的「發生率」— Gamma 與 Poisson 的共舞</h2>
<p>如果我們要估計的不是一個 0 到 1 的機率，而是一個**「率」 (rate)** 呢？例如：</p>
<ul>
<li>一個客服中心，平均<strong>每小時</strong>接到多少通電話？</li>
<li>一個路口，平均<strong>每 10 分鐘</strong>會經過多少輛車？</li>
<li>你的程式碼，平均<strong>每 1000 行</strong>有多少個 bug？</li>
</ul>
<p>這些事件的共同點是，它們在一個連續區間（時間、空間）內發生，我們可以去「計數」(count)，理論上發生的次數可以 是 0, 1, 2, … 一直到無限大。</p>
<p>這類「計數」問題，正是 <strong>Poisson 分佈</strong>的主場。而當我們想對 Poisson 分佈的「率」(<span class="math inline">\(\lambda\)</span>) 進行貝氏推論時，就輪到它的共軛夥伴——<strong>Gamma 分佈</strong>——登場了。</p>
<p><strong>第一步：我們的「似然」— Poisson 分佈</strong></p>
<p>和之前一樣，貝氏推論的第一步是建立我們的<strong>似然 (Likelihood)</strong>。</p>
<p>我們要估計的核心參數是 <span class="math inline">\(\lambda\)</span> (lambda)，代表「單位時間（或單位空間）內的平均事件發生率」。</p>
<p><strong>Poisson 分佈</strong>告訴我們，如果平均率是 <span class="math inline">\(\lambda\)</span>，那麼在一個單位時間內，實際觀測到 <span class="math inline">\(k\)</span> 次事件的機率為：</p>
<p><span class="math display">\[
\mathbb{P}(k \mid \lambda) = \frac{\lambda^k e^{-\lambda}}{k!}
\]</span> 假設我們進行了 <span class="math inline">\(n\)</span> 次觀測（例如，我們觀察了 <span class="math inline">\(n\)</span> 個小時），得到的數據是 <span class="math inline">\(D = \{x_1, x_2, \dots, x_n\}\)</span>，其中 <span class="math inline">\(x_i\)</span> 是第 <span class="math inline">\(i\)</span> 個小時觀測到的事件次數。</p>
<p>「給定 <span class="math inline">\(\lambda\)</span>」，觀測到這整組數據 <span class="math inline">\(D\)</span> 的聯合機率（似然）就是把所有機率乘起來： <span class="math display">\[
\mathbb{P}(D \mid \lambda) = \prod_{i=1}^n \frac{\lambda^{x_i} e^{-\lambda}}{x_i!}
\]</span></p>
<p>在貝氏推論中，我們只關心和 <span class="math inline">\(\lambda\)</span> 相關的項。把上式重新整理：</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}(D \mid \lambda) &amp;\propto \left( \prod_{i=1}^n \lambda^{x_i} \right) \left( \prod_{i=1}^n e^{-\lambda} \right) \\
&amp;\propto \lambda^{\sum x_i} \cdot e^{-n\lambda}
\end{aligned}
\]</span></p>
<p>令 <span class="math inline">\(S = \sum x_i\)</span>（我們觀測到的<strong>總事件數</strong>），我們的似然函數可以簡潔地表示為：</p>
<blockquote class="blockquote">
<p><strong>Likelihood: <span class="math inline">\(\mathbb{P}(D \mid \lambda) \propto \lambda^S e^{-n\lambda}\)</span></strong></p>
</blockquote>
<p><strong>第二步：我們的「事前」— Gamma 分佈</strong></p>
<p>現在，我們需要為 <span class="math inline">\(\lambda\)</span> 選擇一個<strong>事前分佈 (Prior)</strong>。 <span class="math inline">\(\lambda\)</span> 作為一個「率」，它必須大於 0。我們需要一個定義在 <span class="math inline">\((0, \infty)\)</span> 上的機率分佈。</p>
<p>更重要的是，我們希望這個事前分佈 <span class="math inline">\(\mathbb{P}(\lambda)\)</span> 乘上似然 <span class="math inline">\(\lambda^S e^{-n\lambda}\)</span> 之後，能得到一個形式相同的分佈。</p>
<p>看看似然的形式：<span class="math inline">\(\lambda\)</span> 的某次方，再乘以 <span class="math inline">\(e\)</span> 的 <span class="math inline">\(\lambda\)</span> 負次方。 什麼分佈長這樣呢？ 答案就是 <strong>Gamma 分佈</strong>！</p>
<p>Gamma 分佈由兩個超參數 <span class="math inline">\(\alpha\)</span> (shape, 形狀) 和 <span class="math inline">\(\beta\)</span> (rate, 率) 定義：</p>
<blockquote class="blockquote">
<p><strong>Prior: <span class="math inline">\(\mathbb{P}(\lambda) = \text{Gamma}(\lambda \mid \alpha, \beta) \propto \lambda^{\alpha-1} e^{-\beta\lambda}\)</span></strong></p>
</blockquote>
<p><strong>直觀解釋 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\beta\)</span>：</strong> 你可以把 <span class="math inline">\(\alpha\)</span> 想像成你的「事前信念中的<strong>總事件數</strong>」，而 <span class="math inline">\(\beta\)</span> 是「事前信念中的<strong>總觀測單位數</strong>」。 例如，如果你「猜」這個率 <span class="math inline">\(\lambda\)</span> 大約是 5（例如 5 通電話 / 1 小時），你可以設 <span class="math inline">\(\alpha=5, \beta=1\)</span>。</p>
<p><strong>第三步：貝氏魔法！推導「事後分佈」</strong></p>
<p>我們再次使出貝氏定理的武器：</p>
<blockquote class="blockquote">
<p><strong>事後分佈 (Posterior) <span class="math inline">\(\propto\)</span> 似然 (Likelihood) <span class="math inline">\(\times\)</span> 事前分佈 (Prior)</strong></p>
</blockquote>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}(\lambda \mid D) &amp;\propto \mathbb{P}(D \mid \lambda) \times \mathbb{P}(\lambda) \\
&amp;\propto [\lambda^S e^{-n\lambda}] \times [\lambda^{\alpha-1} e^{-\beta\lambda}]  \\
&amp;\propto \lambda^{(S + \alpha) - 1} \cdot e^{-(n + \beta)\lambda}
\end{aligned}
\]</span></p>
<p>它是不是 <span class="math inline">\(\lambda\)</span> 的 (某數 - 1) 次方，再乘以 <span class="math inline">\(e\)</span> 的 (負某數) <span class="math inline">\(\lambda\)</span> 次方？ <strong>這正是一個新的 Gamma 分佈！</strong></p>
<hr>
<section id="結論優雅的更新規則" class="level3">
<h3 class="anchored" data-anchor-id="結論優雅的更新規則">結論：優雅的更新規則</h3>
<p>我們證明了：</p>
<blockquote class="blockquote">
<p>如果你的事前信念是 <span class="math inline">\(\text{Gamma}(\alpha, \beta)\)</span>，</p>
<p>接著你觀測了 <span class="math inline">\(n\)</span> 個單位，總共發生了 <span class="math inline">\(S = \sum x_i\)</span> 次事件，</p>
<p>你的事後信念就會更新為 <span class="math inline">\(\text{Gamma}(\alpha_{\text{new}}, \beta_{\text{new}})\)</span>。</p>
</blockquote>
<p>更新規則超級簡單，只是加法：</p>
<ul>
<li><strong><span class="math inline">\(\alpha_{\text{new}} = \alpha + S\)</span></strong> (舊的事件數 + 新觀測到的事件數)</li>
<li><strong><span class="math inline">\(\beta_{\text{new}} = \beta + n\)</span></strong> (舊的觀測單位 + 新觀測的單位數)</li>
</ul>
<p>舉個例子：</p>
<ol type="1">
<li><strong>事前 (Prior)：</strong> 你是新來的客服經理，你<strong>猜測</strong>客服中心平均每小時接 10 通電話。你對這個猜測不太確定，所以你設定了 <span class="math inline">\(\text{Gamma}(\alpha=10, \beta=1)\)</span> 當作你的事前信念。（信念強度 = 1 小時的觀測）</li>
<li><strong>數據 (Data)：</strong> 你實際去觀測了 5 個小時（<span class="math inline">\(n=5\)</span>），接到的電話數分別是 <span class="math inline">\(\{12, 8, 11, 10, 9\}\)</span>。</li>
<li><strong>似然 (Likelihood)：</strong> 觀測單位 <span class="math inline">\(n=5\)</span>。觀測總數 <span class="math inline">\(S = 12+8+11+10+9 = 50\)</span>。</li>
<li><strong>事後 (Posterior)：</strong></li>
</ol>
<ul>
<li><span class="math inline">\(\alpha_{\text{new}} = \alpha + S = 10 + 50 = 60\)</span></li>
<li><span class="math inline">\(\beta_{\text{new}} = \beta + n = 1 + 5 = 6\)</span></li>
<li>你更新後的信念是 <span class="math inline">\(\text{Gamma}(60, 6)\)</span>。</li>
</ul>
<p>Gamma 分佈的期望值是 <span class="math inline">\(\alpha / \beta\)</span>。</p>
<ul>
<li>你<strong>原先</strong>的期望值是 <span class="math inline">\(\alpha/\beta = 10 / 1 = 10\)</span> 通/小時。</li>
<li>你<strong>更新後</strong>的期望值是 <span class="math inline">\(\alpha_{\text{new}} / \beta_{\text{new}} = 60 / 6 = 10\)</span> 通/小時。</li>
</ul>
<p>在這個例子中，你的平均估計沒有變，因為你的觀測數據 (50/5 = 10) 剛好符合你的猜測！但是，你的<strong>信心</strong>大大增加了（Gamma(60, 6) 是一個比 Gamma(10, 1) 更尖、更窄的分佈），因為你的信念現在是基於 6 個小時的數據，而不僅僅是 1 個小時的猜測。</p>
<div style="text-align: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Gamma_60_6.png" class="img-fluid figure-img" width="400"></p>
<figcaption>圖一： Gamma(10,1) 與 Gamma(60,6) 的比較</figcaption>
</figure>
</div>
</div>
<p>這就是 Gamma-Poisson 共軛家族的美妙之處：它提供了一個直觀且計算簡單的方法，讓我們不斷用新的「計數數據」來更新我們對「事件發生率」的信念。</p>
<hr>
</section>
</section>
<section id="共軛分佈三估計常態分佈的平均值與變異數-normal-inverse-gamma" class="level2">
<h2 class="anchored" data-anchor-id="共軛分佈三估計常態分佈的平均值與變異數-normal-inverse-gamma">共軛分佈（三）：估計常態分佈的平均值與變異數 (Normal-Inverse-Gamma)</h2>
<p>在前面的文章中，我們學會了如何：</p>
<ol type="1">
<li>用 <strong>Beta-Binomial</strong> 估計<strong>機率</strong> (0 到 1 之間)。</li>
<li>用 <strong>Gamma-Poisson</strong> 估計<strong>頻率</strong> (大於 0 的計數)。</li>
</ol>
<p>現在，我們要來處理統計學中最常見的問題：估計一個<strong>平均值 (mean)</strong>。</p>
<ul>
<li>一群學生的<strong>平均身高</strong>是多少？</li>
<li>一批產品的<strong>平均壽命</strong>是多久？</li>
<li>某支股票的<strong>平均日報酬率</strong>是多少？</li>
</ul>
<p>這些測量值——身高、時間、報酬率——都是<strong>連續變數</strong>。而說到連續變數，統計學的王者，<strong>常態分佈 (Normal Distribution)</strong> <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span>，就該登場了。</p>
<p><strong>挑戰：兩個未知數</strong></p>
<p>。但這裡有一個挑戰。不像 Binomial (只有 <span class="math inline">\(p\)</span>) 或 Poisson (只有 <span class="math inline">\(\lambda\)</span>) 只有一個參數，常態分佈 <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span> 描述了兩個我們都不知道的參數：</p>
<ol type="1">
<li>平均值 <span class="math inline">\(\mu\)</span>：我們主要想估計的目標。</li>
<li>變異數 <span class="math inline">\(\sigma^2\)</span>：數據的波動程度，我們通常也不知道。</li>
</ol>
<p>我們希望能建立一個貝氏模型，讓我們對 <span class="math inline">\(\mu\)</span> 和 <span class="math inline">\(\sigma^2\)</span> 的「信念」在看到新數據 <span class="math inline">\(D = \{x_1, ..., x_n\}\)</span> 後，能自動更新。</p>
<p><strong>暖身: 從最大概似估計 (MLE) 看起</strong></p>
<p>在進入貝氏模型之前，讓我們先用傳統的「最大概似估計 (Maximum Likelihood Estimation, MLE)」來暖身。這能幫助我們理解 Likelihood Function 的長相。</p>
<p>假設 <span class="math inline">\(D = \{x_1, ..., x_n\}\)</span> 來自 <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span>，Likelihood Function 為： <span class="math display">\[
\begin{align}
\mathbb{P}(D \mid \mu, \sigma^2) &amp;= \prod_{i=1}^n \left( \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}} \right)  \\
&amp;\propto \frac{1}{\sigma^n} e^{-\frac{\sum_{i=1}^n (x_i-\mu)^2}{2\sigma^2}}
\end{align}
\]</span> 為了方便計算，我們取其負對數 (Negative Log-Likelihood): <span class="math display">\[
\begin{align}
- \ln(\mathbb{P}(D \mid \mu, \sigma^2))
&amp;= \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2 + n \ln(\sigma) + \text{constant}
\end{align}
\]</span> 這邊constant的意思是跟 <span class="math inline">\(\mu, \sigma\)</span> 無關。</p>
<p>用 maximal likelihood 的觀點，這個 <span class="math inline">\(\mu, \sigma\)</span> 的極值發生在哪裡? 我們記等式左邊這個 log-likelihood 為 <span class="math inline">\(f = - \ln(\mathbb{P}(D \mid \mu, \sigma^2))\)</span>。令其偏微分各自為0。 <span class="math display">\[
\begin{align}
\frac{\partial f}{\partial \mu} &amp;=  \frac{1}{\sigma^2}\sum_{i=1}^n (\mu - x_i) = 0  \\
\frac{\partial f}{\partial \sigma} &amp;=  -\frac{1}{\sigma^3}\sum_{i=1}^n (x_i-\mu)^2 + \frac{n}{\sigma} = 0  
\end{align}
\]</span> 解出來為 <span class="math display">\[
\begin{align}
\hat{\mu} &amp;= \frac{1}{n}\sum_{i=1}^n x_i = \overline{x} \\
\hat{\sigma}^2 &amp;= \frac{1}{n}\sum_{i=1}^n (x_i-\overline{x})^2  
\end{align}
\]</span> (可以驗證一下，二階導數矩陣正定，所以 <span class="math inline">\(f\)</span> 是取到最小值)</p>
<blockquote class="blockquote">
<p>題外話：為什麼是除以 <span class="math inline">\(n\)</span> 而不是 <span class="math inline">\(n-1\)</span>？ 一般說的樣本標準差是「除以 <span class="math inline">\(n-1\)</span>」這裡推導出來怎麼是除以 <span class="math inline">\(n\)</span>? 難道推導錯誤了嗎? 沒有，若要最大化似然，確實應該要除以 <span class="math inline">\(n\)</span>，而一般常用的除以 <span class="math inline">\(n-1\)</span>，那是我們希望這個 <span class="math inline">\(\sigma^2\)</span> 是個無偏估計 (unbiased-estimation)。也就是說我們希望 <span class="math display">\[
\mathbb{E}[\tilde{\sigma}^2] = \sigma^2
\]</span> 把期望值的定義寫出來，也就是 <span class="math display">\[
\int \tilde{\sigma}^2 \mathbb{P}(D \mid \mu, \sigma^2)  dx_1...dx_n = \sigma^2
\]</span> 也就是一般的樣本標準差 <span class="math inline">\(\tilde{\sigma}^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i-\overline{x})^2\)</span>，是在所有無偏估計中，擁有最小variance的 (Why?)</p>
</blockquote>
<section id="貝式解法-normal-inverse-gamma-nig-模型" class="level3">
<h3 class="anchored" data-anchor-id="貝式解法-normal-inverse-gamma-nig-模型">貝式解法: Normal-Inverse-Gamma (NIG) 模型</h3>
<p>好了，拉回我們的正題：貝氏更新。我們要找一個共軛事前分佈 <span class="math inline">\(\mathbb{P}(\mu, \sigma^2)\)</span>，使得它乘上 Likelihood <span class="math inline">\(\mathbb{P}(D \mid \mu, \sigma^2)\)</span> 之後，得到的 Posterior <span class="math inline">\(\mathbb{P}(\mu, \sigma^2 \mid D)\)</span> 仍然和 Prior 具有相同的形式。這個神奇的 Prior 就是 Normal-Inverse-Gamma (NIG) 分佈。它由四個超參數 <span class="math inline">\((\mu_0, \kappa_0, \alpha_0, \beta_0)\)</span> 定義，其結構如下：</p>
<ol type="1">
<li>對 <span class="math inline">\(\sigma^2\)</span> 的信念：我們假設 <span class="math inline">\(\sigma^2\)</span> 服從一個 Inverse-Gamma 分佈: <span class="math display">\[
\mathbb{P}(\sigma^2) \propto (\sigma^2)^{-\alpha_0-1}e^{-\frac{\beta_0}{\sigma^2}}
\]</span></li>
<li>對 <span class="math inline">\(\mu\)</span> 的信念 (給定 <span class="math inline">\(\sigma^2\)</span>)：我們假設 <span class="math inline">\(\mu\)</span> 服從一個常態分佈，但這個常態分佈的變異數取決於 <span class="math inline">\(\sigma^2\)</span>： <span class="math display">\[
\mathbb{P}(\mu \mid \sigma^2) \sim \mathcal{N}(\mu_0, \sigma^2/\kappa_0)
\propto \frac{1}{\sigma /\sqrt{\kappa_0} } e^{-\frac{(\mu-\mu_0)^2}{2\sigma^2 / \kappa_0}}
\]</span></li>
</ol>
<p>這樣就描述了我們對 <span class="math inline">\(\mathbb{P}(\mu, \sigma^2)\)</span> 的 joint distribution， <span class="math display">\[
\begin{align}
\mathbb{P}(\mu, \sigma^2)
&amp;= \mathbb{P}(\sigma^2) \cdot \mathbb{P}(\mu \mid \sigma^2) \\
&amp;\propto  (\sigma^2)^{-\alpha_0-1}e^{-\frac{\beta_0}{\sigma^2}} \frac{1}{\sigma /\sqrt{\kappa_0} } e^{-\frac{(\mu-\mu_0)^2}{2\sigma^2 / \kappa_0}}
\end{align}
\]</span> 一樣是取negative log-likelihood看比較清楚: <span class="math display">\[
\begin{align}
-\ln(\mathbb{P}(\mu, \sigma^2))
&amp;=  \frac{\kappa_0(\mu-\mu_0)^2}{2\sigma^2} + \frac{\beta_0}{\sigma^2} + (2\alpha_0 + 3) \ln(\sigma) + \text{constant}
\end{align}
\]</span></p>
<p>精彩的來了，我們要算經過一筆資料 <span class="math inline">\(D\)</span> 更新後的事後分布是否也是一樣的形式呢? 而四個超參數 <span class="math inline">\((\alpha_0, \beta_0, \mu_0, \kappa_0)\)</span> 會如何變動呢?</p>
<p><span class="math display">\[
\begin{align}
-\ln(\mathbb{P}(\mu, \sigma^2 \mid D))
&amp;= -\ln(\mathbb{P}(D \mid \mu, \sigma^2)) -\ln(\mathbb{P}(\mu, \sigma^2)) + \text{constant} \\
&amp;=  \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2 + n \ln(\sigma) \\
&amp;\qquad + \frac{\kappa_0(\mu-\mu_0)^2}{2\sigma^2} + \frac{\beta_0}{\sigma^2} + (2\alpha_0 + 3) \ln(\sigma) + \text{constant} \\
&amp;= \left( \frac{n+\kappa_0}{2\sigma^2} \right)\mu^2
+ \left( -\frac{(\sum_{i=1}^n x_i) + \kappa_0 \mu_0}{\sigma^2} \right)\mu \\
&amp;\qquad + \frac{(\sum_{i=1}^n x_i^2) + \kappa_0 \mu_0^2 + 2\beta_0}{2\sigma^2}
+ (n+2\alpha_0+3)\ln(\sigma) + \text{constant}
\end{align}
\]</span> 比較一下係數，可以依序解出 <span class="math display">\[
\begin{align}
\kappa_n &amp;= n + \kappa_0  \\
\mu_n &amp;= \frac{(\sum_{i=1}^n x_i) + \kappa_0 \mu_0}{n+\kappa_0}  \\
\alpha_n &amp;= \alpha_0 + \frac{n}{2}  \\
\beta_n &amp;= -\left( \frac{n+\kappa_0}{2} \right)\mu_n^2
+ \frac{(\sum_{i=1}^n x_i^2) + \kappa_0 \mu_0^2 + 2\beta_0}{2} \quad\text{(by completing the square)} \\
&amp;= - \frac{(\sum_{i=1}^n x_i + \kappa_0 \mu_0)^2}{2(n+\kappa_0)} + \frac{(\sum_{i=1}^n x_i^2) + \kappa_0 \mu_0^2 + 2\beta_0}{2}
\end{align}
\]</span></p>
</section>
<section id="總結更新規則與直觀解釋" class="level3">
<h3 class="anchored" data-anchor-id="總結更新規則與直觀解釋">總結：更新規則與直觀解釋</h3>
<p>我們成功了！Posterior <span class="math inline">\(\mathbb{P}(\mu, \sigma^2 \mid D)\)</span> 確實保持了 Normal-Inverse-Gamma (NIG) 的形式，其超參數 <span class="math inline">\((\mu_n, \kappa_n, \alpha_n, \beta_n)\)</span> 透過以下規則更新：</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">超參數</th>
<th style="text-align: left;">事前 (Prior)</th>
<th style="text-align: left;">事後 (Posterior) 更新規則</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\kappa_n\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\kappa_0\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\kappa_n = \kappa_0 + n\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\mu_n\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mu_0\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mu_n = \frac{\kappa_0 \mu_0 + n\overline{x}}{\kappa_0 + n}\)</span> (其中 <span class="math inline">\(\overline{x} = \frac{1}{n}\sum x_i\)</span>)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\alpha_n\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\alpha_0\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\alpha_n = \alpha_0 + \frac{n}{2}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\beta_n\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\beta_0\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\beta_n = \beta_0 + \frac{1}{2} \sum_{i=1}^n (x_i - \overline{x})^2 + \frac{n\kappa_0}{2(n+\kappa_0)}(\overline{x} - \mu_0)^2\)</span></td>
</tr>
</tbody>
</table>
<p><strong>這代表什麼？</strong>，讓我們來直觀地解讀這些更新規則：</p>
<ul>
<li><p><strong><span class="math inline">\(\kappa_n = \kappa_0 + n\)</span></strong>： 我們對 <span class="math inline">\(\mu\)</span> 的信心（<span class="math inline">\(\kappa\)</span>）等於「先前的信心」加上「新數據的點數」。這非常合理。</p></li>
<li><p><strong><span class="math inline">\(\mu_n = \frac{\kappa_0 \mu_0 + n\overline{x}}{\kappa_0 + n}\)</span></strong>： <strong>這是最漂亮的一條規則！</strong> 我們更新後的平均值 <span class="math inline">\(\mu_n\)</span>，是「先前平均值 <span class="math inline">\(\mu_0\)</span>」和「數據平均值 <span class="math inline">\(\overline{x}\)</span>」的<strong>加權平均</strong>。 權重分別是 <span class="math inline">\(\kappa_0\)</span>（先前的信心）和 <span class="math inline">\(n\)</span>（數據的數量）。如果 <span class="math inline">\(\kappa_0\)</span> 很小（不確定的 prior），<span class="math inline">\(\mu_n\)</span> 就會很接近 <span class="math inline">\(\overline{x}\)</span>。如果 <span class="math inline">\(\kappa_0\)</span> 很大（強烈的 prior），<span class="math inline">\(\mu_n\)</span> 就會很接近 <span class="math inline">\(\mu_0\)</span>。</p></li>
<li><p><strong><span class="math inline">\(\alpha_n = \alpha_0 + \frac{n}{2}\)</span></strong>： 我們對 <span class="math inline">\(\sigma^2\)</span> 的信念（<span class="math inline">\(\alpha\)</span>）也隨著數據點 <span class="math inline">\(n\)</span> 而增加。</p></li>
<li><p><strong><span class="math inline">\(\beta_n = \beta_0 + \dots\)</span></strong>： 我們對 <span class="math inline">\(\sigma^2\)</span> 的信念（<span class="math inline">\(\beta\)</span>）更新，等於「先前的 <span class="math inline">\(\beta_0\)</span>」加上「數據內部的變異 <span class="math inline">\(\sum (x_i - \overline{x})^2\)</span>」再加上「數據平均值與先前平均值之間的差異 <span class="math inline">\((\overline{x} - \mu_0)^2\)</span>」。這也完全符合直覺。</p></li>
</ul>
<p>至此，我們完成推導了常態分佈的共軛模型！它讓我們能夠在 <span class="math inline">\(\mu\)</span> 和 <span class="math inline">\(\sigma^2\)</span> 都未知的情況下，優雅地更新我們的信念。</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="TaiNing1237/quarto-blog" data-repo-id="R_kgDOQX3NUw" data-category="General" data-category-id="DIC_kwDOQX3NU84Cx6xN" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="light">
</div> <!-- /content -->




</body></html>