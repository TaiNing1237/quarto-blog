[
  {
    "objectID": "posts/poker/lesson_3_count.html",
    "href": "posts/poker/lesson_3_count.html",
    "title": "Poker Lesson 3 - hands 組合數",
    "section": "",
    "text": "今天我們來算一下 同花順、四條、葫蘆、同花、順子、三條、兩對、一對、散牌 在撲克中出現的機率。\n\n\n\nPoker Counts Takeaway\n\n\n同花順 (straight flush) 的方法數: 在一副標準的 52 張撲克牌中，有 4 種花色，每種花色有 13 張牌。同花順是指五張連續數字且同花色的牌。首先，我們可以有 10 種不同的同花順組合（A-5, 2-6, …, 10-A）。 每種花色有 1 種組合，所以同花順的組合數量為： \\[\n4 \\times 10 = 40\n\\]\n四條 (four of a kind) 的方法數: 要組成四條，我們需要從 13 個數字中選擇 1 個數字來組成四張牌，然後從剩下的 12 個數字中選擇 1 個數字來組成單張牌。每個數字的四張牌有 \\(\\binom{4}{4} = 1\\) 種組合，而單張牌有 4 種組合。所以四條的組合數量為： \\[\n13 \\times 1 \\times 12 \\times 4 = 624\n\\]\n葫蘆 (full house) 的方法數: 葫蘆是指三張同數字的牌加上兩張同數字的牌。首先，我們從 13 個數字中選擇一個數字來組成三張牌，然後從剩下的 12 個數字中選擇一個數字來組成兩張牌。每個數字的三張牌有 \\(\\binom{4}{3} = 4\\) 種組合，而兩張牌有 \\(\\binom{4}{2} = 6\\) 種組合。所以葫蘆的組合數量為： \\[\n13 \\times 4 \\times 12 \\times 6 = 3744\n\\]\n同花 (flush) 的方法數: 要組成同花，我們需要從同一花色中選出 5 張牌。但要扣掉同花順。所以，同花的組合數量為： \\[\n4 \\times \\binom{13}{5} - 40 = 4 \\times 1287 - 40 = 5108\n\\]\n順子 (straight) 的方法數: 順子是指五張連續數字的牌，不考慮花色。在 13 個數字中（A, 2, 3, …, 10, J, Q, K），我們可以有 10 種不同的順子組合（A-5, 2-6, …, 10-A）。每張牌可以有 4 種花色，一樣要扣掉同花順。所以順子的組合數量為： \\[\n10 \\times 4^5 - 40 = 10 \\times 1024 - 40 = 10200\n\\]\n三條 (three of a kind) 的方法數: 要組成三條，我們需要從 13 個數字中選擇 1 個數字來組成三張牌，然後從剩下的 12 個數字中選擇 2 個數字來組成單張牌。每個數字的三張牌有 \\(\\binom{4}{3} = 4\\) 種組合，而單張牌有 4 種組合。所以三條的組合數量為： \\[\n13 \\times 4 \\times \\binom{12}{2} \\times 4^2 = 13 \\times 4 \\times 66 \\times 16 = 54912\n\\]\n兩對 (two pairs) 的方法數: 要組成兩對，我們需要從 13 個數字中選擇 2 個數字來組成兩對，然後從剩下的 11 個數字中選擇 1 個數字來組成單張牌。每個數字的兩張牌有 \\(\\binom{4}{2} = 6\\) 種組合，而單張牌有 4 種組合。所以兩對的組合數量為： \\[\n\\binom{13}{2} \\times 6^2 \\times 11 \\times 4 = 78 \\times 36 \\times 44 = 123552\n\\]\n一對 (one pair) 的方法數: 要組成一對，我們需要從 13 個數字中選擇 1 個數字來組成一對，然後從剩下的 12 個數字中選擇 3 個數字來組成單張牌。每個數字的兩張牌有 \\(\\binom{4}{2} = 6\\) 種組合，而單張牌有 4 種組合。所以一對的組合數量為： \\[\n13 \\times 6 \\times \\binom{12}{3} \\times 4^3 = 13 \\times 6 \\times 220 \\times 64 = 1098240\n\\]\n散牌 (high card) 的方法數: 散牌是指沒有任何組合的牌。我們需要從 13 個數字中選擇 5 個數字來組成散牌，然後從每個數字中選擇 1 張牌。每張牌有 4 種組合。但是要扣掉可能是順子和同花的組合數量。所以散牌的組合數量為： \\[\n\\binom{13}{5} \\times 4^5 - 10200 - 5108 - 40 = 1287 \\times 1024 - 10200 - 5108 - 40 = 1302540\n\\]\n檢查一下所有手牌組合數量的總和是否等於 52 張牌中選 5 張的總組合數： \\[\n\\binom{52}{5} = 2598960\n\\]\n而各手牌組合數量總和為： \\[\n40 + 624 + 3744 + 5108 + 10200 + 54912 + 123552 + 1098240 + 1302540 = 2598960\n\\] 符合預期!\n直方圖:\n\n\nPlotly = require(\"https://cdn.plot.ly/plotly-2.27.0.min.js\")\n\n// 定義手牌類型及其組合數量\nhandTypes = [\n  { type: \"Full House\", count: 3744 },\n  { type: \"Flush\", count: 5148 },\n  { type: \"Straight\", count: 10240 },\n  { type: \"Three of a Kind\", count: 54912 },\n  { type: \"Two Pairs\", count: 123552 },\n  { type: \"One Pair\", count: 1098240 },\n  { type: \"High Card\", count: 1302540 }\n];\n\n// 繪製直方圖\n{\n  const div = DOM.element('div');\n  const totalCombos = 2598960;\n  Plotly.newPlot(div, [{\n    type: 'bar',\n    x: handTypes.map(d =&gt; d.type),\n    y: handTypes.map(d =&gt; (d.count / totalCombos * 100)),\n    marker: { color: '#69b3a2' },\n    text: handTypes.map(d =&gt; `${(d.count / totalCombos * 100).toFixed(3)}%`),\n    textposition: 'auto'\n  }], {\n    yaxis: {\n      title: 'Probability (%)',\n      type: 'log',\n      gridcolor: '#e5e5e5'\n    },\n    xaxis: {\n      title: 'Hand Type'\n    },\n    margin: { t: 30, b: 80, l: 80, r: 30 },\n    plot_bgcolor: '#ffffff',\n    paper_bgcolor: '#ffffff'\n  }, {\n    staticPlot: true\n  });\n  return div;\n}\n\n\n\n\n\n\n(a) ?(caption)\n\n\n\n\n\n\n\n\n\n(b) ?(caption)\n\n\n\n\n\n\n\n\n\n(c) ?(caption)\n\n\n\nFigure 1: 撲克手牌組合數量直方圖\n\n\n\nTexas Hold’em\n但在德州撲克中，假設跟到第五張，我們是從七張牌中選五張來組成最好的手牌。所以我們可以計算從七張牌中組成各種手牌的組合數量。\n首先，我們必須確立樣本空間的總數。在德州撲克中，玩家最終會有 7 張牌（2 張手牌 + 5 張公牌），我們需要從這 52 張牌中選出 7 張，總組合數為：\n\\[\n\\binom{52}{7} = 133,784,560\n\\] 這個數字比 5 張牌的 260 萬大了約 50 倍，計算過程中會出現許多「重疊」的情況（例如手上有 6 張同花色的牌，或者同時擁有兩對和三條），我們必須確保只計算該手牌能組成的最佳牌型。\n同花順 (Straight Flush) 的方法數: 這包含了皇家同花順。在 7 張牌中要湊出同花順，意味著這 7 張牌中至少包含了 5 張連續且同花色的牌。 這需要考慮 7 張牌剛好構成 5 張、6 張或 7 張連續同花的情況。經過排容原理計算後，共有： \\[41,584 \\]機率約為 \\(0.03\\%\\)。\n四條 (Four of a Kind) 的方法數: 這比較好算。我們需要從 13 個數字選 1 個做為四條（\\(\\binom{13}{1}\\)），這四張牌全拿（\\(\\binom{4}{4}\\)）。剩下的 3 張牌可以從剩餘的 48 張牌中任意選擇（\\(\\binom{48}{3}\\)）。 注意：即便剩下的 3 張牌湊成一對甚至三條，因為四條是大牌，最佳手牌依然是四條，所以不用扣除。 \\[\n13 \\times 1 \\times \\binom{48}{3} = 13 \\times 17,296 = 224,848\n\\]\n葫蘆 (Full House) 的方法數: 這是 7 張牌中最容易算錯的部分。因為在 7 張牌中，你可能拿到「三條+三條」（取大的三條配小的一對算葫蘆）、「三條+兩對」、「三條+一對」。 經過詳細分類計算（此處省略繁瑣的排容過程），總共有： \\[\n3,473,184\n\\]\n同花 (Flush) 的方法數: 我們需要 7 張牌中至少有 5 張同一花色，且不包含同花順。 算法是：選 1 種花色，然後從該花色 13 張選 7 張、選 6 張配 1 張雜牌、或選 5 張配 2 張雜牌。最後減去同花順的數量。 組合數為：\n\\[\n4 \\times [\\binom{13}{7} + \\binom{13}{6}\\binom{39}{1} + \\binom{13}{5}\\binom{39}{2}] - 41,584 = 4,047,644\n\\]\n在德州撲克中，葫蘆機率 依然小於 同花機率，並沒有反轉!\nP.S. 在短牌撲克（Short Deck Poker）中，由於去掉了 2-5，共 36 張牌，葫蘆反而變得比同花更容易出現(反轉!)，這是因為牌數減少後，同花的組合數下降幅度較大所致。所以在短牌規則中通常是「同花 &gt; 葫蘆」。\n順子 (Straight) 的方法數: 類似同花，需找出 5、6 或 7 張連續數字，扣除同花順。 總數為： \\[6,180,020 \\]\n三條 (Three of a Kind) 的方法數: 指 7 張牌中包含三張同號，但剩下的 4 張牌不能形成另外的三條或一對（否則會變成葫蘆或四條），也不能形成順子或同花。 總數為： \\[\n6,461,620\n\\]\n兩對 (Two Pairs) 的方法數:\n這在德州撲克中非常常見。你可以拿到 3 對（取最大的兩對），或者 2 對配 3 張散牌。 總數為： \\[31,433,400 \\]\n一對 (One Pair) 的方法數: 總數為： \\[58,627,800 \\]\n這意味著在 Showdown 階段，你有接近 44% 的機率至少持有一對。\n散牌 (High Card) 的方法數: 什麼都沒湊成，連一對都沒有。 總數為： \\[\n23,294,460\n\\]\n有趣的是，在 7 張牌選 5 的規則下，拿散牌的機率比拿一對還要低（散牌約 17.4%，一對約 43.8%）。這解釋了為什麼在德州撲克中，光靠一張「A High」通常很難贏到底。\n\n德州撲克手牌分佈直方圖:\n我們將上述 7 選 5 的數據更新到圖表中，你會發現分佈趨勢與標準 5 張牌撲克有所不同，特別是葫蘆與同花的順序對調，以及散牌數量的驟降。\n\n\ntexasHandTypes = [\n  // { type: \"Straight Flush\",  count: 41_584 },\n  // { type: \"Four of a Kind\",  count: 224_848 },\n  { type: \"Full House\",      count: 3_473_184 },\n  { type: \"Flush\",           count: 4_047_644 },\n  { type: \"Straight\",        count: 6_180_020 },\n  { type: \"Three of a Kind\", count: 6_461_620 },\n  { type: \"Two Pairs\",       count: 31_433_400 },\n  { type: \"One Pair\",        count: 58_627_800 },\n  { type: \"High Card\",       count: 23_294_460 }\n];\n\n// 繪製直方圖\n{\n  const div = DOM.element('div');\n  const totalCombos = 133_784_560;\n  Plotly.newPlot(div, [{\n    type: 'bar',\n    x: texasHandTypes.map(d =&gt; d.type),\n    y: texasHandTypes.map(d =&gt; (d.count / totalCombos * 100)),\n    marker: { color: 'steelblue' },\n    text: texasHandTypes.map(d =&gt; `${(d.count / totalCombos * 100).toFixed(2)}%`),\n    textposition: 'auto'\n  }], {\n    yaxis: {\n      title: 'Probability (%)',\n      type: 'log',\n      gridcolor: '#e5e5e5'\n    },\n    xaxis: {\n      title: 'Hand Type'\n    },\n    margin: { t: 30, b: 80, l: 80, r: 30 },\n    plot_bgcolor: '#ffffff',\n    paper_bgcolor: '#ffffff'\n  }, {\n    staticPlot: true\n  });\n  return div;\n}\n\n\n\n\n\n\n(a) ?(caption)\n\n\n\n\n\n\n\n\n\n(b) ?(caption)\n\n\n\nFigure 2: 德州撲克 (7選5) 手牌組合數量直方圖\n\n\n\n\n\n總結與洞察\n\n\n\nCard Type\n5-Card Prob\n7-Card Prob\n\n\n\n\nFour of a Kind\n0.0240 %\n0.168 %\n\n\nFull House\n0.1441 %\n2.60 %\n\n\nFlush\n0.1965 %\n3.03 %\n\n\nStraight\n0.3925 %\n4.62 %\n\n\nThree of a Kind\n2.1128 %\n4.83 %\n\n\nTwo Pairs\n4.7539 %\n23.50 %\n\n\nOne Pair\n42.2569 %\n43.82 %\n\n\nHigh Card\n50.1177 %\n17.41 %\n\n\n\n對比 5 張牌與 7 張牌的機率，我們可以得到幾個對實戰有幫助的結論：\n\n散牌大幅減少：在 5 張牌中，散牌佔了 50%；但在 7 張牌中，散牌只佔 17%。這意味著在河牌圈（River），對手大概率是有牌的（至少有一對）。\n\n高頻手牌：一對和兩對在 7 張牌中非常常見，分別佔了約 44% 和 23.5%。這表示在實戰中，持有至少一對的機率非常高，這也影響了下注策略和讀牌判斷。\n葫蘆與同花機率大幅上升：從傳統撲克轉到德州撲克，最大的直覺衝擊在於牌型差距的縮小。在傳統撲克中，拿三條比拿葫蘆容易 15 倍，但在德州撲克中，這個差距縮小到了不到 2 倍。這意味著：在德州撲克中，當你拿到三條或順子時，撞上對手拿到同花或葫蘆的機率，比你想像中高得多。我們可以把大牌分成兩組來記：\n\n常見強牌組 (約 5%)：三條 與 順子。這兩者出現頻率很高，是底池主要競爭者。\n稀有魔王組 (約 3%)：同花 與 葫蘆。雖然只比上面少一點，但在機率上這意味著它們通常能贏過上面的牌。"
  },
  {
    "objectID": "posts/poker/lesson_5_hyperbolic_payoff.html",
    "href": "posts/poker/lesson_5_hyperbolic_payoff.html",
    "title": "Poker Lesson 5 - 雙曲面 Payoff",
    "section": "",
    "text": "來延續上一篇的設定，假設 A 跟 B 玩 Seven-card stud， \\(40-80\\) 元有限注，池底有 \\(P = 100\\) 元。\n現在來到最後一輪 River 下注(已經發了最後一張牌)，前六張都是開的，最後一張只有自己知道。\n\nA 的手牌是: A♠ A♣ K♣ 9♦ 7♦ 6♥ [?]\nB 的手牌是: K♠ 9♠ 8♠ 7♠ 4♦ 2♦ [?]\n\nPlayer A 先手應該要 pass。\n輪到 Player B:\n\n若有 Nuts (湊成黑桃同花) 就一下會下注；\n若是 Air 則要用 \\(\\beta\\) 的機率決定是否詐唬 (下注 \\(B = 80\\) 元)。\n\n上一篇用了完整的機率算法，解出 \\(\\displaystyle \\beta = \\frac{rB}{(1-r)(B+P)}\\) 是 Player B 的最佳防禦策略，其中 \\(r=1/5\\) 是 Player B 拿到 Nuts 的機率 (從 Player A 的視角來看)。\n而當 Player B bet時，Player A 的最佳回應策略是以 \\(\\displaystyle \\alpha = \\frac{P}{B+P}\\) 的機率去跟注。\n今天來用另一種方式，本質上是一樣，但是可能省去了複雜的期望值計算，並且提供了更直觀的心智模型。\n\n翹翹板 Payoff\n我們說要考慮 Player A 對於 Player B 的下注時的回應，要以 \\(\\alpha\\) 的機率來決定是否跟注 (call \\(C = 80\\) 元)。\n這邊兩個機率 \\(\\alpha\\) 跟 \\(\\beta\\)，若直接硬算，那就會是跟上一篇一樣。但是其實只要算 \\(\\alpha = {0, 1}\\), \\(\\beta = {0, 1}\\) 四種極端情況即可，因為剩下的情況會是這兩種的 mixed strategy。\n\n若 Player B 拿到 Nuts，則 B 一定會 bet。在兩種極端狀況下，Player A 的期望值 payoff：\n\n\n\n\n\n\n\n\n\nPlayer A: Always Fold  (\\(\\alpha = 0\\))\nPlayer A: Always Call  (\\(\\alpha = 1\\))\n\n\n\n\nPlayer B: Always Bet\n\\(0\\)\n\\(-B\\)\n\n\n\n若 Player B 拿到 Air。在四種極端狀況下，Player A 的期望值 payoff：\n\n\n\n\n\n\n\n\n\nPlayer A: Always Fold  (\\(\\alpha = 0\\))\nPlayer A: Always Call  (\\(\\alpha = 1\\))\n\n\n\n\nPlayer B: Always Bet (\\(\\beta = 1\\))\n\\(0\\)\n\\(B + P\\)\n\n\nPlayer B: Always Check (\\(\\beta = 0\\))\n\\(P\\)\n\\(P\\)\n\n\n\n\n我們將以上三種情況，分別畫下來：\n\nSenario 1: Player B 拿到 Nuts，則 Player B 一定下注 (value bet)\nSenario 2: Player B 拿到 Air，假設 Player B 總是下注 (bluff bet)\nSenario 3: Player B 拿到 Air，假設 Player B 總是過牌 (honest check)\n\n那麼對 Player A 來說，這三種情況的 payoff 分別是：\n\n\n\nFigure 1: Seesaw Payoff\n\n\n因為 Player A 要最大化 payoff，所以會選數值大的 (翹翹板高處)。\n先讓我們暫時把這三個翹翹板記在心中，待會再回來看。\n\n\n雙曲面\n假設 Player B 拿到 Nuts 的機率是 \\(r\\) (此處 \\(r=1/5\\))，那麼一個 average over Player B 手牌的 Player A payoff 表格，即是兩個表的加權平均：\n\n\n\n\n\n\n\n\n\nPlayer A: Always Fold  (\\(\\alpha = 0\\))\nPlayer A: Always Call  (\\(\\alpha = 1\\))\n\n\n\n\nPlayer B: Always Bet (\\(\\beta = 1\\))\n\\(0\\)\n\\(-rB + (1-r)(B+P)\\)\n\n\nPlayer B: Always Check (\\(\\beta = 0\\))\n\\((1-r)P\\)\n\\(-rB + (1-r)P\\)\n\n\n\n先來簡化一下符號，將上面的 payoff 寫成矩陣 \\(X = [X_{\\beta \\alpha}]\\)：\n\n\n\n\n\\(\\alpha = 0\\)\n\\(\\alpha = 1\\)\n\n\n\n\n\\(\\beta = 1\\)\n\\(X_{10}\\)\n\\(X_{11}\\)\n\n\n\\(\\beta = 0\\)\n\\(X_{00}\\)\n\\(X_{01}\\)\n\n\n\n在這個例子中是 \\[\n\\begin{align}\nX_{10} & = 0 \\\\\nX_{11} & = -rB + (1-r)(B+P) \\\\\nX_{00} & = (1-r)P \\\\\nX_{01} & = -rB + (1-r)P\n\\end{align}\n\\]\n既然已經知道 Pure Strategy 的 所有可能 情況，那麼 mix strategy 就是這些情況的 convex linear combination，而 \\(\\alpha, \\beta \\in [0, 1]\\) 就是權重。\n所以在 mix strategy 時， Player A 的 payoff 期望值就是： \\[\n\\begin{align}\n\\mathbb{E}[\\text{Payoff}_A] & = \\alpha \\beta X_{11} + \\alpha (1-\\beta) X_{01} + (1-\\alpha) \\beta X_{10} + (1-\\alpha)(1-\\beta) X_{00}  \\tag{1}\\label{eq:hyperbolic_payoff} \\\\\n\\end{align}\n\\]\n將 \\(\\mathbb{E}[\\text{Payoff}_A]\\) 視為一個 \\(\\alpha, \\beta\\) 的函數，畫在三維空間中的話，就是一個雙曲面 (hyperbolic surface)：\n\n\n\nFigure 2: Hyperbolic Payoff Surface\n\n\n這是一個標準的雙曲面 (hyperbolic paraboloid)，中間有個鞍點 (saddle point)。\n雖然是個二次函數，但當固定其中一個變數 (例如 \\(\\beta\\))，截面其實是一條直線。可以將雙曲面想成是一條直線一邊旋轉一邊平移所掃出來的曲面。\n當我們看 Side View 時 (相 \\(\\beta\\) 這個維度投影掉)，可以明顯看出這個直線掃出的感覺，這不就是剛剛的翹翹板? 沒錯，這個雙曲面其實就是把三個翹翹板「合體」起來的結果。\n讓我們 zoom in 看一下這個投影在 \\(\\alpha\\)-\\(\\beta\\) 平面圖，其實也就是把 payoff 畫成 heatmap：\n\n\n\nFigure 3: 2D Payoff Surface - Heatmap\n\n\n這張圖 Figure 3 很視覺化了驗證了之前推導的結果:\n\n當 \\(\\alpha = \\alpha^* = \\frac{P}{B+P}\\) 時， 不管 Player B 選擇哪個，期望值都是一樣的 (白色區域)。\n而當 \\(\\beta = \\beta^* = \\frac{rB}{(1-r)(B+P)}\\) 時， 不管 Player A 選擇哪個，期望值也是一樣的 (白色區域)。\n而當其中有人偏離平衡點時，另一人可以取 0 或 1 的極端策略來懲罰對方 (payoff 變成紅色或藍色)。\n\n\n\n一般式\n現在幾何直覺已經很清楚了，讓我們回到一般式 \\(\\eqref{eq:hyperbolic_payoff}\\)，來算一下公式解。\n首先，這個鞍點會在 \\(\\alpha, \\beta \\in (0, 1)\\) 內部出現的充要條件是： \\[\n\\begin{align}\n(X_{00} - X_{01})(X_{10} - X_{11}) & &lt; 0  \\\\\n(X_{00} - X_{10})(X_{01} - X_{11}) & &lt; 0\n\\end{align}\n\\] 也就是在端點時，兩條斜率相乘是負的。\n而在此時，鞍點的位置 \\((\\alpha^*, \\beta^*)\\) 是： \\[\n\\begin{align}\n\\alpha^* & = \\frac{X_{00} - X_{10}}{X_{00} - X_{01} - X_{10} + X_{11}} \\\\\n\\beta^* & = \\frac{X_{00} - X_{01}}{X_{00} - X_{10} - X_{01} + X_{11}}\n\\end{align}\n\\] 這可以有很多種解法，其中一種直覺 (圖解法) 是讓 \\[\n\\alpha : (1 - \\alpha) = X_{00} - X_{10} : X_{11} - X_{01}\n\\]\n而這個鞍點的 payoff ，也就是這個 game 的 value 是： \\[\n\\mathbb{E}[\\text{Payoff}_A]^* = \\frac{X_{00} X_{11} - X_{01} X_{10}}{X_{00} - X_{01} - X_{10} + X_{11}}\n\\]\n這當然也有很多種解法，例如直接把 \\(\\alpha^*, \\beta^*\\) 帶回去。但另外一個有趣的算法是，若同時扣掉 \\(h\\) 將這個鞍點的高度(垂直平移到0)的話，那雙曲面函數 兩組對角線項 相乘會相等: \\[\n(X_{00} - h)(X_{11} - h) = (X_{01} - h)(X_{10} - h)\n\\]\n這樣自動就解出鞍點的高度 \\(h\\) 了。\n\n\n講回翹翹板\n我們再次回到翹翹板的圖像，原本翹翹板中間的桿子是沒有實質意義的，但現在剛好可以想成是線性的插值。\n\n\n\n\n\nSenario 1\nSenario 2\nSenario 3\n\n\n\n\n直觀解釋\nValue Bet\nBluff Bet\nHonest Check\n\n\n發生的機率\n\\(r\\)\n\\((1-r)\\beta\\)\n\\((1-r)(1-\\beta)\\)\n\n\n翹翹板\n\\(0:-B\\)\n\\(0:(B+P)\\)\n\\(P:P\\)\n\n\n\n而 Player B 的最佳策略，就是使得 Player A 感受到翹翹板是平衡的 (這就是無差異原則!)，也就是用機率加權後是平衡的。\n於是我們再次推導出 bluff-to-value ratio: \\[\n\\frac{\\mathbb{P}[\\text{bluff}]}{\\mathbb{P}[\\text{value}]} = \\frac{(1-r)\\beta}{r} = \\frac{B}{B+P}\n\\] 因為第三種狀況是 balanced，所以不用考慮。"
  },
  {
    "objectID": "posts/poker/lesson_2.html",
    "href": "posts/poker/lesson_2.html",
    "title": "Poker Lesson 2 - 一次只能買到一張牌",
    "section": "",
    "text": "假設 limit Texas Hold’em 中，大小盲注是 30/60，目前翻出三張牌(the flop)，彩池中有 90 元，輪到 Player A 行動。翻牌是 A♥ K♦ 7♥。\n\nPlayer A 手上有 A♠ K♠。\nPlayer B 手上有 J♥ 3♥。\n\n現在 Player A bet 60 元，Player B 該如何決定要不要跟注 (call) 呢?\n\n\n\nFigure 1: Poker Game\n\n\n\n先來算一下彩池賠率 (pot odds)。加上 Player A 的下注，彩池變成 150 元 (90 + 60)，Player B 需要付出 60 元來跟注，所以彩池賠率是 150:60，也就是 2.5:1。\n對應到需要的勝率 (required equity)：\\(\\displaystyle \\frac{1}{2.5 + 1} = 0.2857\\)，也就是說 Player B 需要有至少 28.57% 的勝率才能跟注。\nPlayer B 目前局勢落後，因為 Player A 是 two-pairs，而 Player B 目前只有 high card J。\n但 Player B 只差一張紅心就能組成 flush (同花)。來計算組成紅心的機率。\n這邊有個小陷阱，如果是紅心 K，那麼 Player A 就會組成 full house (葫蘆)，比同花更大，所以要扣除紅心 K。\n翻牌後還剩下 45 張牌 (52 - 7)，其中有 9 張紅心 (13 - 4)，扣掉紅心 K 後還剩 8 張紅心。所以 Player B 組成 flush 的機率是 \\(\\displaystyle \\frac{8}{45} \\approx 17.78\\%\\)。\n這是在第四張牌 (the turn) Player B 就組成 flush 的機率。問題來了，我們該算第四張牌，還是算到第五張牌(the river) 呢? 這勝率 (Equity) 大約多了一倍。\n這就牽扯到 Player A 目前的狀況，如果 Player A 已經 all-in，或者判斷在下一輪 Player A 高機率不會再下注。那 Player B 就可以算到第五張牌，直覺上 Player B 用這一個跟注的 call 「買到」看兩張牌的機會。\n但一般情況，Player A 很可能在第四張牌後繼續下注，(而且 Player A組成 full house 的機率也不低)。\n所以我們來算算看期望值 (EV)：\n\n此時彩池有 210 元，其中 60 元是 Player B 即將跟注進去的錢。\n\nPlayer B 在第四張牌就組成 flush 的機率是 \\(\\frac{8}{45}\\)。Player A 之後就不會再加注或跟注。所以假設最好情況，Player B 直接贏得彩池 150 元。(也就是不考慮 Player A 可能組成 full house 翻盤的情況)。這邊的 EV 是 \\(\\frac{8}{45} \\times 150 \\approx 26.67\\) 元。\n如果第四張牌讓 Player A 組成 full house (還有兩張 K和兩張 A)，Player B 就直接輸掉這手牌。這機率是 \\(\\frac{4}{45}\\)。這邊的 EV 是 \\(\\frac{4}{45} \\times (-60) \\approx -5.33\\) 元。\n如果第四張牌沒讓 Player A 組成 full house，也沒讓 Player B 組成 flush (機率 \\(\\frac{33}{45}\\))，比賽繼續，到了第五張牌。\n\n這時候 Player A 很可能會繼續下注 60 元，Player B 跟注，彩池變成 330 元。其中 120 元是 Player B 跟注進去的錢。\n\nPlayer B 在第五張牌組成 flush 的機率是 \\(\\frac{8}{44}\\)。這邊的 EV 是 \\(\\frac{33}{45} * \\frac{8}{44} * (330 - 120) = 28\\) 元。\n如果第五張牌沒讓 Player B 組成 flush (機率 \\(\\frac{36}{44}\\))，Player B 就輸掉這手牌。這邊的 EV 是 \\(\\frac{33}{45} \\times \\frac{36}{44} \\times (-120) = -72\\) 元。\n\n\n\n\n\n\n\ngraph TD\n    %% 定義節點樣式\n    R4((The Turn))\n    Node2((The River))\n    \n    %% 定義結構與連接線標籤\n    R4 -- \"33/45\" --&gt; Node2\n    R4 -- \"4/45\" --&gt; A1[A win: -60]\n    R4 -- \"8/45\" --&gt; B1[B win: +150]\n\n    Node2 -- \"36/44\" --&gt; A2[A win: -120]\n    Node2 -- \"8/44\" --&gt; B2[B win: +210]\n    \n    %% 樣式調整\n    style Node2 fill:#f9fff0, stroke:#333,stroke-width:2px\n    style A1 fill:#ffcccc, stroke:#cc0000,stroke-width:2px\n    style A2 fill:#ffcccc, stroke:#cc0000,stroke-width:2px\n    style B1 fill:#ccffcc, stroke:#00cc00,stroke-width:2px\n    style B2 fill:#ccffcc, stroke:#00cc00,stroke-width:2px\n\n\nFigure 2: Player A bet 60 on Turn and bet 60 on River\n\n\n\n\n綜合以上情況，Player B 的總 EV 是： \\[\nEV = 26.67 - 5.33 + 28 - 72 = -22.66\n\\] 所以是負的，不建議跟注。\n關鍵在最後一個case，也是機率最大的case。若兩輪都沒中 Player B 的flush，從現在看來，損失是 120 元。\n\nIf Player A checks on the River:\n我們可以看一下如果 Player A 在第四張牌後不繼續下注，Player B 輸的話就只損失 60 元，但贏的話可以贏 150 元，這樣 Player B 的 EV 會變成正的。\n\n\n\n\n\ngraph TD\n    %% 定義節點樣式\n    R4((The Turn))\n    Node2((The River))\n    \n    %% 定義結構與連接線標籤\n    R4 -- \"33/45\" --&gt; Node2\n    R4 -- \"4/45\" --&gt; A1[A win: -60]\n    R4 -- \"8/45\" --&gt; B1[B win: +150]\n\n    Node2 -- \"36/44\" --&gt; A2[\"&lt;b&gt;A win: &lt;span style='color:red'&gt;-60&lt;/span&gt;&lt;/b&gt;\"]\n    Node2 -- \"8/44\" --&gt; B2[\"&lt;b&gt;B win: &lt;span style='color:green'&gt;+150&lt;/span&gt;&lt;/b&gt;\"]\n    \n    %% 樣式調整\n    style Node2 fill:#f9fff0, stroke:#333,stroke-width:2px\n    style A1 fill:#ffcccc, stroke:#cc0000,stroke-width:2px\n    style A2 fill:#ffcccc, stroke:#cc0000,stroke-width:2px\n    style B1 fill:#ccffcc, stroke:#00cc00,stroke-width:2px\n    style B2 fill:#ccffcc, stroke:#00cc00,stroke-width:2px\n\n\nFigure 3: Player A checks on the River\n\n\n\n\n\\[\nEV = \\frac{8}{45} \\times 150 - \\frac{4}{45} \\times 60 + \\frac{33}{45} \\times \\left( \\frac{8}{44} \\times 150 - \\frac{36}{44} \\times 60 \\right) = 7.27\n\\]\n\n\n小結\n回到最一開始，我們計算 Pot Odds 是 2.5:1，對應需要的勝率是 28.57%。 Player B 實際上只有大約 17.78% 的機率在第四張牌就組成 flush，並不足夠跟注。"
  },
  {
    "objectID": "posts/normal-distribution/day_4.html",
    "href": "posts/normal-distribution/day_4.html",
    "title": "一天證明一個 Normal Distribution 的性質 Day4：充分統計量與消息理論",
    "section": "",
    "text": "今天來講統計學中「參數估計」(Parameter Estimation) 一個非常優雅的概念：充分統計量 (Sufficient Statistic)。\n假設我們從一個分佈 i.i.d. 取樣了 \\(n\\) 個數據點： \\[\nX_1, \\ldots, X_n \\sim P_\\theta\n\\] 這個分佈包含一個未知的參數 \\(\\theta\\)。原則上我們想估計這個 \\(\\theta\\)（例如使用 Maximum Likelihood Estimation）。但在進行複雜估計之前，我們先思考一個問題：有沒有辦法把這 \\(n\\) 個數據點「壓縮」成一個更小的統計量 (statistic)，同時完全不損失任何關於 \\(\\theta\\) 的資訊？ 這就是 Sufficient Statistic 的核心精神。\n\n從丟硬幣開始：直觀的推導\n以最簡單的丟硬幣為例。假設硬幣正面朝上的機率是未知的 \\(\\theta\\)，\\((0 &lt; \\theta &lt; 1)\\)。我們丟了 \\(n\\) 次，得到結果 \\(X_1, \\ldots, X_n\\)，其中 \\(X_i \\in \\{0, 1\\}\\)。\n這 \\(n\\) 個數據點的聯合機率分佈 (Joint Distribution) 為： \\[\nP(\\mathbf{X} = \\mathbf{x} \\mid \\theta)\n:= P(X_1=x_1, X_2=x_2, \\ldots, X_n=x_n \\mid \\theta)\n= \\theta^{\\sum x_i} (1-\\theta)^{n - \\sum x_i}\n\\] 觀察這個式子，你會發現它只依賴於 \\(\\sum x_i\\)（也就是正面朝上的總次數），而不在乎 \\(0\\) 和 \\(1\\) 出現的具體順序。\n令 \\(T(\\mathbf{x}) = \\sum_{i=1}^n x_i\\) 為統計量。若我們固定 \\(T(\\mathbf{x}) = \\mu\\)，也就是已知正面出現了 \\(\\mu\\) 次，那麼原本實驗結果 \\(\\mathbf{x}\\) 的條件機率分佈為何？\n\\[\n\\begin{aligned}\nP(\\mathbf{X} = \\mathbf{x} \\mid \\theta, T(\\mathbf{X}) = \\mu)\n&= \\frac{P(\\mathbf{X} = \\mathbf{x} \\mid \\theta)}{P(T(\\mathbf{X}) = \\mu \\mid \\theta)} \\\\\n&= \\frac{\\theta^{\\mu} (1-\\theta)^{n - \\mu}}{\\binom{n}{\\mu} \\theta^{\\mu} (1-\\theta)^{n - \\mu}} \\\\\n&= \\frac{1}{\\binom{n}{\\mu}} \\qquad \\textbf{(跟 $\\theta$ 無關！)} \\\\\n\\end{aligned}\n\\]\n注意到最後的結果完全不包含 \\(\\theta\\)！\n這意味著：一旦我們知道正面出現了幾次（\\(T(\\mathbf{X})\\)），具體是「正反正」還是「反正正」出現的機率都是 \\(1/\\binom{n}{\\mu}\\)，這純粹是排列組合問題，與硬幣本身的性質 \\(\\theta\\) 無關。\n因此，我們定義：如果滿足下式，則 \\(T(\\mathbf{X})\\) 是 \\(\\theta\\) 的 Sufficient Statistic： \\[\nP(\\mathbf{X} \\mid \\theta, T(\\mathbf{X})) = P(\\mathbf{X} \\mid T(\\mathbf{X})) \\tag{1}\\label{eq:suff_stat_def}\n\\] 換句話說，給定 \\(T(\\mathbf{X})\\) 後，原始數據 \\(\\mathbf{X}\\) 分佈不再依賴於參數 \\(\\theta\\)。\n\n\n引進 Information Theory 的語言\n消息理論完全是基於機率論的東西，但他所定義的各種概念，似乎捕捉到了甚麼「資訊」的本質。可以從今天這個角度來欣賞一下。\n先定義一個隨機變數的資訊熵(Shannon Entropy)，若是離散: \\[\nH(X) = - \\sum_{x} P(X=x) \\log P(X=x)\n\\] 若是連續函數，則改成積分(稱之為 differential entropy，可能取值為負): \\[\nH(X) = - \\int f_X(x) \\log f_X(x) dx\n\\] 還記得在 前面文章，已經證明過常態分佈會最大化熵(給定平均值和變異數的限制下)。\n接著，mutual information 定義為: \\[\n\\begin{aligned}\nI(X; Y)\n&= H(X) - H(X|Y)   \\\\\n&= H(Y) - H(Y|X)   \\\\\n&= H(X) + H(Y) - H(X, Y)   \\\\\n\\end{aligned}\n\\] 以上這些定義都是等價的。\n如果 \\(X\\) 和 \\(Y\\) 是獨立的，那麼 \\(H(X|Y) = H(X)\\)，所以 \\(I(X; Y) = 0\\)。反過來說，如果 \\(I(X; Y) = 0\\)，那麼 \\(X\\) 和 \\(Y\\) 必須是獨立的。所以 mutual information 衡量了兩個隨機變數與獨立性的差距。\n\n\nSufficient Statistic 的各種等價定義\n還記得我們只關心 likelihood function，就是 \\(P(\\mathbf{X} \\mid \\theta)\\)。\n如果說 \\(\\theta\\) 也是隨機變數，那我們就可以開始討論 \\(\\theta\\) 的分布、資訊熵、mutual information 之類的。但這邊的設定 \\(\\theta\\) 只是一個待定的參數，若要討論 \\(\\theta\\) 的分布，那就是開始對 prior distribution 做假設了。而這邊很巧妙的是，我們可以推導出一些性質，是不論 \\(\\theta\\) 的prior是怎樣，都會成立的性質! 所以說\n\n消息理論假設「有prior」，但不在乎prior是什麼。\n\n以下假設 \\(\\theta\\) 有個 prior 分布，並假設 \\(T\\) 是個 Sufficient Statistic 滿足 \\(\\eqref{eq:suff_stat_def}\\)，那麼我們有: \\[\n\\begin{aligned}\nP(\\mathbf{X} \\mid \\theta, T(\\mathbf{X}))\n&= P(\\mathbf{X} \\mid T(\\mathbf{X}))  \\qquad \\text{(Sufficient Statistic 定義)} \\\\\n\\end{aligned}\n\\] 同乘以 \\(P(\\theta \\mid T(\\mathbf{X}))\\)， \\[\nP(\\mathbf{X}, \\theta \\mid T(\\mathbf{X}))\n= P(\\mathbf{X} \\mid T(\\mathbf{X})) P(\\theta \\mid T(\\mathbf{X}))  \\tag{2}\\label{eq:suff_stat_indep}   \n\\]\n這個式子的解讀就是: 給定 \\(T(\\mathbf{X})\\) 後，\\(\\mathbf{X}\\) 和 \\(\\theta\\) 是條件獨立的 (conditionally independent)。 一般的推導很可能會直接跳結論:\n\nConditioning on \\(T(X)\\), \\(X\\) and \\(\\theta\\) have mutual information equals \\(0\\)。\n\n但我想帶大家走一下這段推導。\n同時取 \\(\\log\\): \\[\n\\log P(\\mathbf{X}, \\theta \\mid T(\\mathbf{X}))\n= \\log P(\\mathbf{X} \\mid T(\\mathbf{X})) + \\log P(\\theta \\mid T(\\mathbf{X}))\n\\] 將上式對 \\(\\mathbf{X}, \\theta\\) 取條件期望值(限制在 \\(T(\\mathbf{X})\\)): \\[\n\\begin{aligned}\n\\int P(\\mathbf{X}, \\theta \\mid T(\\mathbf{X})) \\log P(\\mathbf{X}, \\theta \\mid T(\\mathbf{X})) d\\mathbf{X} d\\theta\n&= \\int P(\\mathbf{X}, \\theta \\mid T(\\mathbf{X})) \\log P(\\mathbf{X} \\mid T(\\mathbf{X})) d\\mathbf{X} d\\theta \\\\\n&\\quad + \\int P(\\mathbf{X}, \\theta \\mid T(\\mathbf{X})) \\log P(\\theta \\mid T(\\mathbf{X})) d\\mathbf{X} d\\theta \\\\\n&= \\int P(\\mathbf{X} \\mid T(\\mathbf{X})) \\log P(\\mathbf{X} \\mid T(\\mathbf{X})) d\\mathbf{X} \\\\\n&\\quad + \\int P(\\theta \\mid T(\\mathbf{X})) \\log P(\\theta \\mid T(\\mathbf{X})) d\\theta \\\\\n\\end{aligned}\n\\] 所以同乘以 \\(-1\\)，我們有: \\[\nH(\\mathbf{X}, \\theta \\mid T(\\mathbf{X}))\n= H(\\mathbf{X} \\mid T(\\mathbf{X})) + H(\\theta \\mid T(\\mathbf{X}))  \\tag{3}\\label{eq:suff_stat_entropy}\n\\]\n這個式子的解讀就是: 給定 \\(T(\\mathbf{X})\\) 後，\\(\\mathbf{X}\\) 和 \\(\\theta\\) 的條件熵是可加的 (additive)。\n再進一步，這也是 conditional mutual information 的定義: \\[\n\\begin{align}\nI(\\mathbf{X}; \\theta \\mid T(\\mathbf{X}))\n&\\coloneqq H(\\mathbf{X} \\mid T(\\mathbf{X})) + H(\\theta \\mid T(\\mathbf{X})) - H(\\mathbf{X}, \\theta \\mid T(\\mathbf{X}))   \\\\\n&= 0  \\tag{4}\\label{eq:suff_stat_mutual_info}\n\\end{align}\n\\]\n因為這個結構的特殊性 ( \\(\\theta \\rightarrow \\mathbf{X} \\rightarrow T(\\mathbf{X})\\) 是個馬可夫鏈)，我們本來就會有: \\[\nI(\\theta; T(\\mathbf{X}) \\mid \\mathbf{X}) = 0  \\tag{Markov-chain}\\label{eq:markov_chain}\n\\] 根據\n\nMutual information 的 chain rule: \\[\nI(X; Y, Z) = I(X; Z) + I(X; Y \\mid Z)\n\\]\n\n\\[\nI(\\theta; T(\\mathbf{X}) \\mid \\mathbf{X}) + I(\\theta; \\mathbf{X})\n= I(\\theta; \\mathbf{X} \\mid T(\\mathbf{X})) + I(\\theta; T(\\mathbf{X}))\n\\] 因為是 \\(\\eqref{eq:markov_chain}\\)，所以左邊第一項是 \\(0\\)，因此我們有: \\[\nI(\\theta; \\mathbf{X}) - I(\\theta; T(\\mathbf{X}))\n= I(\\theta; \\mathbf{X} \\mid T(\\mathbf{X})) \\ge 0 \\tag{Data-Processing Inequality}\\label{eq:data_processing_ineq}\n\\] 而等號成立當且僅當 \\(\\mathbf{X}\\) 和 \\(\\theta\\) 在給定 \\(T(\\mathbf{X})\\) 後是條件獨立的，也就是說 \\(T(\\mathbf{X})\\) 是個 Sufficient Statistic。\n所以如果 \\(T(\\mathbf{X})\\) 是個 Sufficient Statistic，那麼 \\[\nI(\\theta; \\mathbf{X}) = I(\\theta; T(\\mathbf{X}))  \\tag{5}\\label{eq:suff_stat_info_eq}\n\\]\n綜合以上: \\(\\eqref{eq:suff_stat_def}\\)、\\(\\eqref{eq:suff_stat_indep}\\)、\\(\\eqref{eq:suff_stat_entropy}\\)、\\(\\eqref{eq:suff_stat_mutual_info}\\)、\\(\\eqref{eq:suff_stat_info_eq}\\)，都是等價的定義。\n\n\nNormal Distribution 的 Sufficient Statistic 很簡單，就是樣本均值和樣本變異數:\n假設我們有 \\(X_1, X_2, \\ldots, X_n\\) 是來自常態分佈 \\(\\mathcal{N}(\\mu, \\sigma^2)\\) 的 i.i.d. 樣本，則 \\(\\theta = (\\mu, \\sigma^2)\\) 的 Sufficient Statistic 就是樣本均值和樣本變異數: \\[\nT(\\mathbf{X}) = \\left( \\bar{X}, S^2 \\right) = \\left( \\frac{1}{n} \\sum_{i=1}^{n} X_i, \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2 \\right)\n\\] 也就是說，給定樣本均值和樣本變異數後，原本的樣本數據對於 \\(\\mu, \\sigma^2\\) 不再提供任何額外資訊。\n為什麼哩? 因為常態分佈的 likelihood function 只依賴於樣本均值和樣本變異數: \\[\n\\begin{align}\nP(\\mathbf{X} \\mid \\mu, \\sigma^2)\n&= \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left( -\\frac{(X_i - \\mu)^2}{2 \\sigma^2} \\right)  \\\\\n&= \\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\right)^n \\exp\\left( -\\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (X_i - \\mu)^2 \\right) \\\\\n&= \\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\right)^n \\exp\\left( -\\frac{1}{2 \\sigma^2} \\left[ (n-1) S^2 + n (\\bar{X} - \\mu)^2 \\right] \\right) \\\\\n\\end{align}\n\\]\n直觀上來說，這個 likelihood function 跟 \\(X\\) 有關的部分只剩下 \\(\\bar{X}\\) 和 \\(S^2\\)，所以這兩個統計量已經「充分」地捕捉了關於 \\(\\mu\\) 和 \\(\\sigma^2\\) 的所有資訊。\n但根據定義 \\(\\eqref{eq:suff_stat_def}\\)，我們需要計算 \\(P(\\mathbf{X} \\mid \\mu, \\sigma^2, T(\\mathbf{X}))\\)，並驗證它不依賴於 \\(\\mu\\) 和 \\(\\sigma^2\\)。\n嘿! 但這裡出現了小麻煩，因為是連續的機率密函數，所以不知道怎麼處理 OAO (請參見: 硬核系列-測度論)。\n\n\n直覺是對的: Fisher-Neyman Factorization Theorem\n我們直觀上覺得應該可以從 likelihood function 看出來 \\(T(\\mathbf{X})\\) 是 Sufficient Statistic。這也就是以下定理:\n\n(Fisher-Neyman Factorization Theorem) 若 \\(X_1, \\ldots, X_n\\) 的聯合機率密度函數 (joint pdf/pmf) 為 \\(f(\\mathbf{x} \\mid \\theta)\\)。則 \\(T(\\mathbf{X})\\) 是 \\(\\theta\\) 的 Sufficient Statistic 若且唯若 存在兩個非負函數 \\(g\\) 和 \\(h\\)，使得： \\[\nf(\\mathbf{x} \\mid \\theta) = g(T(\\mathbf{x}), \\theta) \\cdot h(\\mathbf{x})\n\\]\n\n\\(g(T(\\mathbf{x}), \\theta)\\)：這部分包含了 \\(\\theta\\)，但它只透過 \\(T(\\mathbf{x})\\) 來依賴數據 \\(\\mathbf{x}\\)。\n\\(h(\\mathbf{x})\\)：這部分可以依賴所有的數據 \\(\\mathbf{x}\\)，但絕對不能包含 \\(\\theta\\)。"
  },
  {
    "objectID": "posts/normal-distribution/day_9.html",
    "href": "posts/normal-distribution/day_9.html",
    "title": "一天證明一個 Normal Distribution 的性質 Day9： Poisson Summation Formula",
    "section": "",
    "text": "小時候就學過無窮等比級數:\n\\[\n\\sum_{n=0}^{\\infty} r^n = \\frac{1}{1-r}, \\quad |r|&lt;1\n\\] 當時候胡思亂想，想說指數部分如果改成平方怎麼算? \\[\n\\sum_{n=0}^{\\infty} r^{n^2} = ?\n\\] 嗯… 當然是完全不會算，沒想到在大學的殿堂裡還會再次遇見他… Poisson Summation Formula (PSF)。\n這真的是數學中最奇怪又深奧的恆等式。\n\n柏松求和公式 - 高斯函數\n柏松求和公式 (Poisson Summation Formula) 告訴我們，如果上面那個函數叫做 \\(S(t)\\): \\[\nS(t) := \\sum_{n=-\\infty}^{\\infty} e^{-\\pi t n^2}  \\qquad \\forall t &gt; 0\n\\] 對於所有 \\(t&gt;0\\) 都有定義，這裡我們稍微未卜先知，做了一個變數變換 \\(r=e^{-\\pi t}\\)，不過這是等價的。\n那麼我們可以把它寫成另一個無窮級數: \\[\nS(t) = \\frac{1}{\\sqrt{t}} \\sum_{k=-\\infty}^{\\infty} e^{-\\frac{\\pi k^2}{t}} = \\frac{1}{\\sqrt{t}} S\\left(\\frac{1}{t}\\right)\n\\tag{1}\\label{eq:psf_s}\n\\] 疑，形式還是 \\(S\\) 但是 input \\(t\\) 變成了倒數 \\(\\frac{1}{t}\\)。前面還乘以 \\(\\frac{1}{\\sqrt{t}}\\)。\n你說，這樣有了 \\(S(t)\\) 跟 \\(S(\\frac{1}{t})\\) 的關係，我們還是沒辦法算出來啊？對！但這個對稱性本身，暗示了一個更宏大的幾何結構。\n\n\nJacobi Theta Function 與神秘的空間 M [Optional]\n當一個函數滿足一個 functional equation (函數方程式) 的時候，這時候我們就要來研究是否有唯一解? 若不唯一，那解有哪些?\n我們很自然地可以將 \\(S(t)\\) 定義到複數平面 (只要實部大於 0)。 這樣我們就可以把 \\(S(t)\\) 視為一個所謂的 Jacobi Theta Function: \\[\n\\theta(z; \\tau) := \\sum_{n=-\\infty}^{\\infty} e^{\\pi i \\tau n^2} e^{2 \\pi i n z}\n\\] 這邊 \\(z \\in \\mathbb{C}\\)，\\(\\tau \\in \\mathbb{H}\\) (上半平面，對應原本的 \\(it\\))。因為 \\(n^2\\) 跑得比 \\(n\\) 快，所以我們只需要 \\(\\tau\\) 的虛部大於 0 就可以保證收斂性。\n常見的 theta function 總共有四種，我們這邊用到的是 \\(\\theta_3\\)（當 \\(z=0\\) 時就是我們原本的級數）。\n這時候，讓我們退一步看。剛剛那個 \\(S(t) = \\frac{1}{\\sqrt{t}}S(1/t)\\) 的性質，用複數語言寫出來，其實是在說這個函數在經過特定的「變換」（比如 \\(\\tau \\to -1/\\tau\\)）後，會變回自己乘以一個因子。\n那我們能不能用將這個等式推廣到複數函數 \\(\\theta(0; \\tau)\\) 上呢？ 可以的! 經過一些複雜的分析技巧，我們可以得到: \\[\n\\theta(0; \\frac{-1}{\\tau}) = \\sqrt{\\frac{\\tau}{i}} \\, \\theta(0; \\tau)\n\\]\n這類具有高度對稱性的函數，居住在一個叫做 模形式 (Modular Forms) 的向量空間 \\(M\\) 裡。\n簡單來說，如果一個函數 \\(f\\) 滿足： \\[\nf\\left(\\frac{a\\tau+b}{c\\tau+d}\\right) = (c\\tau+d)^k f(\\tau)   \n\\qquad , \\forall \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\in SL(2, \\mathbb{Z}), \\forall \\tau \\in \\mathbb{H}\n\\] 我們就說它是權重為 \\(k\\) 的模形式。我們的 \\(\\theta\\) 函數大致上就是這樣的一個東西（權重 \\(k=1/2\\)）。\n\n這個空間 \\(M\\) 是有限維的！\n在某些特定的限制條件下(例如 \\(k=12\\))，這個空間的維度甚至只有 1。這意味著什麼？這意味著如果我們在這個空間裡找到了兩個函數，它們極有可能是同一個東西（或是只差一個常數倍）。這也就是為什麼數論學家能用 Theta Function 解決很多整數拆分問題的原因——因為路只有一條。\n\n這邊有點含糊了，因為當 \\(k=1/2\\) 根號複數 \\(\\sqrt{c\\tau+d}\\) 會有選分支的問題。所以其實權重 \\(k=1/2\\) 的模形式要退縮到一個較小的子群，通常是 Level 4 的同餘子群 \\(\\Gamma_0(4)\\)。而在空間 \\(M_{1/2}(\\Gamma_0(4))\\) 中，其維度為 1。\n這以後有機會再深入講解。\n\n\n離散高斯\n且讓我們看著 \\(\\theta\\) 的定義式，是不是長得有點像特徵函數 (Characteristic Function) 呢？\n讓我們回憶一下機率論。對於一個連續的高斯分佈 (Normal Distribution)，其機率密度函數 (PDF) 是 \\(e^{-x^2}\\) 的形式，而它的特徵函數 (Characteristic Function, Fourier Transform of PDF) 算出來依然是 \\(e^{-\\xi^2}\\) 的形式。\n那如果是離散的呢？\n假設我們有一個隨機變數 \\(X\\)，它只能取整數值 \\(n \\in \\mathbb{Z}\\)，而且取 \\(n\\) 的機率正比於高斯分佈: \\[\nP(X=n) \\propto e^{-\\pi t n^2}\n\\] 這就是一個 Discrete Gaussian。\n那麼這個離散變數的特徵函數 (Characteristic Function) 是什麼？定義是 \\(\\mathbb{E}[e^{i \\omega X}]\\)： \\[\n\\phi_X(\\omega) = \\sum_{n=-\\infty}^{\\infty} P(X=n) e^{i n \\omega} \\propto \\sum_{n=-\\infty}^{\\infty} e^{-\\pi t n^2} e^{i n \\omega}\n\\] 確實就是 \\(\\theta_3(z; \\tau)\\) 阿! (只要令 \\(\\omega = 2\\pi z\\), \\(\\tau = -i t\\))\n所以，我們可以這樣理解：\n\n連續世界：高斯函數的傅立葉變換是高斯函數。\n離散世界：Theta Function 其實就是「離散高斯分佈」的特徵函數。\n\n不過這邊是成正比，分母還差了一個 \\(\\theta_3(0; it)\\)，這是為了讓機率加總為 1。\n\n\n\n\nPoisson Summation Formula\n\n\n\n\n附錄：Poisson Summation Formula (PSF) 的證明\n雖然前面講得很玄，但數學還是要回歸嚴謹。為什麼 \\(\\sum f(n)\\) 會等於 \\(\\sum \\hat{f}(k)\\)？這裡給出一個完整的證明。\n定理敘述: 假設 \\(f(x) \\in L^1(\\mathbb{R})\\) 是一個連續函數，且滿足足夠的衰減條件（見證明）。定義其傅立葉變換為 \\(\\hat{f}(\\xi) = \\int_{-\\infty}^\\infty f(x)e^{-2\\pi i\\xi x}dx\\)。則： \\[\n\\sum_{n=-\\infty}^{\\infty} f(n) = \\sum_{k=-\\infty}^{\\infty} \\hat{f}(k)\n\\]\n證明：\n我們引入一個輔助函數，將 \\(f(x)\\) 進行「週期化 (Periodization)」，假設級數收斂： \\[\nF(x) := \\sum_{n=-\\infty}^{\\infty} f(x+n)\n\\]\n第一步：確認 \\(F(x)\\) 的性質\n顯然 \\(F(x)\\) 是一個週期為 1 的函數。\n既然是週期函數，我們就可以將其展開為 傅立葉級數 (Fourier Series)： \\[\nF(x) = \\sum_{k=-\\infty}^{\\infty} c_k e^{2\\pi i k x}\n\\] 其中係數 \\(c_k\\) 的計算公式為： \\[\nc_k = \\int_{0}^{1} F(x) e^{-2\\pi i k x} dx\n\\]\n第二步：連結 \\(c_k\\) 與 \\(\\hat{f}\\) (關鍵步驟)\n將 \\(F(x)\\) 的定義代入 \\(c_k\\)： \\[\nc_k = \\int_{0}^{1} \\left( \\sum_{n=-\\infty}^{\\infty} f(x+n) \\right) e^{-2\\pi i k x} dx\n\\]\n這裡我們遇到了一個數學上的危險操作：積分與求和交換 (Interchange of Summation and Integration)。 \\[\n\\int_{0}^{1} \\sum_{n} (...) \\stackrel{?}{=} \\sum_{n} \\int_{0}^{1} (...)\n\\] 交換的條件： 根據 Fubini 定理或控制收斂定理 (Dominated Convergence Theorem)，我們需要 \\(f(x)\\) 衰減得夠快。一個充分條件是存在常數 \\(C&gt;0\\) 與 \\(\\delta &gt; 0\\)，使得： \\[\n|f(x)| \\le \\frac{C}{(1+|x|)^{1+\\delta}}\n\\] 這保證了 \\(\\sum |f(x+n)|\\) 是一致收斂的，積分與求和便可交換。\n交換後： \\[\nc_k = \\sum_{n=-\\infty}^{\\infty} \\int_{0}^{1} f(x+n) e^{-2\\pi i k x} dx\n\\] 利用 \\(e^{-2\\pi i k x}\\) 的週期性， \\(e^{-2\\pi i k x} = e^{-2\\pi i k (x+n)}\\)，我們可以把積分變數變換 \\(y = x+n\\)： \\[\nc_k = \\sum_{n=-\\infty}^{\\infty} \\int_{n}^{n+1} f(y) e^{-2\\pi i k y} dy\n\\] 觀察這個式子，這是把實數軸切成一段一段長度為 1 的區間，然後全部加起來。根據積分的可加性，這等於從 \\(-\\infty\\) 積到 \\(\\infty\\)： \\[\nc_k = \\int_{-\\infty}^{\\infty} f(y) e^{-2\\pi i k y} dy = \\hat{f}(k)\n\\]\n第三步：代回並取值\n現在我們知道 \\(F(x) = \\sum_{k} \\hat{f}(k) e^{2\\pi i k x}\\)。\n令 \\(x=0\\)，我們得到： \\[\nF(0) = \\sum_{k=-\\infty}^{\\infty} \\hat{f}(k)\n\\] 回顧 \\(F(x)\\) 的原始定義 \\(F(0) = \\sum_{n} f(n)\\)，兩式合併即得證： \\[\n\\sum_{n=-\\infty}^{\\infty} f(n) = \\sum_{k=-\\infty}^{\\infty} \\hat{f}(k)\n\\]\nQ.E.D."
  },
  {
    "objectID": "posts/normal-distribution/day_8.html",
    "href": "posts/normal-distribution/day_8.html",
    "title": "一天證明一個 Normal Distribution 的性質 Day8： C n 取 k",
    "section": "",
    "text": "「從 52 張撲克牌中，隨機抽 7 張牌，有多少種不同的組合方式？」\n「\\(\\displaystyle \\binom{52}{7}\\)，嗯… 可能要按一下計算機?」\n「不用，我們可以利用常態分佈的近似來心算看看！」\n\\[\n\\binom{52}{7} = \\frac{52\\times 51 \\times 50 \\times 49 \\times 48 \\times 47 \\times 46}{7 \\times 6 \\times 5 \\times 4 \\times 3 \\times 2 \\times 1}\n\\]\n\n常態分佈近似?\n我們要用機率的觀點，計算 \\(\\frac{1}{2^{52}}\\binom{52}{7}\\)。也就是擲 \\(52\\) 枚公正的硬幣，有多少機率會出現 \\(7\\) 個正面朝上？\n\n平均值: \\(Np = 52 \\times \\frac{1}{2} = 26\\)\n變異數: \\(Np(1-p) = 52 \\times \\frac{1}{2} \\times \\frac{1}{2} = 13\\)\n標準差: \\(\\sqrt{Np(1-p)} = \\sqrt{13} \\approx 3.6\\)\n\n所以有正面朝上的個數分布大約就是： \\(\\mathcal{N}(26, 3.6^2)\\)。\n但因為常態分佈是描述連續的機率密度，而我們擲硬幣是離散的。\n所以應該是要算：從 \\(6.5\\) 到 \\(7.5\\) 的區間下的常態分佈面積，但也可以用 \\(x=7\\) 那個點的機率密度來近似(想成用寬度為1的長方形近似)。\n\\[\n\\begin{aligned}\nf(z)\n&= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(z-\\mu)^2}{2\\sigma^2}}\n= \\frac{1}{\\sqrt{2\\pi\\times 13}} e^{-\\frac{(7-26)^2}{2 \\times 13}}\n= \\frac{1}{\\sqrt{26\\pi}} e^{-\\frac{361}{26}}  \\\\\n&\\approx \\frac{1}{\\sqrt{80}} \\times e^{-14}\n\\end{aligned}\n\\] 記得還要乘上 \\(2^{52}\\) 才是組合數。\n這邊需要一些常用的近似值：\n\n\\(2^{10} \\approx 10^3\\)\n\\(e^3 \\approx 20\\)\n\\(e^7 \\approx 1100\\)\n\n所以估計 \\[\n\\binom{52}{7}\n\\approx 2^{52} \\times \\frac{1}{\\sqrt{80}} \\times e^{-14}\n\\approx \\frac{4 \\times 10^{15}}{9 \\times 10^6 \\times 1.1^2}\n\\approx 4 \\times 10^8\n\\]\n實際上 \\(\\binom{52}{7} = 133784560\\)，我們的估計值大約是 \\(3\\) 倍左右! 意外地不準! 為什麼?\n\n\n梯形近似\n其實以心算來說已經夠了，如果再算更複雜還不如用電腦算。 但我們實在很想知道，這個不準的來源，是來自於哪裡?\n\n常態分佈 \\(\\approx\\) 二項分佈。\n用長方形(寬度 1) 近似曲線底下的面積。\n\n我們來算算看梯形面積會不會好一點? (P.S 這裡已經失去了速算的意義，單純研究誤差來源)\n\\[\n\\begin{aligned}\n\\frac{f(6.5)+f(7.5)}{2}\n&= \\frac{1}{2} \\frac{1}{\\sqrt{2\\pi\\times 13}} \\left( e^{-\\frac{(6.5-26)^2}{2 \\times 13}} + e^{-\\frac{(7.5-26)^2}{2 \\times 13}} \\right) \\\\\n&\\approx \\frac{1}{2\\times 9} \\left( e^{-14.625} + e^{-13.1} \\right) \\\\\n&\\approx \\frac{1}{18} \\left( \\frac{1}{2.2 \\times 10^6} + \\frac{1}{5 \\times 10^5} \\right)\n\\approx \\frac{1}{18} \\times \\frac{1.22}{5 \\times 10^5}\n\\approx \\frac{1}{7.4 \\times 10^6}\n\\end{aligned}\n\\] 剛才是約 \\(\\frac{1}{9 \\times 10^6}\\)，已經是高估三倍了，這次是 \\(\\frac{1}{7.4 \\times 10^6}\\)，誤差又更大了!!\n因為 convexity 的關係，在超過 \\(\\mu \\pm \\sigma\\)後，常態分佈曲線已經是『下凸』 (Convex / Concave Up，像碗一樣向上彎)，梯形面積反而比長方形面積誤差還更大，所以誤差更大了。當然可以用切線近似之類的，但本質上最大問題，在於常態分佈本身在離 \\(\\mu\\) 很遠的地方，已經無法很好近似二項分佈了。\n(P.S. 雖然說常態分佈的尾巴比二項分布還要厚，但是已經比其他真實世界的分佈還薄很多，例如股市漲跌幅、或是Student-t分布。)\n\n\n大偏差理論 Large Deviation Theory\n我們花了一番力氣證明，無論是用長方形還是梯形去近似常態分佈下的面積，都無法修復 3 倍以上的誤差。這意味著：問題不在於積分技巧，而在於模型本身。\n這個現象在機率論裡有一個專門的學科在研究，叫做 大偏差理論 (Large Deviation Theory, LDT)。\n我們擲 52 次硬幣，平均正面次數 \\(\\mu=26\\)。但我們卻想計算 \\(k=7\\) 的機率。\n\\(k=7\\) 是一個極端事件 (Extreme Event)。雖然常態分佈是根據中央極限定理（Central Limit Theorem, CLT）得出的，它保證了當 \\(n\\) 夠大時，中央區域的表現很像常態分佈。但是，CLT 並不保證尾部的行為。\nLDT 研究的正是這些極端事件發生的機率是如何指數級地衰減。它告訴我們，當事件離平均值越遠，常態分佈的近似（它假設了 \\(e^{-x^2}\\) 的衰減）就會嚴重失真。對於 \\(z \\approx -5.3\\) 這種極端偏差，我們必須使用更精確的工具。\n\n根據 大偏差理論 (Large Deviation Theory) 中的 Cramér’s Theorem，對於樣本平均數 \\(\\bar{X}_n\\) 偏離期望值 \\(p\\) 的機率，其衰減速度並不是常態分佈描述的 \\(e^{-x^2}\\)，而是由 速率函數 (Rate Function) \\(I(x)\\) 決定的： \\[\nP(\\bar{X}_n \\approx x) \\approx e^{-n I(x)}\n\\] 對於伯努利分佈（投硬幣），這個速率函數 \\(I(x)\\) 其實就是 相對熵 (KL Divergence)： \\[\nI(x) = x \\ln \\frac{x}{p} + (1-x) \\ln \\frac{1-x}{1-p}\n\\] 在我們的例子中，\\(n=52, p=0.5, x=7/52\\)。如果我們計算 \\(e^{-n I(x)}\\)，你會發現它剛好等於我們後面要用的那個「神奇修正項」： \\[\ne^{-52 \\cdot I(7/52)} = \\left(\\frac{7/52}{1/2}\\right)^{-7} \\left(\\frac{45/52}{1/2}\\right)^{-45} \\times (\\text{常數})\n\\] 也就是說，LDT 準確預測了常態分佈在尾部會失效，並且告訴我們失效的幅度（修正項）剛好就是相對熵的指數！為了具體計算這個值，我們可以使用 LDT 常用的技巧：指數傾斜 (Exponential Tilting)，也就是把機率測度變換到讓事件變成「中心」的地方!\n\n既然在原本的機率分佈 (\\(p=0.5\\)) 下， \\(k=7\\) 是個看不清楚的邊緣地帶，那我們不如創造一個新的平行宇宙，在這個宇宙裡，\\(k=7\\) 才是最正常的事件。\n我們可以定義一個新的二項分佈，讓它的參數變成 \\(p'=\\frac{7}{52}\\)，這樣我們就可以把 \\(k=7\\) 這個事件，轉換成在新分佈下的「平均事件」。\n在新的分布下，\n\n平均值 \\(\\mu' = Np = 52 \\times \\frac{7}{52} = 7\\)，\n變異數 \\(\\sigma'^2 = Np(1-p) = 52 \\times \\frac{7}{52} \\times \\left(1 - \\frac{7}{52}\\right) = 7 \\times \\frac{45}{52} \\approx 6.06\\)，\n\n\\[\nP'(X=7) = \\binom{52}{7} (p')^7 (1-p')^{45}\n\\]\n根據我們的常態分佈近似（因為現在 7 是平均值，也就是峰值，近似會非常準確）： \\[\nP'(X=7) \\approx \\frac{1}{\\sqrt{2\\pi \\sigma'^2}}\n\\]\n將兩式合併，我們就可以反推組合數 \\(\\binom{52}{7}\\)： \\[\n\\begin{aligned}\n\\binom{52}{7}\n&\\approx \\frac{1}{\\left(\\frac{7}{52}\\right)^7 \\left(\\frac{45}{52}\\right)^{45}} \\times \\frac{1}{\\sqrt{2\\pi \\times 52 \\frac{7}{52} \\frac{45}{52} }} \\\\\n&\\approx \\frac{1}{\\sqrt{2\\pi}} \\frac{52^{52}}{7^7 \\times 45^{45}} \\times \\frac{\\sqrt{52}}{\\sqrt{7\\times 45}} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\frac{\\sqrt{52}}{\\sqrt{7\\times 45}} e^{52 H(\\frac{7}{52})}\n\\end{aligned}\n\\]\n這正是 Stirling Formula 的形式！ 我們繞了一大圈，利用機率觀點的「換底操作」，重新發現了階乘近似公式。雖然這完全失去了心算的意義，但這揭示了常態分佈、大偏差理論與組合數學之間深刻的連結。\n作為收尾，我們比較一下這跟剛才 \\(p=1/2\\) 時的近似值： \\[\n\\begin{aligned}\n\\binom{52}{7}\n&\\approx 2^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(k-\\frac{N}{2})^2}{2 \\sigma^2}} \\qquad\\text{ , where } \\sigma^2=\\frac{N}{4} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\frac{1}{\\sqrt{\\frac{N}{4}}} e^{N\\ln(2) - \\frac{(k-\\frac{N}{2})^2}{\\frac{N}{2}}} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\frac{2}{\\sqrt{N}} e^{N \\left[ \\ln(2) - 2\\left(\\frac{1}{2} - \\frac{k}{N}\\right)^2 \\right] } \\\\\n\\end{aligned}\n\\]\n假設我們不看係數的誤差，令 \\(x=\\frac{k}{N}\\)，比較兩個指數部分的差異，正確的是用 \\(H(x)\\)，而錯誤近似的是用 \\(\\ln(2) - 2\\left(\\frac{1}{2} - x\\right)^2\\)，是一個二次函數的近似。\n這樣就看得很清楚了，當 \\(x=1/2\\) 兩者相等，但當 \\(x\\) 遠離 \\(1/2\\) 時，二次函數的近似會越來越高估 \\(H(x)\\) 的值，導致組合數被高估。\n\n\n\nFigure 1: \\(H(x)\\) vs \\(\\ln(2) - 2\\left(\\frac{1}{2} - x\\right)^2\\)"
  },
  {
    "objectID": "posts/normal-distribution/day_3.html",
    "href": "posts/normal-distribution/day_3.html",
    "title": "一天證明一個 Normal Distribution 的性質 Day3：多變量常態分佈",
    "section": "",
    "text": "今天來講 \\(n\\) 維空間的多變量常態分佈 (Multivariate Normal Distribution)。高維空間的常態分佈雖然在形式上只是把一維的做 \\(n\\) 次方，但他豐富的特性，卻讓它在統計學、機器學習，甚至於密碼學、純數學理論中都有非常重要的應用。\n對一個 \\(n\\) 維隨機向量 \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_n)^T\\)，我們說它服從多變量常態分佈，記作 \\(\\mathbf{X} \\sim \\mathcal{N}_n(\\boldsymbol{\\mu}, \\Sigma)\\)，如果它的機率密度函數 (PDF) 為： \\[\nf_{\\mathbf{X}}(\\mathbf{x}) = \\frac{1}{(2\\pi)^{n/2} |\\Sigma|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right)\n\\] 其中 \\(\\boldsymbol{\\mu} \\in \\mathbb{R}^n\\) 是均值向量 (mean vector)，\\(\\Sigma \\in \\mathbb{R}^{n \\times n}\\) 是協方差矩陣 (covariance matrix)，且必須是正定矩陣 (positive definite matrix)。\nOK，老實說這個公式看起來有點嚇人，但其實我們知道所有對稱的實數正定矩陣都可以被對角化 (diagonalized)，寫成 \\(\\Sigma = P D P^{-1}\\)，所以經過一個 orthogonal 轉換 \\(\\mathbf{X}=P\\mathbf{Y}\\)，\\(f_\\mathbf{X}(x)\\) 可以寫成 \\[\nf_{\\mathbf{X}}(\\mathbf{x}) = f_{\\mathbf{Y}}(\\mathbf{y}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}} \\exp\\left( -\\frac{(y_i - \\mu_i)^2}{2\\sigma_i^2} \\right)\n\\] 噹噹! 其實就是 \\(n\\) 個獨立的一維常態分佈的乘積啦！\n新手常犯的錯誤：\n\n請注意這邊的用詞，我們刻意區分「多變量常態分佈 (Multivariate Normal Distribution)」和 「常態分佈(Normal Distribution)」。\n隨機變數 \\(X, Y\\) 都是常態分佈，並 不代表 \\((X, Y)\\) 這個二維向量服從多變量常態分佈，除非 \\(X\\) 和 \\(Y\\) 是獨立的 (independent)！這點非常重要。獨立是個很強的條件， \\((X, Y)\\) 的聯合分佈 (joint distribution) 可能會有非常複雜的形式。\n如果隨機變數 \\((X, Y)\\) 服從二維多變量常態分佈，而且 covariance 為 \\(0\\) ，那麼 \\(X\\) 和 \\(Y\\) 一定是獨立的！這是個簡單推論，但卻是多變量常態分佈的一個非常特別的性質，其他分佈並不一定成立。\nKac-Berstein Theorem [Optional]。事實上，我們只需要假設 \\(X\\) 和 \\(Y\\) 是獨立的隨機變數(一維)，且 \\(X-Y\\) 和 \\(X+Y\\) 也是獨立的，那麼 \\((X, Y)\\) 必須服從二維的多變量常態分佈。(WHY?)\nCramer’s decomposition Theorem [Optional]。如果 \\(X\\) 和 \\(Y\\) 是獨立的隨機變數，且 \\(X+Y\\) 服從常態分佈，那麼 \\(X\\) 和 \\(Y\\) 必須服從常態分佈。(WHY?)\n\n\n旋轉不變性 (Rotational Invariance)\n如果我們取標準的多變量常態分佈 \\(\\mathbf{X} \\sim \\mathcal{N}_n(\\mathbf{0}, I_n)\\)，其中 \\(I_n\\) 是 \\(n\\) 維單位矩陣 (identity matrix)。那對於任意的正交矩陣 (orthogonal matrix) \\(Q\\)（即 \\(Q^T Q = I_n\\)），我們有: \\[  \n\\mathbf{Y} = Q \\mathbf{X} \\sim \\mathcal{N}_n(\\mathbf{0}, I_n) \\quad \\tag{1}\\label{eq:rotate_invar}\n\\] 這是因為 \\(f_{\\mathbf{X}}(\\mathbf{x}) \\propto e^{-\\frac{1}{2} \\mathbf{x}^T \\mathbf{x}}\\)，而 \\(\\mathbf{x}^T \\mathbf{x}\\) 在正交變換下是不變的。\n而對於一般的多變量常態分佈 \\(\\mathbf{X} \\sim \\mathcal{N}_n(\\boldsymbol{\\mu}, \\sigma^2\\mathbf{I}_n)\\)，我們需要做個平移: \\[\n\\mathbf{Y} = Q(\\mathbf{X} - \\mathbf{\\mu}) \\sim \\mathcal{N}_n(\\boldsymbol{0}, \\sigma^2 I_n)  \\tag{2}\\label{eq:rotate_invar_shift}\n\\]\n這其實是個非常奇妙的性質，以至於只有常態分佈才有這個特性。\n從最直覺的觀點: PDF 函數\n假設一個在 \\(n\\) 維空間中的PDF函數 \\(f_\\mathbf{X}(x)\\) 具有旋轉不變性，同時又是 \\(n\\) 個一維獨立變數的乘積。那首先，這些一維的變數必須都相同分佈 (identically distributed) 而且對原點對稱，否則旋轉後會改變分佈。那不妨假設一維的 PDF 是 \\(g(x)\\)，那旋轉不變性要求對於所有的 \\(n\\) 維向量 \\((x_1, x_2, \\ldots, x_n)\\)，我們有： \\[\ng\\left(\\sqrt{\\sum_{i=1}^n x_i^2} \\right) = \\prod_{i=1}^{n} g(x_i)\n\\] 令 \\(h(t) = \\ln(g(\\sqrt{t}))\\)，我們有： \\[\nh\\left(\\sum_{i=1}^n x_i^2\\right) = \\sum_{i=1}^{n} h(x_i^2)\n\\] 這是柯西函數方程，因為 \\(h\\) 是連續的，所以我們有 \\(h(t) = kt\\)，因此 \\(g(x) = e^{kx^2}\\)。為了讓 \\(g(x)\\) 成為一個合法的 PDF，我們需要 \\(k&lt;0\\)，這正是常態分佈的形式！\n\n\n應用: 樣本均值(Sample Mean) 和 樣本變異數(Sample Variance) 是獨立的\n這算是一個神奇的應用場景吧! 假設有 \\(X_1, X_2, \\ldots, X_n\\) 是來自(一維)常態分佈 \\(\\mathcal{N}(\\mu, \\sigma^2)\\) 的獨立同分佈樣本 (i.i.d. samples)。我們定義樣本均值和樣本變異數如下： \\[\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i, \\quad S^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\n\\] 那麼 \\(\\bar{X}\\) 和 \\(S^2\\) 是獨立的隨機變數！\n我們將 \\(n\\) 個sample視為是 \\(n\\) 維空間的隨機向量，\\(\\mathbf{X} = [X_1, X_2, \\ldots, X_n]^T\\)。 因為i.i.d. 所以是multivariate normal，\\(\\mathbf{X} \\sim \\mathcal{N}_n(\\mu \\mathbf{1}_n, \\sigma^2 I_n)\\)。\n要減去常數向量 \\(\\mu \\mathbf{1}_n= [\\mu, \\mu, \\ldots, \\mu]^T\\) 後，\\(\\mathbf{X} - \\mu \\mathbf{1}_n\\) 才有旋轉不變性 \\(\\eqref{eq:rotate_invar_shift}\\)。(隨然實驗學家不知道真的 \\(\\mu\\) 是多少，但還是可以進行這樣的推導)。\n令 \\(u_1 = \\frac{1}{\\sqrt{n}}[1, 1, \\ldots, 1]^T\\)，這是一個單位向量 (unit vector)，代表均值的方向。然後選擇 \\(n-1\\) 個正交於 \\(u_1\\) 的單位向量 \\(u_2, u_3, \\ldots, u_n\\)，所以 \\([u_1, u_2, \\ldots, u_n]\\) 形成一個正交矩陣 \\(Q\\)。令 \\[\n\\mathbf{Y} \\coloneqq Q^T (\\mathbf{X} - \\mu \\mathbf{1}_n)\n\\] ，其實也就是 \\[\n\\begin{aligned}\nY_1 &\\coloneqq u_1^T \\cdot (\\mathbf{X} - \\mu \\mathbf{1}_n), \\quad \\\\\nY_i &\\coloneqq u_i^T \\cdot (\\mathbf{X} - \\mu \\mathbf{1}_n) \\quad \\text{ for } i=2, 3, \\ldots, n\n\\end{aligned}\n\\] 根據旋轉不變性 \\(\\eqref{eq:rotate_invar_shift}\\)，我們有 \\(Y_1, Y_2, \\ldots, Y_n\\) 是獨立的隨機變數，並且 \\[\n||\\mathbf{Y}||^2 = ||\\mathbf{X} - \\mu \\mathbf{1}_n||^2 = \\sum_{i=1}^{n} (X_i - \\mu)^2\n\\] 上面的等號就是個恆等式，僅代表作標轉換，並非取期望值什麼的。\n這時看出來了嗎?\n\n\\(\\bar{X}\\) 是 \\(Y_1\\) 的函數。因為 \\(\\bar{X} = \\mu + \\frac{1}{\\sqrt{n}} Y_1\\)。\n\\(S^2\\) 是 \\(Y_2, Y_3, \\ldots, Y_n\\) 的函數。因為 \\[\n\\begin{aligned}\n(n-1)S^2\n&= \\sum_{i=1}^{n} (X_i - \\bar{X})^2   \\\\\n&= \\sum_{i=1}^{n} (X_i - \\mu + \\mu - \\bar{X})^2   \\\\\n&= \\sum_{i=1}^{n} (X_i - \\mu)^2 + 2\\sum_{i=1}^{n}(X_i - \\mu)(\\mu - \\bar{X}) + n(\\mu - \\bar{X})^2   \\\\\n&= \\sum_{i=1}^{n} (X_i - \\mu)^2 - 2n(\\mu - \\bar{X})^2 + n(\\mu - \\bar{X})^2   \\\\\n&= \\sum_{i=1}^{n} (X_i - \\mu)^2 - n(\\mu - \\bar{X})^2  \\\\\n&= ||\\mathbf{Y}||^2 - Y_1^2 = \\sum_{i=2}^{n} Y_i^2\n\\end{aligned}\n\\] 所以 \\(S^2 = \\frac{1}{n-1} \\sum_{i=2}^{n} Y_i^2\\)。\n\n\n若隨機變數 \\(A, B\\) 是獨立的，則對於任意(可測)函數 \\(f, g\\)。必定有 \\(f(A)\\) 和 \\(g(B)\\) 也是獨立的。\n\n如此一來就證明了 \\(\\bar{X}\\) 和 \\(S^2\\) 是獨立的隨機變數！ Q.E.D.\n這裡也順便證明了一個經典結果：對於 \\(X_i \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)，我們有 \\[\n\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\n\\] 這裡把高中時期統計學講的 「剩下\\(n-1\\)個自由度」給講得清清楚楚了。"
  },
  {
    "objectID": "posts/2025-1112-conjugacy_prior/index.html",
    "href": "posts/2025-1112-conjugacy_prior/index.html",
    "title": "Conjugate Prior",
    "section": "",
    "text": "在貝氏統計 (Bayesian statistics) 中，我們經常需要估計一個未知參數的機率分佈。\n讓我們從一個最經典的問題開始：丟硬幣。\n假設我們有一枚硬幣，它有 \\(p\\) 的機率正面朝上。我們不知道 \\(p\\) 究竟是多少（可能是一枚完美的硬幣，\\(p=0.5\\)，也可能是一枚被動過手腳的硬幣）。\n為了估計 \\(p\\)，我們開始做實驗：連續丟這枚硬幣。 假設我們總共丟了 \\(N\\) 次，結果是 \\(\\alpha\\) 次正面和 \\(\\beta\\) 次反面（其中 \\(\\alpha + \\beta = N\\)）。\n現在，問題來了：我們對 \\(p\\) 的最佳估計是什麼？"
  },
  {
    "objectID": "posts/2025-1112-conjugacy_prior/index.html#共軛分佈二估計事件的發生率-gamma-與-poisson-的共舞",
    "href": "posts/2025-1112-conjugacy_prior/index.html#共軛分佈二估計事件的發生率-gamma-與-poisson-的共舞",
    "title": "Conjugate Prior",
    "section": "共軛分佈（二）：估計事件的「發生率」— Gamma 與 Poisson 的共舞",
    "text": "共軛分佈（二）：估計事件的「發生率」— Gamma 與 Poisson 的共舞\n如果我們要估計的不是一個 0 到 1 的機率，而是一個**「率」 (rate)** 呢？例如：\n\n一個客服中心，平均每小時接到多少通電話？\n一個路口，平均每 10 分鐘會經過多少輛車？\n你的程式碼，平均每 1000 行有多少個 bug？\n\n這些事件的共同點是，它們在一個連續區間（時間、空間）內發生，我們可以去「計數」(count)，理論上發生的次數可以 是 0, 1, 2, … 一直到無限大。\n這類「計數」問題，正是 Poisson 分佈的主場。而當我們想對 Poisson 分佈的「率」(\\(\\lambda\\)) 進行貝氏推論時，就輪到它的共軛夥伴——Gamma 分佈——登場了。\n第一步：我們的「似然」— Poisson 分佈\n和之前一樣，貝氏推論的第一步是建立我們的似然 (Likelihood)。\n我們要估計的核心參數是 \\(\\lambda\\) (lambda)，代表「單位時間（或單位空間）內的平均事件發生率」。\nPoisson 分佈告訴我們，如果平均率是 \\(\\lambda\\)，那麼在一個單位時間內，實際觀測到 \\(k\\) 次事件的機率為：\n\\[\n\\mathbb{P}(k \\mid \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\] 假設我們進行了 \\(n\\) 次觀測（例如，我們觀察了 \\(n\\) 個小時），得到的數據是 \\(D = \\{x_1, x_2, \\dots, x_n\\}\\)，其中 \\(x_i\\) 是第 \\(i\\) 個小時觀測到的事件次數。\n「給定 \\(\\lambda\\)」，觀測到這整組數據 \\(D\\) 的聯合機率（似然）就是把所有機率乘起來： \\[\n\\mathbb{P}(D \\mid \\lambda) = \\prod_{i=1}^n \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\n\\]\n在貝氏推論中，我們只關心和 \\(\\lambda\\) 相關的項。把上式重新整理：\n\\[\n\\begin{aligned}\n\\mathbb{P}(D \\mid \\lambda) &\\propto \\left( \\prod_{i=1}^n \\lambda^{x_i} \\right) \\left( \\prod_{i=1}^n e^{-\\lambda} \\right) \\\\\n&\\propto \\lambda^{\\sum x_i} \\cdot e^{-n\\lambda}\n\\end{aligned}\n\\]\n令 \\(S = \\sum x_i\\)（我們觀測到的總事件數），我們的似然函數可以簡潔地表示為：\n\nLikelihood: \\(\\mathbb{P}(D \\mid \\lambda) \\propto \\lambda^S e^{-n\\lambda}\\)\n\n第二步：我們的「事前」— Gamma 分佈\n現在，我們需要為 \\(\\lambda\\) 選擇一個事前分佈 (Prior)。 \\(\\lambda\\) 作為一個「率」，它必須大於 0。我們需要一個定義在 \\((0, \\infty)\\) 上的機率分佈。\n更重要的是，我們希望這個事前分佈 \\(\\mathbb{P}(\\lambda)\\) 乘上似然 \\(\\lambda^S e^{-n\\lambda}\\) 之後，能得到一個形式相同的分佈。\n看看似然的形式：\\(\\lambda\\) 的某次方，再乘以 \\(e\\) 的 \\(\\lambda\\) 負次方。 什麼分佈長這樣呢？ 答案就是 Gamma 分佈！\nGamma 分佈由兩個超參數 \\(\\alpha\\) (shape, 形狀) 和 \\(\\beta\\) (rate, 率) 定義：\n\nPrior: \\(\\mathbb{P}(\\lambda) = \\text{Gamma}(\\lambda \\mid \\alpha, \\beta) \\propto \\lambda^{\\alpha-1} e^{-\\beta\\lambda}\\)\n\n直觀解釋 \\(\\alpha\\) 和 \\(\\beta\\)： 你可以把 \\(\\alpha\\) 想像成你的「事前信念中的總事件數」，而 \\(\\beta\\) 是「事前信念中的總觀測單位數」。 例如，如果你「猜」這個率 \\(\\lambda\\) 大約是 5（例如 5 通電話 / 1 小時），你可以設 \\(\\alpha=5, \\beta=1\\)。\n第三步：貝氏魔法！推導「事後分佈」\n我們再次使出貝氏定理的武器：\n\n事後分佈 (Posterior) \\(\\propto\\) 似然 (Likelihood) \\(\\times\\) 事前分佈 (Prior)\n\n\\[\n\\begin{aligned}\n\\mathbb{P}(\\lambda \\mid D) &\\propto \\mathbb{P}(D \\mid \\lambda) \\times \\mathbb{P}(\\lambda) \\\\\n&\\propto [\\lambda^S e^{-n\\lambda}] \\times [\\lambda^{\\alpha-1} e^{-\\beta\\lambda}]  \\\\\n&\\propto \\lambda^{(S + \\alpha) - 1} \\cdot e^{-(n + \\beta)\\lambda}\n\\end{aligned}\n\\]\n它是不是 \\(\\lambda\\) 的 (某數 - 1) 次方，再乘以 \\(e\\) 的 (負某數) \\(\\lambda\\) 次方？ 這正是一個新的 Gamma 分佈！\n\n\n結論：優雅的更新規則\n我們證明了：\n\n如果你的事前信念是 \\(\\text{Gamma}(\\alpha, \\beta)\\)，\n接著你觀測了 \\(n\\) 個單位，總共發生了 \\(S = \\sum x_i\\) 次事件，\n你的事後信念就會更新為 \\(\\text{Gamma}(\\alpha_{\\text{new}}, \\beta_{\\text{new}})\\)。\n\n更新規則超級簡單，只是加法：\n\n\\(\\alpha_{\\text{new}} = \\alpha + S\\) (舊的事件數 + 新觀測到的事件數)\n\\(\\beta_{\\text{new}} = \\beta + n\\) (舊的觀測單位 + 新觀測的單位數)\n\n舉個例子：\n\n事前 (Prior)： 你是新來的客服經理，你猜測客服中心平均每小時接 10 通電話。你對這個猜測不太確定，所以你設定了 \\(\\text{Gamma}(\\alpha=10, \\beta=1)\\) 當作你的事前信念。（信念強度 = 1 小時的觀測）\n數據 (Data)： 你實際去觀測了 5 個小時（\\(n=5\\)），接到的電話數分別是 \\(\\{12, 8, 11, 10, 9\\}\\)。\n似然 (Likelihood)： 觀測單位 \\(n=5\\)。觀測總數 \\(S = 12+8+11+10+9 = 50\\)。\n事後 (Posterior)：\n\n\n\\(\\alpha_{\\text{new}} = \\alpha + S = 10 + 50 = 60\\)\n\\(\\beta_{\\text{new}} = \\beta + n = 1 + 5 = 6\\)\n你更新後的信念是 \\(\\text{Gamma}(60, 6)\\)。\n\nGamma 分佈的期望值是 \\(\\alpha / \\beta\\)。\n\n你原先的期望值是 \\(\\alpha/\\beta = 10 / 1 = 10\\) 通/小時。\n你更新後的期望值是 \\(\\alpha_{\\text{new}} / \\beta_{\\text{new}} = 60 / 6 = 10\\) 通/小時。\n\n在這個例子中，你的平均估計沒有變，因為你的觀測數據 (50/5 = 10) 剛好符合你的猜測！但是，你的信心大大增加了（Gamma(60, 6) 是一個比 Gamma(10, 1) 更尖、更窄的分佈），因為你的信念現在是基於 6 個小時的數據，而不僅僅是 1 個小時的猜測。\n\n\n\n\n圖一： Gamma(10,1) 與 Gamma(60,6) 的比較\n\n\n\n這就是 Gamma-Poisson 共軛家族的美妙之處：它提供了一個直觀且計算簡單的方法，讓我們不斷用新的「計數數據」來更新我們對「事件發生率」的信念。"
  },
  {
    "objectID": "posts/2025-1112-conjugacy_prior/index.html#共軛分佈三估計常態分佈的平均值與變異數-normal-inverse-gamma",
    "href": "posts/2025-1112-conjugacy_prior/index.html#共軛分佈三估計常態分佈的平均值與變異數-normal-inverse-gamma",
    "title": "Conjugate Prior",
    "section": "共軛分佈（三）：估計常態分佈的平均值與變異數 (Normal-Inverse-Gamma)",
    "text": "共軛分佈（三）：估計常態分佈的平均值與變異數 (Normal-Inverse-Gamma)\n在前面的文章中，我們學會了如何：\n\n用 Beta-Binomial 估計機率 (0 到 1 之間)。\n用 Gamma-Poisson 估計頻率 (大於 0 的計數)。\n\n現在，我們要來處理統計學中最常見的問題：估計一個平均值 (mean)。\n\n一群學生的平均身高是多少？\n一批產品的平均壽命是多久？\n某支股票的平均日報酬率是多少？\n\n這些測量值——身高、時間、報酬率——都是連續變數。而說到連續變數，統計學的王者，常態分佈 (Normal Distribution) \\(\\mathcal{N}(\\mu, \\sigma^2)\\)，就該登場了。\n挑戰：兩個未知數\n。但這裡有一個挑戰。不像 Binomial (只有 \\(p\\)) 或 Poisson (只有 \\(\\lambda\\)) 只有一個參數，常態分佈 \\(\\mathcal{N}(\\mu, \\sigma^2)\\) 描述了兩個我們都不知道的參數：\n\n平均值 \\(\\mu\\)：我們主要想估計的目標。\n變異數 \\(\\sigma^2\\)：數據的波動程度，我們通常也不知道。\n\n我們希望能建立一個貝氏模型，讓我們對 \\(\\mu\\) 和 \\(\\sigma^2\\) 的「信念」在看到新數據 \\(D = \\{x_1, ..., x_n\\}\\) 後，能自動更新。\n暖身: 從最大概似估計 (MLE) 看起\n在進入貝氏模型之前，讓我們先用傳統的「最大概似估計 (Maximum Likelihood Estimation, MLE)」來暖身。這能幫助我們理解 Likelihood Function 的長相。\n假設 \\(D = \\{x_1, ..., x_n\\}\\) 來自 \\(\\mathcal{N}(\\mu, \\sigma^2)\\)，Likelihood Function 為： \\[\n\\begin{align}\n\\mathbb{P}(D \\mid \\mu, \\sigma^2) &= \\prod_{i=1}^n \\left( \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}} \\right)  \\\\\n&\\propto \\frac{1}{\\sigma^n} e^{-\\frac{\\sum_{i=1}^n (x_i-\\mu)^2}{2\\sigma^2}}\n\\end{align}\n\\] 為了方便計算，我們取其負對數 (Negative Log-Likelihood): \\[\n\\begin{align}\n- \\ln(\\mathbb{P}(D \\mid \\mu, \\sigma^2))\n&= \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i-\\mu)^2 + n \\ln(\\sigma) + \\text{constant}\n\\end{align}\n\\] 這邊constant的意思是跟 \\(\\mu, \\sigma\\) 無關。\n用 maximal likelihood 的觀點，這個 \\(\\mu, \\sigma\\) 的極值發生在哪裡? 我們記等式左邊這個 log-likelihood 為 \\(f = - \\ln(\\mathbb{P}(D \\mid \\mu, \\sigma^2))\\)。令其偏微分各自為0。 \\[\n\\begin{align}\n\\frac{\\partial f}{\\partial \\mu} &=  \\frac{1}{\\sigma^2}\\sum_{i=1}^n (\\mu - x_i) = 0  \\\\\n\\frac{\\partial f}{\\partial \\sigma} &=  -\\frac{1}{\\sigma^3}\\sum_{i=1}^n (x_i-\\mu)^2 + \\frac{n}{\\sigma} = 0  \n\\end{align}\n\\] 解出來為 \\[\n\\begin{align}\n\\hat{\\mu} &= \\frac{1}{n}\\sum_{i=1}^n x_i = \\overline{x} \\\\\n\\hat{\\sigma}^2 &= \\frac{1}{n}\\sum_{i=1}^n (x_i-\\overline{x})^2  \n\\end{align}\n\\] (可以驗證一下，二階導數矩陣正定，所以 \\(f\\) 是取到最小值)\n\n題外話：為什麼是除以 \\(n\\) 而不是 \\(n-1\\)？ 一般說的樣本標準差是「除以 \\(n-1\\)」這裡推導出來怎麼是除以 \\(n\\)? 難道推導錯誤了嗎? 沒有，若要最大化似然，確實應該要除以 \\(n\\)，而一般常用的除以 \\(n-1\\)，那是我們希望這個 \\(\\sigma^2\\) 是個無偏估計 (unbiased-estimation)。也就是說我們希望 \\[\n\\mathbb{E}[\\tilde{\\sigma}^2] = \\sigma^2\n\\] 把期望值的定義寫出來，也就是 \\[\n\\int \\tilde{\\sigma}^2 \\mathbb{P}(D \\mid \\mu, \\sigma^2)  dx_1...dx_n = \\sigma^2\n\\] 也就是一般的樣本標準差 \\(\\tilde{\\sigma}^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i-\\overline{x})^2\\)，是在所有無偏估計中，擁有最小variance的 (Why?)\n\n\n貝式解法: Normal-Inverse-Gamma (NIG) 模型\n好了，拉回我們的正題：貝氏更新。我們要找一個共軛事前分佈 \\(\\mathbb{P}(\\mu, \\sigma^2)\\)，使得它乘上 Likelihood \\(\\mathbb{P}(D \\mid \\mu, \\sigma^2)\\) 之後，得到的 Posterior \\(\\mathbb{P}(\\mu, \\sigma^2 \\mid D)\\) 仍然和 Prior 具有相同的形式。這個神奇的 Prior 就是 Normal-Inverse-Gamma (NIG) 分佈。它由四個超參數 \\((\\mu_0, \\kappa_0, \\alpha_0, \\beta_0)\\) 定義，其結構如下：\n\n對 \\(\\sigma^2\\) 的信念：我們假設 \\(\\sigma^2\\) 服從一個 Inverse-Gamma 分佈: \\[\n\\mathbb{P}(\\sigma^2) \\propto (\\sigma^2)^{-\\alpha_0-1}e^{-\\frac{\\beta_0}{\\sigma^2}}\n\\]\n對 \\(\\mu\\) 的信念 (給定 \\(\\sigma^2\\))：我們假設 \\(\\mu\\) 服從一個常態分佈，但這個常態分佈的變異數取決於 \\(\\sigma^2\\)： \\[\n\\mathbb{P}(\\mu \\mid \\sigma^2) \\sim \\mathcal{N}(\\mu_0, \\sigma^2/\\kappa_0)\n\\propto \\frac{1}{\\sigma /\\sqrt{\\kappa_0} } e^{-\\frac{(\\mu-\\mu_0)^2}{2\\sigma^2 / \\kappa_0}}\n\\]\n\n這樣就描述了我們對 \\(\\mathbb{P}(\\mu, \\sigma^2)\\) 的 joint distribution， \\[\n\\begin{align}\n\\mathbb{P}(\\mu, \\sigma^2)\n&= \\mathbb{P}(\\sigma^2) \\cdot \\mathbb{P}(\\mu \\mid \\sigma^2) \\\\\n&\\propto  (\\sigma^2)^{-\\alpha_0-1}e^{-\\frac{\\beta_0}{\\sigma^2}} \\frac{1}{\\sigma /\\sqrt{\\kappa_0} } e^{-\\frac{(\\mu-\\mu_0)^2}{2\\sigma^2 / \\kappa_0}}\n\\end{align}\n\\] 一樣是取negative log-likelihood看比較清楚: \\[\n\\begin{align}\n-\\ln(\\mathbb{P}(\\mu, \\sigma^2))\n&=  \\frac{\\kappa_0(\\mu-\\mu_0)^2}{2\\sigma^2} + \\frac{\\beta_0}{\\sigma^2} + (2\\alpha_0 + 3) \\ln(\\sigma) + \\text{constant}\n\\end{align}\n\\]\n精彩的來了，我們要算經過一筆資料 \\(D\\) 更新後的事後分布是否也是一樣的形式呢? 而四個超參數 \\((\\alpha_0, \\beta_0, \\mu_0, \\kappa_0)\\) 會如何變動呢?\n\\[\n\\begin{align}\n-\\ln(\\mathbb{P}(\\mu, \\sigma^2 \\mid D))\n&= -\\ln(\\mathbb{P}(D \\mid \\mu, \\sigma^2)) -\\ln(\\mathbb{P}(\\mu, \\sigma^2)) + \\text{constant} \\\\\n&=  \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i-\\mu)^2 + n \\ln(\\sigma) \\\\\n&\\qquad + \\frac{\\kappa_0(\\mu-\\mu_0)^2}{2\\sigma^2} + \\frac{\\beta_0}{\\sigma^2} + (2\\alpha_0 + 3) \\ln(\\sigma) + \\text{constant} \\\\\n&= \\left( \\frac{n+\\kappa_0}{2\\sigma^2} \\right)\\mu^2\n+ \\left( -\\frac{(\\sum_{i=1}^n x_i) + \\kappa_0 \\mu_0}{\\sigma^2} \\right)\\mu \\\\\n&\\qquad + \\frac{(\\sum_{i=1}^n x_i^2) + \\kappa_0 \\mu_0^2 + 2\\beta_0}{2\\sigma^2}\n+ (n+2\\alpha_0+3)\\ln(\\sigma) + \\text{constant}\n\\end{align}\n\\] 比較一下係數，可以依序解出 \\[\n\\begin{align}\n\\kappa_n &= n + \\kappa_0  \\\\\n\\mu_n &= \\frac{(\\sum_{i=1}^n x_i) + \\kappa_0 \\mu_0}{n+\\kappa_0}  \\\\\n\\alpha_n &= \\alpha_0 + \\frac{n}{2}  \\\\\n\\beta_n &= -\\left( \\frac{n+\\kappa_0}{2} \\right)\\mu_n^2\n+ \\frac{(\\sum_{i=1}^n x_i^2) + \\kappa_0 \\mu_0^2 + 2\\beta_0}{2} \\quad\\text{(by completing the square)} \\\\\n&= - \\frac{(\\sum_{i=1}^n x_i + \\kappa_0 \\mu_0)^2}{2(n+\\kappa_0)} + \\frac{(\\sum_{i=1}^n x_i^2) + \\kappa_0 \\mu_0^2 + 2\\beta_0}{2}\n\\end{align}\n\\]\n\n\n總結：更新規則與直觀解釋\n我們成功了！Posterior \\(\\mathbb{P}(\\mu, \\sigma^2 \\mid D)\\) 確實保持了 Normal-Inverse-Gamma (NIG) 的形式，其超參數 \\((\\mu_n, \\kappa_n, \\alpha_n, \\beta_n)\\) 透過以下規則更新：\n\n\n\n\n\n\n\n\n超參數\n事前 (Prior)\n事後 (Posterior) 更新規則\n\n\n\n\n\\(\\kappa_n\\)\n\\(\\kappa_0\\)\n\\(\\kappa_n = \\kappa_0 + n\\)\n\n\n\\(\\mu_n\\)\n\\(\\mu_0\\)\n\\(\\mu_n = \\frac{\\kappa_0 \\mu_0 + n\\overline{x}}{\\kappa_0 + n}\\) (其中 \\(\\overline{x} = \\frac{1}{n}\\sum x_i\\))\n\n\n\\(\\alpha_n\\)\n\\(\\alpha_0\\)\n\\(\\alpha_n = \\alpha_0 + \\frac{n}{2}\\)\n\n\n\\(\\beta_n\\)\n\\(\\beta_0\\)\n\\(\\beta_n = \\beta_0 + \\frac{1}{2} \\sum_{i=1}^n (x_i - \\overline{x})^2 + \\frac{n\\kappa_0}{2(n+\\kappa_0)}(\\overline{x} - \\mu_0)^2\\)\n\n\n\n這代表什麼？，讓我們來直觀地解讀這些更新規則：\n\n\\(\\kappa_n = \\kappa_0 + n\\)： 我們對 \\(\\mu\\) 的信心（\\(\\kappa\\)）等於「先前的信心」加上「新數據的點數」。這非常合理。\n\\(\\mu_n = \\frac{\\kappa_0 \\mu_0 + n\\overline{x}}{\\kappa_0 + n}\\)： 這是最漂亮的一條規則！ 我們更新後的平均值 \\(\\mu_n\\)，是「先前平均值 \\(\\mu_0\\)」和「數據平均值 \\(\\overline{x}\\)」的加權平均。 權重分別是 \\(\\kappa_0\\)（先前的信心）和 \\(n\\)（數據的數量）。如果 \\(\\kappa_0\\) 很小（不確定的 prior），\\(\\mu_n\\) 就會很接近 \\(\\overline{x}\\)。如果 \\(\\kappa_0\\) 很大（強烈的 prior），\\(\\mu_n\\) 就會很接近 \\(\\mu_0\\)。\n\\(\\alpha_n = \\alpha_0 + \\frac{n}{2}\\)： 我們對 \\(\\sigma^2\\) 的信念（\\(\\alpha\\)）也隨著數據點 \\(n\\) 而增加。\n\\(\\beta_n = \\beta_0 + \\dots\\)： 我們對 \\(\\sigma^2\\) 的信念（\\(\\beta\\)）更新，等於「先前的 \\(\\beta_0\\)」加上「數據內部的變異 \\(\\sum (x_i - \\overline{x})^2\\)」再加上「數據平均值與先前平均值之間的差異 \\((\\overline{x} - \\mu_0)^2\\)」。這也完全符合直覺。\n\n至此，我們完成推導了常態分佈的共軛模型！它讓我們能夠在 \\(\\mu\\) 和 \\(\\sigma^2\\) 都未知的情況下，優雅地更新我們的信念。"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2025-1114-distribution_10/index.html",
    "href": "posts/2025-1114-distribution_10/index.html",
    "title": "10 Types of Common Distributions",
    "section": "",
    "text": "Let’s list 10 common continuous probability distributions along with their probability density functions (PDFs), I’ve ignored the constant normalization factors for simplicity. Here’s a summary table:\n\nContinuous Probability Distributions Table\n\n\nTable 1: My Distribution Table\n\n\n\n\n\n\n\n\n\n\n\nCF\nName, Notation\nParameters\nPDF\nRange\nMean\nVariance\n\n\n\n\n\\(e^{i\\mu t - \\frac{1}{2}\\sigma^2 t^2}\\)\nNormal  \\(\\mathcal{N}(\\mu, \\sigma^2)\\)\nmean: \\(\\mu\\)  variance: \\(\\sigma^2\\)\n\\(\\displaystyle e^{-\\frac{1}{2\\sigma^2}x^2+\\frac{\\mu}{\\sigma^2}x}\\)\n\\(x \\in (-\\infty, \\infty)\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\n\n\\((1 - 2it)^{-\\frac{k}{2}}\\)\nChi-Squared  \\(\\chi^2(k)\\)\ndegree of freedom (d.o.f): \\(k\\)\n\\(\\displaystyle x^{\\frac{k}{2}-1}e^{-\\frac{x}{2}}\\)\n\\(x\\in(0, \\infty)\\)\n\\(k\\)\n\\(2k\\)\n\n\nugly\nChi [Optional]  \\(\\chi(k)\\)\ndegree of freedom (d.o.f): \\(k\\)\n\\(\\displaystyle x^{k-1}e^{-\\frac{x^2}{2}}\\)\n\\(x\\in(0, \\infty)\\)\n\\(\\sqrt{2} \\frac{\\Gamma(\\frac{k+1}{2})}{\\Gamma(\\frac{k}{2})}\\)\n\\(k - \\left(\\sqrt{2} \\frac{\\Gamma(\\frac{k+1}{2})}{\\Gamma(\\frac{k}{2})}\\right)^2\\)\n\n\n\\(\\frac{\\lambda}{\\lambda - it}\\)\nExponential  \\(\\mathrm{Exp}(\\lambda)\\)\nrate: \\(\\lambda\\)\n\\(\\displaystyle e^{-\\lambda x}\\)\n\\(x\\in(0, \\infty)\\)\n\\(\\frac{1}{\\lambda}\\)\n\\(\\frac{1}{\\lambda^2}\\)\n\n\n\\(\\left(1 - i\\theta t\\right)^{-\\alpha}\\)\nGamma  \\(\\mathrm{Gamma}(\\alpha, \\theta)\\)\nshape: \\(\\alpha\\)  scale: \\(\\theta\\)\n\\(\\displaystyle x^{\\alpha-1} e^{-\\frac{x}{\\theta}}\\)\n\\(x\\in(0, \\infty)\\)\n\\(\\alpha\\theta\\)\n\\(\\alpha\\theta^2\\)\n\n\n\\({}_1F_1(\\alpha; \\alpha+\\beta; it)\\)\nBeta  \\(\\mathrm{Beta}(\\alpha, \\beta)\\)\nshape1: \\(\\alpha\\)  shape2: \\(\\beta\\)\n\\(\\displaystyle x^{\\alpha-1}(1-x)^{\\beta-1}\\)\n\\(x\\in(0, 1)\\)\n\\(\\frac{\\alpha}{\\alpha+\\beta}\\)\n\\(\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)\n\n\n\\(\\frac{\\sin(t)}{t}\\)\nUniform  \\(\\mathrm{Uniform}(-1, 1)\\)\nNA\n\\(\\displaystyle \\text{constant}\\)\n\\(x\\in(-1, 1)\\)\n\\(0\\)\n\\(\\frac{1}{3}\\)\n\n\n\\(e^{-|t|}\\)\nCauchy  \\(\\mathrm{Cauchy}\\)\nNA\n\\(\\displaystyle \\frac{1}{1+x^2}\\)\n\\(x\\in(-\\infty, \\infty)\\)\nundefined\nundefined\n\n\n\\(\\frac{K_{\\frac{\\nu}{2}}(\\sqrt{\\nu}|t|) (\\sqrt{\\nu}|t|)^{\\frac{\\nu}{2}}}{\\Gamma(\\frac{\\nu}{2}) 2^{\\frac{\\nu}{2}-1}}\\)\nStudent’s t  \\(t(\\nu)\\)\ndof: \\(\\nu\\)\n\\(\\displaystyle \\left(1+\\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}\\)\n\\(x\\in(-\\infty, \\infty)\\)\n\\(0\\) for \\(\\nu&gt;1\\)\n\\(\\frac{\\nu}{\\nu-2}\\), for \\(\\nu&gt;2\\)\n\n\nugly\nF-distribution  \\(F(d_1, d_2)\\)\ndof 1: \\(d_1\\)  dof 2: \\(d_2\\)\n\\(\\left(\\frac{d_1 x}{d_1 x + d_2}\\right)^{\\frac{d_1}{2}} \\left(\\frac{d_2}{d_1 x + d_2}\\right)^{\\frac{d_2}{2}} x^{-1}\\)\n\\(x\\in(0, \\infty)\\)\n\\(\\frac{d_2}{d_2 - 2}\\), for \\(d_2\\) &gt;2\n\\(\\frac{2 d_2^2 (d_1 + d_2 - 2)}{d_1 (d_2 - 2)^2 (d_2 - 4)}\\), for \\(d_2\\) &gt;4\n\n\n\n\n\n\\({\\displaystyle K_{\\nu }}\\) is the modified Bessel function of the second kind.\n\\({}_1F_1(a; b; z)\\) is the confluent hypergeometric function.\n\n\n\nDiscrete Distributions\nHere are lists for common discrete probability distributions:\n\n\nTable 2: My Discrete Distribution Table\n\n\n\n\n\n\n\n\n\n\n\nCF\nName, Notation\nParameters\nPMF\nRange\nMean\nVariance\n\n\n\n\n\\(1 - p + p e^{it}\\)\nBernoulli  \\(\\mathrm{Bernoulli}(p)\\)\nsuccess prob.: \\(p\\)\n\\(\\displaystyle p^x (1-p)^{1-x}\\)\n\\(x\\in\\{0,1\\}\\)\n\\(p\\)\n\\(p(1-p)\\)\n\n\n\\((1 - p + p e^{it})^n\\)\nBinomial  \\(\\mathrm{Binomial}(n, p)\\)\ntrials: \\(n\\)  success prob.: \\(p\\)\n\\(\\displaystyle \\binom{n}{x} p^x (1-p)^{n-x}\\)\n\\(x\\in\\{0,1,\\ldots,n\\}\\)\n\\(np\\)\n\\(np(1-p)\\)\n\n\n\\(e^{\\lambda(e^{it}-1)}\\)\nPoisson  \\(\\mathrm{Poisson}(\\lambda)\\)\nrate: \\(\\lambda\\)\n\\(\\displaystyle \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\n\\(x\\in\\{0,1,2,\\ldots\\}\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\n\n\\(\\frac{p e^{it}}{1 - (1-p)e^{it}}\\)\nGeometric  \\(\\mathrm{Geometric}(p)\\)\nsuccess prob.: \\(p\\)\n\\(\\displaystyle (1-p)^{x} p\\)\n\\(x\\in\\{0,1,2,\\ldots\\}\\)\n\\(\\frac{1-p}{p}\\)\n\\(\\frac{1-p}{p^2}\\)\n\n\n\\(\\left(\\frac{p e^{it}}{1 - (1-p)e^{it}}\\right)^r\\)\nNegative Binomial  \\(\\mathrm{NegBin}(r, p)\\)\nsuccesses: \\(r\\)  success prob.: \\(p\\)\n\\(\\displaystyle \\binom{x+r-1}{r-1} p^r (1-p)^x\\)\n\\(x\\in\\{0,1,2,\\ldots\\}\\)\n\\(\\frac{r(1-p)}{p}\\)\n\\(\\frac{r(1-p)}{p^2}\\)\n\n\n\n\n\n\n\nRelationships\nObvious:\n\nExponential is a special case of Gamma: \\(\\mathrm{Exp}(\\lambda) \\sim \\mathrm{Gamma}(1, \\frac{1}{\\lambda})\\).\nChi-squared is a special case of Gamma: \\(\\chi^2(k) \\sim \\mathrm{Gamma}(\\frac{k}{2}, 2)\\).\nA special case of Chi-squared is Exponential: \\(\\chi^2(2) \\sim \\mathrm{Exp}(\\frac{1}{2})\\). This is called the Rayleigh distribution.\nChi is the square root of Chi-squared: \\(\\chi(k) \\sim \\sqrt{\\chi^2(k)}\\).\nNormal is a special case of Chi: \\(\\mathcal{N}(0, 1) \\sim \\chi(1)\\).\nCauchy is a special case of Student’s t: \\(\\mathrm{Cauchy} \\sim t(1)\\).\nThe chi-squared, Student’s t, and F distributions are all naturally come from the Normal distribution in the following senses:\n\n\\(\\displaystyle \\chi^2(k) \\sim \\sum_{i=1}^k Z_i^2\\), where \\(Z_i\\) are i.i.d. standard normal variables.\n\\(\\displaystyle t(\\nu) \\sim \\frac{Z}{\\sqrt{\\chi^2(\\nu)/\\nu}}\\), where \\(Z\\) is a standard normal variable independent of the Chi-squared variable.\n\\(\\displaystyle F(d_1, d_2) \\sim \\frac{\\chi^2(d_1)/d_1}{\\chi^2(d_2)/d_2}\\), which is the ratio of two scaled Chi-squared distributions.\n\nBernoulli is a special case of Binomial: \\(\\mathrm{Bernoulli}(p) \\sim \\mathrm{Binomial}(1, p)\\).\n\nStability:\n\n\\(\\displaystyle \\mathcal{N}(\\mu_1, \\sigma_1^2) + \\mathcal{N}(\\mu_2, \\sigma_2^2) \\sim \\mathcal{N}(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\).\n\\(\\displaystyle \\mathrm{Gamma}(\\alpha_1, \\theta) + \\mathrm{Gamma}(\\alpha_2, \\theta) \\sim \\mathrm{Gamma}(\\alpha_1 + \\alpha_2, \\theta)\\).\n\\(\\displaystyle \\chi^2(k_1) + \\chi^2(k_2) \\sim \\chi^2(k_1 + k_2)\\).\n\\(\\displaystyle \\mathrm{Binomial}(n_1, p) + \\mathrm{Binomial}(n_2, p) \\sim \\mathrm{Binomial}(n_1 + n_2, p)\\).\n\\(\\displaystyle \\mathrm{Poisson}(\\lambda_1) + \\mathrm{Poisson}(\\lambda_2) \\sim \\mathrm{Poisson}(\\lambda_1 + \\lambda_2)\\).\n\nLess Obvious:\n\nBeta is related to Gamma in the following way: \\[\n\\begin{cases}\nX \\sim \\mathrm{Gamma}(\\alpha, \\theta),  \\\\\nY \\sim \\mathrm{Gamma}(\\beta, \\theta),  \\\\\nX \\perp Y\n\\end{cases}\n\\implies\n\\begin{cases}\n\\displaystyle \\frac{X}{X+Y} \\sim \\mathrm{Beta}(\\alpha, \\beta),  \\\\\nX+Y \\sim \\mathrm{Gamma}(\\alpha + \\beta, \\theta), \\\\\n\\displaystyle \\frac{X}{X+Y} \\perp (X+Y)\n\\end{cases}\n\\]\n\nA special case of the above is that taking \\(n\\) independent standard normal \\(Z_i \\sim \\mathcal{N}(0,1)\\) variables, then for any \\(k &lt; n\\): \\[\n\\displaystyle \\frac{Z_1^2 + Z_2^2 + \\cdots + Z_k^2}{Z_1^2 + Z_2^2 + \\cdots + Z_n^2} \\sim \\mathrm{Beta}\\left(\\frac{k}{2}, \\frac{n-k}{2}\\right)\n\\] Notice that comparing to the F-distribution, the numerator is using Chi-squared variable from the denominator.\n\nSample \\(n\\) independent \\(U_i \\sim \\mathrm{Uniform}(0,1)\\) variables. The \\(k\\)-th order statistic follows a Beta distribution: \\(X_{(k)} \\sim \\mathrm{Beta}(k, n-k+1)\\).\n\n\n(To be added)\n\nExponential is related to Poisson process:\n\nIf events occur according to a Poisson process with rate \\(\\lambda\\), then the waiting time until the \\(k\\)-th event follows a Gamma distribution: \\(T_k \\sim \\mathrm{Gamma}(k, \\frac{1}{\\lambda})\\).\nThe Exponential and Gamma distributions are related to the Poisson process, which models the occurrence of random events over time.\n\n\nThe Beta distribution is often used in Bayesian statistics as a prior distribution for probabilities."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zero to the Zeroth Power",
    "section": "",
    "text": "Poker Lesson 5 - 雙曲面 Payoff\n\n\n\n\n\n\n\nPoker\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\nPoker Lesson 4 - 如何詐唬\n\n\n\n\n\n\n\nPoker\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\n一天證明一個 Normal Distribution 的性質 Day9： Poisson Summation Formula\n\n\n\n\n\n\n\nNormal Distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\n一天證明一個 Normal Distribution 的性質 Day8： C n 取 k\n\n\n\n\n\n\n\nNormal Distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\nPoker Lesson 3 - hands 組合數\n\n\n\n\n\n\n\nPoker\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\nPoker Lesson 2 - 一次只能買到一張牌\n\n\n\n\n\n\n\nPoker\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\nPoker Lesson 1 - 池底賠率 (Pot Odds)\n\n\n\n\n\n\n\nPoker\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\n一天證明一個 Normal Distribution 的性質 Day7：Hermite Polynomials\n\n\n\n\n\n\n\nNormal Distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\n一天證明一個 Normal Distribution 的性質 Day6：Chi-squared Test\n\n\n\n\n\n\n\nNormal Distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\n一天證明一個 Normal Distribution 的性質 Day5：獨立性的量尺: Cumulant\n\n\n\n\n\n\n\nNormal Distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\n一天證明一個 Normal Distribution 的性質 Day4：充分統計量與消息理論\n\n\n\n\n\n\n\nNormal Distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\n最短的向量\n\n\n\n\n\n\n\nCryptography\n\n\nNP-Reduction\n\n\nLinear Algebra\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\n10 Types of Common Distributions\n\n\n\n\n\n\n\nstatistic\n\n\nCheat Sheet\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\n一天證明一個 Normal Distribution 的性質 Day3：多變量常態分佈\n\n\n\n\n\n\n\nNormal Distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\n一天證明一個 Normal Distribution 的性質 Day2：特徵函數(CF)與傅立葉變換\n\n\n\n\n\n\n\nNormal Distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\n聊聊 Error Correcting Code (ECC) 的基石：Linear Code\n\n\n\n\n\n\n\nCryptography\n\n\nLinear Algebra\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\nConjugate Prior\n\n\n\n\n\n\n\nstatistic\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\n一天證明一個 Normal Distribution 的性質 Day1：高斯積分與最大化熵(Entropy)\n\n\n\n\n\n\n\nNormal Distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\n  \n\n\n\n\n萬聖節快樂\n\n\n\n\n\n\n\nLinear Algebra\n\n\nCryptography\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2025\n\n\nLTN\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2025\n\n\nTai-Ning Liao\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2025-1031-helloween/index.html",
    "href": "posts/2025-1031-helloween/index.html",
    "title": "萬聖節快樂",
    "section": "",
    "text": "萬聖節就是要講鬼故事!\n由於看了計算 \\(\\pi\\) 的 BBP formula。\n我試著體驗重新發現這個公式的過程，覺得非常奇怪。\n讓我們從 \\(\\frac{\\pi}{4}\\) 的萊布尼茲級數開始回顧。\n\n萊布尼茲級數\n由 \\(\\arctan\\) 函數的微分是的rational function: \\[\n\\int_0^t \\frac{1}{1+x^2} dx = \\arctan(t)\n\\] 我們可以將 \\(\\frac{1}{1+x^2}\\) 展開成無窮級數： \\[\n\\frac{1}{1+x^2} = 1 - x^2 + x^4 - x^6 + x^8 - x^{10} + \\cdots = \\sum_{n=0}^{\\infty} (-1)^n x^{2n}\n\\]\n假設 \\(t&lt;1\\)，此級數在 \\(x\\in [0,t]\\) 上是絕對收斂的，所以我們可以將積分和無窮級數交換順序： \\[\n\\begin{align}\n\\arctan(t)\n&= \\int_0^t \\frac{1}{1+x^2} dx  \\\\\n&= \\int_0^t \\sum_{n=0}^{\\infty} (-1)^n x^{2n} dx  \\\\\n&= \\sum_{n=0}^{\\infty} \\int_0^t (-1)^n x^{2n} dx  \\\\\n&= \\sum_{n=0}^{\\infty} (-1)^n \\frac{t^{2n+1}}{2n+1}  \\tag{1}\\label{eq:arctan_taylor}  \\\\\n\\end{align}\n\\] 想像中，讓 \\(t\\) 由小於 \\(1\\) 漸漸趨近於 \\(1\\)，我們就得到萊布尼茲級數： \\[\n\\arctan(1) = \\frac{\\pi}{4} = \\sum_{n=0}^{\\infty} \\frac{(-1)^n}{2n+1} = 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\cdots  \\quad\\tag{2}\\label{eq:leibniz}\n\\] 但由於右邊這個級數不是絕對收斂，所以我們要特別小心處理。 我們可以使用「Abel和式」(Abel summation) 來嚴格證明這個極限過程是合法的。 \\[\n\\begin{align}\n&\\sum_{n=0}^{\\infty} (-1)^n \\frac{t^{2n+1}}{2n+1}  \\\\\n= &\\lim_{N\\to\\infty} \\sum_{n=0}^{N} (-1)^n \\frac{t^{2n+1}}{2n+1}  \\\\\n\\coloneqq &\\lim_{N\\to\\infty} S_N(t)  \\\\\n\\end{align}\n\\] 若此時直接對兩邊取極限 \\(t\\to 1^-\\)，然後交換極限順序，結果會是對的，但我們需要 uniform convergence 來保證這個交換是合法的。\n\n極限之交換定理: 若 \\(S_N(t)\\) 在 \\(t\\in [0,1]\\) 上對 \\(N\\) 一致收斂 (uniform convergence) 到 \\(S_\\infty(t)\\)，且每個 \\(S_N(t)\\) 在 \\(t=1\\) 處連續，則 \\[\n\\lim_{t\\to 1^-} \\lim_{N\\to\\infty} S_N(t) = \\lim_{N\\to\\infty} \\lim_{t\\to 1^-} S_N(t)\n\\]\n\n想像一個反例就是 \\(t^N\\)，所有小於 \\(1\\) 的 \\(t\\)，當 \\(N\\to\\infty\\) 時都會趨近於 \\(0\\)，但在 \\(t=1\\) 處卻永遠是 \\(1\\)。\n也就是說 \\(\\forall \\epsilon&gt;0\\)， 使 \\(|S_n(t) - S_\\infty(t)| &lt; \\epsilon, \\forall n &gt; N_{\\epsilon, t}\\) 的那個 \\(N_{\\epsilon, t}\\)，是否可以和 \\(t\\) 無關。 \\[\n|S_n(t) - S_\\infty(t)|\n\\le \\left| \\sum_{k=n+1}^{\\infty} (-1)^{k}\\frac{t^{2k+1}}{2k+1} \\right|\n\\le \\frac{t^{2n+3}}{2n+3} \\le \\frac{1}{n}\n\\]\n所以我們取 \\(N_{\\epsilon} = \\lceil \\frac{1}{\\epsilon} \\rceil\\) 即可，這個 \\(N_{\\epsilon}\\) 和 \\(t\\) 無關。故極限可以交換。\n是不是有點繞?\n\n\n更快的收斂\n我們回到\\(\\eqref{eq:leibniz}\\)，他收斂很慢的原因我們也感受到了，就是因為級數展開的誤差項在 \\(x=1\\) 大約是 \\(\\frac{1}{N}\\) 的量級。如果我們可以避開 \\(x=1\\)，例如利用 \\(\\arctan(\\pi/6)=1/\\sqrt{3}\\)，帶入 \\(\\eqref{eq:arctan_taylor}\\)，可以得到 \\[\n\\frac{\\pi}{6} = \\sum_{n=0}^{\\infty} (-1)^n \\frac{(\\frac{1}{\\sqrt{3}})^{2n+1}}{2n+1}\n\\] 所以 \\[\n\\frac{\\sqrt{3}\\pi}{6} = \\sum_{n=0}^{\\infty} (-1)^n \\frac{(\\frac{1}{3})^{n}}{2n+1} = \\frac{1}{1} \\cdot \\frac{1}{1} - \\frac{1}{3} \\cdot \\frac{1}{3} + \\frac{1}{5} \\cdot \\frac{1}{3^2} - \\frac{1}{7} \\cdot \\frac{1}{3^3} + \\cdots\n\\] 這個級數的誤差項大約是 \\(\\frac{1}{N 3^N}\\) 的量級，收斂速度快多了!\n讓我們來看看 BBP 公式，從 \\(\\pi/4\\) 出發，但是用了變數變換 \\(u = 1-x\\): \\[\n\\begin{align}\n\\frac{\\pi}{4}\n&= \\int_0^1 \\frac{1}{1+x^2} dx  \\\\\n&= \\int_0^1 \\frac{1}{2 - 2u + u^2} du  \\\\\n&= \\int_0^1 \\frac{2 + 2u + u^2}{4 + u^4} du   \\qquad\\text{(國中學過的因式分解)} \\\\\n\\end{align}\n\\] 兩邊同乘以 \\(4\\)： \\[\n\\begin{align}\n\\pi = \\int_0^1 \\frac{2 + 2u + u^2}{1 + \\frac{u^4}{4}} du \\\\\n\\end{align}\n\\] 我們發現分母 \\(u^4\\) 神奇地被除以 \\(4\\)，這樣把分母用無窮級數展開，會以 \\(\\frac{1}{4^n}\\) 的速度收斂(也是正負交替)。這條路可以走，也可以算出一種級數(可以自己動筆算算)。\n但 BBP 走的是另外一條路。讓我們回到最一開始: \\[\n\\begin{align}\n\\frac{\\pi}{4}\n&= \\int_0^1 \\frac{1}{1+x^2} dx  \\\\\n&= \\int_0^1 \\frac{1+x}{1+x^2} dx - \\int_0^1 \\frac{x}{1+x^2} dx \\qquad\\text{(莫明地拆開)} \\\\\n&= \\int_0^1 \\frac{2 - u}{2 - 2u + u^2} du - \\int_0^1 \\frac{u}{2 - u^2} du  \\qquad\\text{(第二項是用} u^2=1-x^2 \\text{做的代換)} \\\\\n&= \\int_0^1  \\frac{ (2 - u)(2 - u^2) - u(2 - 2u + u^2) }{(2 - 2u + u^2)(2 - u^2)} du  \\qquad\\text{(胡亂通分)} \\\\\n&= \\int_0^1  \\frac{ 4-4u }{(2 - 2u + u^2)(2 - u^2)} du  \\qquad\\text{(神奇地消除??)}  \\\\\n&= \\int_0^1  \\frac{4(1-u)(2+2u+u^2)(2+u^2)}{(4+u^4)(4-u^2)} du  \\qquad\\text{(反因式分解)} \\\\\n&= \\int_0^1  \\frac{4(4-2u^3-u^4-u^5)}{16-u^8} du  \\tag{3}\\label{eq:correct_path} \\\\\n\\end{align}\n\\]\n調整一下，我們得到 \\[\n\\begin{align}\n\\pi\n&= \\int_0^1  \\frac{4-2u^3-u^4-u^5}{1 - \\frac{u^8}{16}} du  \\\\\n&= \\int_0^1 (4-2u^3-u^4-u^5) \\sum_{n=0}^{\\infty} \\left(\\frac{u^8}{16}\\right)^n du  \\qquad\\text{(用幾何級數展開分母)} \\\\\n&= \\sum_{n=0}^{\\infty} \\int_0^1 (4-2u^3-u^4-u^5) \\left(\\frac{u^8}{16}\\right)^n du  \\qquad\\text{(積分和無窮級數交換順序)} \\\\\n&= \\sum_{n=0}^{\\infty} \\frac{1}{16^n} \\int_0^1 (4u^{8n} - 2u^{8n+3} - u^{8n+4} - u^{8n+5}) du  \\\\\n&= \\sum_{n=0}^{\\infty} \\frac{1}{16^n} \\left( \\frac{4}{8n+1} - \\frac{2}{8n+4} - \\frac{1}{8n+5} - \\frac{1}{8n+6} \\right)  \\tag{BBP}\\label{eq:BBP}  \\\\\n\\end{align}\n\\]\n這就是 BBP 公式! 我只能說我也不理解是怎麼發現的，因為中間步驟如果稍微改一下，就會得到另外的公式。雖然也是對的，收斂也很快。\n這邊示範另一條路， \\[\n\\begin{align}\n\\frac{\\pi}{4}\n&= \\int_0^1 \\frac{1}{1+x^2} dx  \\\\\n&= \\int_0^1 \\frac{1-x}{1+x^2} dx + \\int_0^1 \\frac{x}{1+x^2} dx \\qquad\\text{(莫明地拆開)} \\\\\n&= \\int_0^1 \\frac{u}{2 - 2u + u^2} du + \\int_0^1 \\frac{u}{2 - u^2} du  \\qquad\\text{(第二項是用} u^2=1-x^2 \\text{做的代換)} \\\\\n&= \\int_0^1 \\frac{u(2 + 2u + u^2)}{4 + u^4} du  + \\int_0^1 \\frac{u(2+u^2)}{4 - u^4} du  \\qquad\\text{(反因式分解)} \\\\\n&= \\int_0^1 \\frac{2u^2}{4 + u^4} + u(2+u^2) \\left(\\frac{1}{4+u^4}+\\frac{1}{4-u^4} \\right) du   \\\\\n&= \\int_0^1  \\frac{2u^2(4-u^4) + u(2+u^2)\\cdot 8}{(4 + u^4)(4 - u^4)} du \\\\\n&= \\int_0^1  \\frac{16u + 8u^2 + 8u^3 - 2u^6}{16 - u^8} du  \\qquad\\text{(也是對的!?)} \\\\\n\\end{align}\n\\]\n這樣推出來的公式是\n\\[\n\\begin{align}\n\\pi\n&= \\int_0^1  \\frac{4u + 2u^2 + 2u^3 - 1/2u^6}{1 - \\frac{u^8}{16}} du \\\\\n&= \\sum_{n=0}^{\\infty} \\frac{1}{16^n} \\left( \\frac{4}{8n+2} + \\frac{2}{8n+3} + \\frac{2}{8n+4} - \\frac{1}{2(8n+7)} \\right)  \n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/2025-1112-linear_code/index.html",
    "href": "posts/2025-1112-linear_code/index.html",
    "title": "聊聊 Error Correcting Code (ECC) 的基石：Linear Code",
    "section": "",
    "text": "今天我們來聊聊一個在資訊傳輸中超級重要，但也最基礎的概念：Error Correcting Code (ECC)，特別是其中的「Linear Code」（線性碼）。\n\n什麼是 ECC？\n我們先快速回顧一下 ECC 的基本框架。想像一下：\n\nWord Space (W): 這是你想傳送的「原文」空間，例如一串 01 bits。\nCode Space (C): 這是實際在頻道中傳送的「編碼」空間，通常會比 W 更長（因為加入了冗餘）。\n\n我們需要兩個函數：\n\nEncode (編碼): \\(Encode: W \\to C\\)。把你的原文「加密」成更長的編碼。\nDecode (解碼): \\(Decode: C \\to W\\)。把可能出錯的編碼「翻譯」回原文。\n\n我們的世界充滿了雜訊。所以我們假設一個簡單的錯誤模型：傳輸時，每個 bit 都有一個很小（\\(p \\ll 1\\)）的機率 \\(p\\) 會被「翻轉」（0 變 1，1 變 0）。\n一個 ECC 演算法稱得上「好」，就是指它有很高的機率能「抗噪」：\n\n\\(\\mathbb{P}[Decode(Encode(w) + err) = w] \\sim 1\\)\n\n（即使 \\(Encode(w)\\) 在傳輸中被加上了錯誤 \\(err\\)，我們還是能成功還原 \\(w\\)）\n\n用 Hamming Distance 來「具體」衡量\n用機率來定義有點「飄」，我們來談談更具體（concrete）的定義：Hamming distance。\nHamming distance 的定義很直觀：\n\n衡量兩個相同長度的 01 字串，它們「有幾個位置上的 bit 不一樣」。\n\n例如，\\(\\text{Dist}(01101, 00111) = 2\\)。\n如果我們把這個空間看作 \\((\\mathbb{Z}/2\\mathbb{Z})^n\\)（也就是每個元素都是 0 或 1 的 n 維向量空間），這個 distance 顯然滿足三角不等式。\n重點來了。如果一個編碼 \\(C\\)（也就是 \\(Encode\\) 函數的 image），裡面任兩個相異的 codeword，它們的 Hamming distance 都大於等於 \\(2t+1\\)（\\(t\\) 是某個正整數），那麼：\n這個 ECC 就可以 100% 成功修正（correct）最多 \\(t\\) 個 bits 的錯誤。\n這也很直觀：想像每個 codeword 都是一個中心，各自畫出一個半徑為 \\(t\\) 的「勢力範圍」。\\(2t+1\\) 的距離保證了這些「勢力範圍」彼此不會重疊。當你收到一個有 \\(\\le t\\) 個錯誤的訊息時，它雖然偏離了「正確答案」，但它仍然落在「正確答案」的勢力範圍內，而不會跑進「其他答案」的範圍裡。\n（反之，如果存在兩個 codeword 距離 \\(\\le 2d\\)，那當錯誤剛好發生在中間時，你就無法 100% 確定該 decode 成哪一個了。）\n\n\n\n什麼是 Linear Code？\nOK，複習完畢。那什麼是 Linear Code 呢？\n它為 ECC 帶來了漂亮的代數結構。 所謂 Linear Code，就是指 \\(W\\) 和 \\(C\\) 都是 \\(\\mathbb{Z}/2\\mathbb{Z}\\) 上的向量空間（分別是 \\(k\\) 維和 \\(n\\) 維，通常 \\(k &lt; n\\)），並且 Encode 函數是一個線性函數（Linear Function）。\n也就是說，它滿足： \\[Encode(w_1 + w_2) = Encode(w_1) + Encode(w_2)\\]\n特別提醒： 這裡的「加法」非常重要。在 \\((\\mathbb{Z}/2\\mathbb{Z})^n\\) 的向量空間中，我們談論的加法是逐位元 (bit-wise) 的 XOR（互斥或）。 簡單說，就是 \\(1+1 = 0\\)，不進位。\n使用 Linear Code 的好處非常多，例如 \\(Encode(0) = 0\\) 永遠成立，而且整個 Code Space C 會是一個 \\(n\\) 維空間中的 \\(k\\) 維子空間 (subspace)，這讓分析和計算都變得異常方便。\n\n\nLinear Code 的基本限制 (The Singleton Bound)\n好，終於來到今天的主角：一個衡量 Linear Code 效率的基本不等式。\n我們來定義幾個關鍵數字：\n\n\\(n = dim(C)\\)：Codeword 的長度（\\(C\\) 空間的維度）。\n\\(k = dim(W)\\)：Word 的長度（\\(W\\) 空間的維度）。\n\\(d\\)：這個 Code 的最小 Hamming distance（也就是 \\(C\\) 中任兩個相異 codeword 之間距離的最小值）。\n\n(註: 我們稱滿足這樣條件的code為 \\((n, k)\\)-code, 或是 \\((n, k, d)\\)-code)\n那麼，這三個數字之間存在一個「鐵三角」限制，你不能什麼都要。這個最基礎的上限（upper bound）之一，就是 Singleton Bound：\n\n\\[k + d \\le n + 1\\]\n\n這個不等式告訴我們一個殘酷的現實： 你不可能同時擁有極高的資訊率（\\(k\\) 很大，接近 \\(n\\)）和極強的糾錯能力（\\(d\\) 很大）。\n\n想增加 \\(k\\)（傳更多資訊）？你就必須犧牲 \\(d\\)（糾錯能力下降）。\n想增加 \\(d\\)（抵抗更多錯誤）？你就必須犧牲 \\(k\\)（傳輸效率下降），或者…\n…把 \\(n\\) 變得非常大（用更長的編碼），這會帶來額外的傳輸成本。\n\n這就是 ECC 世界中第一個，也是最重要的一個 trade-off。\n註: 若用剛才的勢力範圍解釋，每個codeword可以有大小為 \\(2^{\\frac{d-1}{2}}\\) 的生得領域，會得到一個除以2的版本: \\[\n\\begin{align}\n2^{k} \\cdot 2^{\\frac{d-1}{2}} &\\le 2^n  \\\\\n\\implies k + \\frac{d-1}{2} &\\le n\n\\end{align}\n\\] 而這個singleton bound比這個還要更強。證明意外的簡單: 因為把C的前d-1個bits給遮起來的話，所有的codeword必須相異，所以 \\[\n2^k = |W| \\le 2^{n-d+1}\n\\] 故得證。\n\n\n\n生成矩陣 (Generator Matrix)\n因為線性函數的行為是完全被基底所決定，如果說 word space \\(W\\) 是 \\(k\\) 維，code space \\(C\\) 是 \\(n\\) 維，那麼我們的Encode函數其實就等價於給一個 \\(k \\times n\\) 的生成矩陣 \\(G\\)。編碼過程就是一個簡單的矩陣乘法： \\[c = wG\\]\n\n\\(w\\) 是一個 \\(1 \\times k\\) 的 row vector (你的原文)。\n\\(c\\) 是一個 \\(1 \\times n\\) 的 row vector (生成的 codeword)。\n(所有運算都在 \\(\\mathbb{Z}/2\\mathbb{Z}\\) 上，也就是 XOR，不進位加法)。\n\n例子 1：(3, 1, 3) Repetition Code\n這是最簡單的 code：把一個 bit 重複三次。\n\n\\(W\\) (訊息): \\(k=1\\)。 (例如 \\(w = [w_1]\\))\n\\(C\\) (編碼): \\(n=3\\)。 (例如 \\(c = [c_1, c_2, c_3]\\))\n編碼規則: \\(0 \\to 000\\), \\(1 \\to 111\\)。\nGenerator Matrix (\\(G\\)): 我們需要一個 \\(1 \\times 3\\) 的矩陣 \\(G\\)，使得 \\(wG = c\\)。 當 \\(w=[1]\\) 時，我們要 \\(c=[1, 1, 1]\\)。 所以 \\(G = [1, 1, 1]\\)。 (驗證：\\([0] \\times [1, 1, 1] = [0, 0, 0]\\)。 \\([1] \\times [1, 1, 1] = [1, 1, 1]\\)。正確！)\nParameters:\n\n\\(n=3\\) (codeword 長度)\n\\(k=1\\) (message 長度)\n\\(d=3\\) (最小距離， \\(d(000, 111) = 3\\))\n\nSingleton Bound 檢查: \\[\n\\begin{align}\n              k + d &\\le n + 1  \\\\\n\\implies \\quad   1 + 3 &\\le 3 + 1\n\\end{align}\n\\] 結論： 這個 code 剛剛好 “tight”（緊密）地滿足了這個不等式！這類 code 稱為 MDS (Maximum Distance Separable) code，它們在 \\(n, k\\) 固定的情況下，達到了 \\(d\\) 的理論最大值。\n\n例子 2： (7, 4, 3) Hamming Code\n讓我們來看一個更實用、更強大，但是沒有 tight 的例子：(7, 4) Hamming Code。\n\n\\(W\\) (訊息): \\(k=4\\)。 ( \\(w = [w_1, w_2, w_3, w_4]\\) )\n\\(C\\) (編碼): \\(n=7\\)。 ( \\(c = [c_1, ... c_7]\\) )\nGenerator Matrix (\\(G\\)): 這是一個 \\(4 \\times 7\\) 矩陣。我們常用一種「系統性 (systematic)」的形式，它的結構是 \\(G = [I_k | P]\\)，其中 \\(I_k\\) 是 \\(k \\times k\\) 的單位矩陣， \\(P\\) 是一個 \\(k \\times (n-k)\\) 矩陣。\n這樣做的好處是，編碼後的前 \\(k\\) 個 bits 就是原文 \\(w\\)，後面 \\(n-k\\) 個 bits 才是「校驗位 (parity bits)」。\n一個 (7, 4) Hamming Code 的標準 \\(G\\) 矩陣是： \\[\n  G = \\begin{bmatrix}\n  1 & 0 & 0 & 0 & 1 & 1 & 0 \\\\\n  0 & 1 & 0 & 0 & 1 & 0 & 1 \\\\\n  0 & 0 & 1 & 0 & 0 & 1 & 1 \\\\\n  0 & 0 & 0 & 1 & 1 & 1 & 1\n  \\end{bmatrix}\n  \\]\n\n(解讀： \\(c_1=w_1, c_2=w_2, c_3=w_3, c_4=w_4\\)。而 \\(c_5 = w_1+w_2+w_4\\), \\(c_6 = w_1+w_3+w_4\\), \\(c_7 = w_2+w_3+w_4\\))\n\nParameters:\n\n\\(n=7\\)\n\\(k=4\\)\n\\(d=3\\) (這個 code 的最小距離是 3。這保證了它可以修正 1 bit 的錯誤)\n\nSingleton Bound 檢查: \\[\n\\begin{align}\n              k + d &\\le n + 1  \\\\\n\\implies \\quad   4 + 3 &\\le 7 + 1\n\\end{align}\n\\] 結論： \\(7 &lt; 8\\)。這個 code 就沒有 tight 這個不等式。\n這告訴我們，Singleton Bound 確實只是一個「上界」，許多非常優秀且實用的 code（像 Hamming code）並不會剛好壓在那條線上。\n\n\n\n\n怎麼快速的「反解」？ (Error Correction)\n如果 \\(c = wG\\)，且傳輸沒有錯誤，那反解很簡單：因為 \\(G\\) 是系統性的 \\(G=[I_4 | P]\\)，我們收到的 \\(c\\) 的前 4 個 bits 就是 \\(w\\)。\n但重點是如果出錯了呢？\n我們收到的 \\(r\\) (received vector) 可能不等於 \\(c\\)。 \\[\nr = c + e \\quad(e \\text{是 error vector})\n\\]\n這時，我們就要用一個神奇的反解矩陣叫做 Parity-Check Matrix (奇偶校驗矩陣)，記為 \\(H\\)。\n\\(H\\) 是一個 \\((n-k) \\times n\\) 矩陣（對 (7, 4) code 來說，就是 \\(3 \\times 7\\)）。 它和 \\(G\\) 有一個非常漂亮的「正交」關係： \\[GH^T = 0\\]\n這意味著，任何一個合法的 codeword \\(c\\)，都滿足： \\[cH^T = 0\\]\n用 H 來抓出錯誤\n當我們收到 \\(r\\) 時，我們立刻計算一個東西，叫做 Syndrome (伴隨式)，記為 \\(S\\)：\n\\[S = rH^T\\]\n現在，神奇的事情發生了。我們把 \\(r = c + e\\) 代入：\n\\(S = (c + e)H^T = cH^T + eH^T\\)\n因為 \\(cH^T = 0\\)（c 是合法 codeword），所以：\n\\[S = eH^T\\]\n這就是解碼的關鍵！\n\n\\(S\\) 的值只跟 error \\(e\\) 有關，跟原始訊息 \\(w\\) 或 \\(c\\) 完全無關！\n如果 \\(S = 0\\)，代表 \\(e=0\\)（或 \\(e\\) 剛好是另一個合法的 codeword，機率很低），我們假設沒有錯誤。\n如果 \\(S \\neq 0\\)，代表發生了錯誤。\n\n對於 Hamming Code 這種「完美」的 code， \\(S\\) 的值和「發生 1-bit 錯誤的位置」是一一對應的。\n(7, 4) Hamming code 的 \\(H\\) 矩陣（對應上面那個 \\(G\\)）是： \\[\nH = \\begin{bmatrix}\n1 & 1 & 0 & 1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 1 & 0 \\\\\n0 & 1 & 1 & 1 & 0 & 0 & 1\n\\end{bmatrix}\n\\] ( \\(H = [P^T | I_{n-k}]\\) )\n如果你計算 \\(S = eH^T\\)，你會發現：\n\n如果 \\(e = [1,0,0,0,0,0,0]\\) (第1 bit 錯)，\\(S = [1, 1, 0]\\)。\n如果 \\(e = [0,1,0,0,0,0,0]\\) (第2 bit 錯)，\\(S = [1, 0, 1]\\)。\n…\n如果 \\(e = [0,0,0,0,0,0,1]\\) (第7 bit 錯)，\\(S = [0, 0, 1]\\)。\n\n每個 1-bit 錯誤都會產生一個獨一無二的 \\(S\\)。我們只要建立一個「\\(S\\) -&gt; \\(e\\)」的對照表，收到 \\(r\\) -&gt; 計算 \\(S\\) -&gt; 查表得到 \\(e\\) -&gt; \\(c = r + e\\)（XOR 回去） -&gt; 讀出 \\(c\\) 的前 \\(k\\) bits，就成功解碼 \\(w\\) 了！\n來寫一下 \\(GH^T\\) \\[\nGH^T = \\begin{bmatrix}\n    1 & 0 & 0 & 0 & 1 & 1 & 0 \\\\\n    0 & 1 & 0 & 0 & 1 & 0 & 1 \\\\\n    0 & 0 & 1 & 0 & 0 & 1 & 1 \\\\\n    0 & 0 & 0 & 1 & 1 & 1 & 1\n    \\end{bmatrix}\n    \\begin{bmatrix}\n    1 & 1 & 0 \\\\\n    1 & 0 & 1 \\\\\n    0 & 1 & 1 \\\\\n    1 & 1 & 1 \\\\\n    1 & 0 & 0 \\\\\n    0 & 1 & 0 \\\\\n    0 & 0 & 1\n    \\end{bmatrix} =\n    \\begin{bmatrix}\n    0 & 0 & 0 \\\\\n    0 & 0 & 0 \\\\\n    0 & 0 & 0 \\\\\n    0 & 0 & 0\n    \\end{bmatrix}\n\\]\n\n\n\\(H\\) 的誕生：來自 \\(C\\) 的對偶空間\n對於任意的 \\((n, k, d)\\) 生成矩陣 \\(G\\)，如果這個 \\(H\\) 要存在的話，所有 \\(H\\) 行向量 (row vectors) 跟每個 \\(G\\) 的行向量內積都必須是 0。而 \\(G\\) 有 \\(k\\) 個行向量，它們生成了 \\(C\\)。所以直觀上來說，\\(H\\) 的 \\((n-k)\\) 個行向量所生成的空間，就是 \\(G\\) 的行向量空間 (Row Space) 的「正交補空間」。\n但是！這邊要特別小心。因為在 \\(\\mathbb{F}_2\\) 裡面做內積，跟一般實數的內積雖然形式極相似，但最大的差別是，一個非零向量自己跟自己的內積不一定大於 0！（應該說在 \\(\\mathbb{F}_2\\) 裡本來就沒有「大於0」這個概念，只有是否為 0）。\n如果一個向量有偶數個 1，例如 \\(\\mathbf{v} = [1, 1] \\in \\mathbb{F}_2^2\\)，自己跟自己內積就是 \\(1 \\cdot 1 + 1 \\cdot 1 = 1 + 1 = 0\\)。 這個向量居然與自己「正交」！\n這導致了一個在 \\(\\mathbb{R}^n\\) 中不會發生的奇特現象：一個子空間 \\(C\\) 和它的「正交補空間」 \\(C^\\perp\\) 是可以重疊的。在 \\(\\mathbb{F}_2^2\\) 中，由 \\([1, 1]\\) 所生成的子空間 \\(C\\)，其 \\(C^\\perp\\) 就是它自己！\n所以，\\(C\\) 和 \\(C^\\perp\\) 的直和 (Direct Sum) 不一定是整個 \\(\\mathbb{F}_2^n\\) 空間： \\[C \\oplus C^\\perp \\neq \\mathbb{F}_2^n\\] 欸，但是，雖然正交空間不一定是我們直觀上的「補空間」，一個來自線性代數的關鍵定理拯救了我們：維度公式依然成立！\n\\[\\text{dim}(C) + \\text{dim}(C^\\perp) = n\\] 這個事實，就是我們構建 \\(H\\) 的數學保證。\n\n我們從 \\(G\\) 出發。\\(G\\) 是一個 \\(k \\times n\\) 矩陣，它的 \\(k\\) 個行向量 (row vectors) 生成 (span) 了 \\(C\\)。因此，\\(C\\) 是 \\(\\mathbb{F}_2^n\\) 中一個維度為 \\(k\\) 的子空間。\n接著，我們定義 \\(C\\) 的對偶碼 (Dual Code) \\(C^\\perp\\)： \\[C^\\perp = \\{ \\mathbf{v} \\in \\mathbb{F}_2^n \\mid \\mathbf{c} \\cdot \\mathbf{v}^T = 0 \\text{ for all } \\mathbf{c} \\in C \\}\\] (這裡 \\(\\mathbf{c} \\cdot \\mathbf{v}^T\\) 是指內積。)\n由於 \\(G\\) 的行向量是 \\(C\\) 的一組基底，一個向量 \\(\\mathbf{v}\\) 屬於 \\(C^\\perp\\) 的充分必要條件是：\\(\\mathbf{v}\\) 與 \\(G\\) 的每一個行向量都正交。 \\[\\mathbf{v} \\in C^\\perp \\iff G \\mathbf{v}^T = \\mathbf{0}_{k \\times 1}\\] (註：\\(G\\) 是 \\(k \\times n\\)，\\(v^T\\) 是 \\(n \\times 1\\)，結果是 \\(k \\times 1\\) 的零向量)\n現在，我們使用關鍵的維度定理：\\(\\dim(C) + \\dim(C^\\perp) = n\\)。 代入 \\(\\dim(C) = k\\)，我們得到： \\[\\dim(C^\\perp) = n - k\\] \\(C^\\perp\\) 是一個維度為 \\(n-k\\) 的子空間。既然是子空間，它就必然存在一組基底。\n我們就此定義 \\(H\\)：\n\n定義： 奇偶校驗矩陣 \\(H\\) (Parity Check Matrix) 是一個 \\((n-k) \\times n\\) 矩陣，它的 \\(n-k\\) 個行向量 (row vectors) 構成了 \\(C^\\perp\\) 的一組基底。\n\n\n\n\\(H\\) 的兩大特性\n這個 \\(H\\) 完美地滿足了 linear code 所需的一切。\n特性一：\\(GH^T = 0\\) (與 \\(G\\) 的正交性)\n\\(H^T\\) 是一個 \\(n \\times (n-k)\\) 矩陣，它的 \\(n-k\\) 個列向量 (column vectors) \\(\\mathbf{h}_j^T\\) 就是 \\(C^\\perp\\) 的基底。 \\(G\\) 是一個 \\(k \\times n\\) 矩陣，它的 \\(k\\) 個行向量 (row vectors) \\(\\mathbf{g}_i\\) 是 \\(C\\) 的基底。\n當我們計算矩陣乘積 \\(GH^T\\) 時，其第 \\((i, j)\\) 個元素的值是： \\[(GH^T)_{ij} = (\\text{Row } i \\text{ of } G) \\cdot (\\text{Column } j \\text{ of } H^T) = \\mathbf{g}_i \\cdot \\mathbf{h}_j^T\\]\n\n\\(\\mathbf{g}_i \\in C\\)\n\\(\\mathbf{h}_j^T\\) 的轉置 \\(\\mathbf{h}_j \\in C^\\perp\\)\n\n根據 \\(C^\\perp\\) 的定義， \\(C\\) 中的向量 \\(\\mathbf{g}_i\\) 與 \\(C^\\perp\\) 中的向量 \\(\\mathbf{h}_j\\) 內積必然為 0。 因此，\\((GH^T)_{ij} = 0\\) 對所有 \\(i, j\\) 都成立。 得證：\\(GH^T = 0_{k \\times (n-k)}\\)。\n特性二：\\(C = \\text{NullSpace}(H)\\) (C 的另一種定義)\n\\(H\\) 不只跟 \\(G\\) 正交，它還提供了另一種描述 \\(C\\) 的方式。 一個向量 \\(\\mathbf{c} \\in \\mathbb{F}_2^n\\) 是碼字 (codeword)，若且唯若 (if and only if)： \\[\\mathbf{c} H^T = \\mathbf{0}_{1 \\times (n-k)}\\] * \\(\\mathbf{c} (1 \\times n) \\times H^T (n \\times (n-k)) \\to \\mathbf{0} (1 \\times (n-k))\\)\n證明： \\(\\mathbf{c} H^T = \\mathbf{0}\\) 意味著 \\(\\mathbf{c}\\) 與 \\(H^T\\) 的所有行向量 (columns) 內積為 0。而 \\(H^T\\) 的行向量就是 \\(C^\\perp\\) 的基底。 如果 \\(\\mathbf{c}\\) 與 \\(C^\\perp\\) 的所有基底都正交，那它就與 \\(C^\\perp\\) 中的所有向量都正交。 哪些向量會與 \\(C^\\perp\\) 中的所有向量都正交？答案就是 \\((C^\\perp)^\\perp\\) 裡面的向量。 而在 \\(\\mathbb{F}_2\\)（以及所有有限體）中，我們有 \\((C^\\perp)^\\perp = C\\)。 因此，\\(\\mathbf{c} H^T = \\mathbf{0} \\iff \\mathbf{c} \\in C\\)。\n\\(G\\) 用來生成 \\(C\\)，\\(H\\) 用來檢驗 \\(C\\)。\n\n\n\n\\(H\\) 的真正威力：症候群解碼 (Syndrome Decoding)\n好了，我們證明了 \\(H\\) 的存在。那它到底有什麼用？ \\(H\\) 的存在，讓我們擁有了「解碼」的能力。\n假設我們傳送了碼字 \\(\\mathbf{c}\\) ( \\(\\in C\\) )。 在傳輸過程中，發生了錯誤，錯誤向量為 \\(err\\)。 我們收到的向量是 \\(\\mathbf{r} = \\mathbf{c} + err\\)。\n我們只拿得到 \\(\\mathbf{r}\\)。我們不知道 \\(\\mathbf{c}\\) 也還不知道 \\(err\\)。 這時，\\(H\\) 登場了。我們計算 \\(\\mathbf{r}\\) 的Syndrome \\(S\\)：\n\\[S = \\mathbf{r} H^T\\] \\(S\\) 是一個 \\(1 \\times (n-k)\\) 的向量。我們把它展開： \\[S = (\\mathbf{c} + err) H^T = \\mathbf{c} H^T + err H^T\\] 根據「特性二」，因為 \\(\\mathbf{c}\\) 是個合法的碼字，我們知道 \\(\\mathbf{c} H^T = \\mathbf{0}\\)。 所以上式變為： \\[S = \\mathbf{0} + err H^T = err H^T\\] 這就是 \\(H\\) 最神奇的地方！\n\n症候群 \\(S\\) 完全由錯誤向量 \\(err\\) 決定，而與原始碼字 \\(\\mathbf{c}\\) 無關。\n\n\n如果 \\(S = \\mathbf{0}\\)，這意味著 \\(err H^T = \\mathbf{0}\\)。\n\n這可能代表 \\(err = \\mathbf{0}\\) (沒有錯誤)。\n但也可能代表 \\(err\\) 剛好也是一個合法的碼字 \\(err \\in C\\) (我們衰到剛好被錯誤推進了另一個碼字)。\n\n如果 \\(S \\neq \\mathbf{0}\\)，我們可以 100% 確定：有錯誤發生！\n\n\n\n定義解碼函數 \\(h(S)\\)\n我們的目標是從 \\(S\\) 反推出 \\(err\\)。 這就是 \\(h\\) 函數，它是整個解碼策略的核心。\n\n定義： 解碼函數 \\(h\\) 是一個映射 \\(h: \\mathbb{F}_2^{n-k} \\to \\mathbb{F}_2^n\\)，它接受一個症候群 \\(S\\)，並輸出一個「最有可能」的錯誤向量 \\(\\hat{err}\\)。\n\\[\\hat{err} = h(S)\\]\n\n「最有可能」是什麼意思？ 在標準的二元對稱通道 (BSC) 中，我們假設錯誤是隨機且獨立發生的。這代表發生 1 個 bit 錯誤的機率，遠大於發生 2 個 bit 錯誤；2 個又遠大於 3 個… 所以，「最有可能」的錯誤向量，就是具有最小漢明權重 (Minimum Hamming Weight) 的那個。\n對於一個給定的 \\(S\\)，滿足 \\(err H^T = S\\) 的 \\(err\\) 可能有很多個。 例如，如果 \\(\\mathbf{e}_1\\) 滿足 \\(\\mathbf{e}_1 H^T = S\\)，那麼 \\(\\mathbf{e}_1 + \\mathbf{c}\\)（其中 \\(\\mathbf{c} \\in C\\)）也會滿足： \\((\\mathbf{e}_1 + \\mathbf{c})H^T = \\mathbf{e}_1 H^T + \\mathbf{c} H^T = S + \\mathbf{0} = S\\)。 所有這些會產生同一個症候群 \\(S\\) 的向量集合，稱為 \\(C\\) 的一個陪集 (Coset)。\n\\(h(S)\\) 的任務，就是在這個陪集中，找出那個漢明權重最小的向量，我們稱之為陪集首領 (Coset Leader)。\n\n\\(h(S)\\) 的正式定義： \\(h(S) = \\hat{err}\\)，其中 \\(\\hat{err}\\) 是集合 \\(\\{ \\mathbf{e} \\in \\mathbb{F}_2^n \\mid \\mathbf{e} H^T = S \\}\\) 中，具有最小漢明權重的向量。 (如果有多個最小權重向量，通常任選一個，例如按字典序排第一個)\n\n\n\n完整的解碼流程\n有了 \\(H\\) 和 \\(h(S)\\)，解碼流程如下：\n\n接收： 收到 \\(\\mathbf{r}\\)。\n計算Syndrome： \\(S = \\mathbf{r} H^T\\)。\n查找錯誤： \\(\\hat{err} = h(S)\\)。( \\(h\\) 函數通常是預先算好存成一個查找表，稱為 Syndrome Look-up Table )\n修正錯誤： \\(\\hat{\\mathbf{c}} = \\mathbf{r} + \\hat{err}\\)。(在 \\(\\mathbb{F}_2\\) 中，加法和減法一樣)\n解碼訊息： 從 \\(\\hat{\\mathbf{c}}\\) 反解出 \\(\\hat{\\mathbf{w}}\\) (例如，若 \\(G\\) 是系統碼，直接取前 \\(k\\) 個 bits)。\n\n\n\n\\(h(S)\\) 何時會成功？\n這個解碼流程能成功的前提是：我們猜的 \\(\\hat{err}\\) 就是 實際發生的 \\(err\\)。 (\\(\\hat{err} = err\\))\n而 \\(h(S)\\) 策略是「猜最小權重」。所以，只要實際發生的 \\(err\\) 剛好就是它所屬陪集 (Coset) 裡的那個最小權重向量 (Coset Leader)，解碼就會成功！\n這就回到了 \\((n, k, d)\\) 中的 \\(d\\) (最小距離)： 一個 code 的最小距離為 \\(d\\)，代表它能保證修正 \\(t = \\lfloor \\frac{d-1}{2} \\rfloor\\) 個錯誤。 這句話的數學意義是：所有權重 \\(wt(err) \\le t\\) 的錯誤向量 \\(err\\)，都必然是它們各自陪集中的唯一首領。\n因此，只要實際發生的錯誤數量 \\(|err| \\le \\lfloor \\frac{d-1}{2} \\rfloor\\)， \\(h(S)\\) 函數就保證能找到正確的 \\(err\\)，解碼也保證會成功！"
  },
  {
    "objectID": "posts/2025-1116-shortest_vector/index.html",
    "href": "posts/2025-1116-shortest_vector/index.html",
    "title": "最短的向量",
    "section": "",
    "text": "從無窮多組解中，找一組最小的\n假設在 \\(\\mathbb{R}^n\\) 空間中有 \\(n\\) 個向量 \\(v_1, ..., v_n\\)，要找他們的整係數線性組合，使得其長度最小(但非零)。 \\[\n\\begin{array}{cl}\n\\min & \\Big\\| v \\Big\\|_2 \\\\\n\\text{s.t.} & v = \\sum_{i=1}^n a_i v_i, \\quad a_i \\in \\mathbb{Z}, \\quad v \\neq 0\n\\end{array}\n\\]\n也可以考慮個整數版本的，但向量的個數要比 \\(n\\) 多一些。假設 \\(v_1, ..., v_m\\) 是 \\(\\mathbb{Z}_q^n\\) 中的 \\(m\\) 個向量。\n\\[\n\\begin{array}{cl}\n\\min & \\sum_{i=1}^m a_i^2 \\\\\n\\text{s.t.} & \\sum_{i=1}^m a_i v_i \\equiv 0 (mod q), \\quad a_i \\in \\mathbb{Z}, \\exists a_i \\neq 0\n\\end{array}\n\\]\n在這個情形中，我們反過來是要求向量合為零時，最小的係數是多少。"
  },
  {
    "objectID": "posts/normal-distribution/day_7.html",
    "href": "posts/normal-distribution/day_7.html",
    "title": "一天證明一個 Normal Distribution 的性質 Day7：Hermite Polynomials",
    "section": "",
    "text": "今天回到我們的小數學風格，談點輕鬆的 Hermite Polynomials。\n如同其他正交多項式，Hermite polynomial也是一組正交多項式。我們來看一下他的生成函數 (generating function)： \\[\ne^{xt - \\frac{t^2}{2}} := \\sum_{n=0}^{\\infty} \\text{He}_n(x) \\frac{t^n}{n!}  \\tag{1}\\label{eq:hermite-generating}\n\\]\n欸，這個生成函數根本就是常態分佈 \\(\\mathcal{N}(x,1)\\) 的 MGF (moment generating function)！\n所以說，第 \\(n\\) 階 \\(\\text{He}_n(x)\\) 的定義其實就是常態分佈 \\(\\mathcal{N}(x,1)\\) 的 n-th moment 乘上 \\(n!\\)。\n這邊我們用的是 機率學家的埃爾米特多項式 (probabilists’ Hermite polynomials)，另外還有物理學家的 (physicists’ Hermite polynomials)，記做 \\(H_n(x)\\)，其中 \\(H_n(x) = 2^{\\frac{n}{2}} \\text{He}_n(\\sqrt{2}x)\\)，也就是在定義上差了兩倍的標準差。本質上是一樣的，所以我們先專注在前者。\n\n\n\n\\(n\\)\n機率學家的 \\(\\text{He}_n(x)\\)\n物理學家的 \\(H_n(x)\\)\n\n\n\n\n0\n1\n1\n\n\n1\n\\(x\\)\n\\(2x\\)\n\n\n2\n\\(x^2 - 1\\)\n\\(4x^2 - 2\\)\n\n\n3\n\\(x^3 - 3x\\)\n\\(8x^3 - 12x\\)\n\n\n4\n\\(x^4 - 6x^2 + 3\\)\n\\(16x^4 - 48x^2 + 12\\)\n\n\n\n\n\nfunction He(n, x) {\n  if (n === 0) return 1;\n  if (n === 1) return x;\n  if (n === 2) return x**2 - 1;\n  if (n === 3) return x**3 - 3*x;\n  if (n === 4) return x**4 - 6*x**2 + 3;\n  return 0;\n}\n\n// 生成數據點 (-4 到 4)\ndata = {\n  const points = [];\n  for (let x = -5; x &lt;= 5; x += 0.05) {\n    [0, 1, 2, 3, 4].forEach(n =&gt; {\n      points.push({\n        x: x,\n        y: He(n, x),\n        n: `He${n}` // 分組標籤\n      });\n    });\n  }\n  return points;\n}\n\n// 繪圖\nPlot.plot({\n  // title: \"Probabilistic Hermite Polynomials (He0 - He4)\",\n  grid: true,\n  caption: null, // 移除 Observable Plot 的版權標記\n  y: { domain: [-6, 6], label: \"He_n(x)\" },\n  color: { legend: true }, // 自動生成圖例\n  marks: [\n    Plot.ruleY([0]), // X軸基準線\n    Plot.line(data, {\n      x: \"x\", \n      y: \"y\", \n      stroke: \"n\", \n      strokeWidth: 2 \n    })\n  ]\n})\n\n\n\n\n\n\n(a) ?(caption)\n\n\n\n\n\n\n\n\n\n(b) ?(caption)\n\n\n\n\n\n\n\n\n\n(c) ?(caption)\n\n\n\nFigure 1: 機率學家的 Hermite 多項式 (He₀ - He₄)\n\n\n\n計算 Hermite 多項式\n對 \\(x\\) 微分:\n若將 \\(\\eqref{eq:hermite-generating}\\) 兩邊對 \\(x\\) 微分，我們可以得到一個遞迴關係式： \\[\n\\begin{align}\n&\\frac{\\partial}{\\partial x} e^{xt - \\frac{t^2}{2}} = t e^{xt - \\frac{t^2}{2}} \\\\\n\\implies &\\sum_{n=0}^{\\infty} \\text{He}_n'(x) \\frac{t^n}{n!} = \\sum_{n=0}^{\\infty} \\text{He}_{n}(x) \\frac{t^{n+1}}{n!} = \\sum_{n=1}^{\\infty} \\text{He}_{n-1}(x) \\frac{t^{n}}{(n-1)!}\n\\end{align}\n\\] 所以我們有： \\[\n\\text{He}_n'(x) = n \\text{He}_{n-1}(x) \\tag{2}\\label{eq:hermite-recursion}\n\\] 這就足以讓我們依序積分計算出 \\(n\\) 階的 Hermite 多項式，但還差常數項。 而常數項 \\(\\text{He}_n(0)\\) 可以直接從原本的生成函數 \\(\\eqref{eq:hermite-generating}\\) 帶入 \\(x=0\\) 得到： \\[\ne^{-\\frac{t^2}{2}} = \\sum_{n=0}^{\\infty} \\text{He}_n(0) \\frac{t^n}{n!}\n\\] 注意到左邊其實是偶函數，所以所有奇數階的 Hermite 多項式在 \\(x=0\\) 都是 0。 而偶數階的話，我們可以將指數函數展開： \\[\ne^{-\\frac{t^2}{2}} = \\sum_{k=0}^{\\infty} \\frac{(-1)^k (t^2/2)^k}{k!} = \\sum_{k=0}^{\\infty} \\frac{(-1)^k t^{2k}}{2^k k!}\n\\] 所以 \\[\n\\text{He}_{2k}(0) = \\frac{(-1)^k (2k)!}{2^k k!}, \\quad \\text{He}_{2k+1}(0) = 0  \\tag{3}\\label{eq:hermite-constant}\n\\]\n但從這個遞迴式實在看不出為什麼 Hermite 多項式一定是整係數。畢竟一直積分是可能會產生分母的。\n對 \\(t\\) 微分:\n應該來試試看對 \\(t\\) 微分會得到什麼？\n\\[\n\\begin{align}\n&\\frac{\\partial}{\\partial t} e^{xt - \\frac{t^2}{2}} = (x - t) e^{xt - \\frac{t^2}{2}}   \\\\\n\\implies &\\sum_{n=0}^{\\infty} \\text{He}_n(x) \\frac{t^{n-1}}{(n-1)!} = \\sum_{n=0}^{\\infty}  x \\text{He}_n(x) \\frac{t^n}{n!} - \\sum_{n=0}^{\\infty} \\text{He}_n(x) \\frac{t^{n+1}}{n!}  \\\\\n\\implies &\\sum_{n=0}^{\\infty} \\text{He}_{n+1}(x) \\frac{t^{n}}{n!} = \\sum_{n=0}^{\\infty}  x \\text{He}_n(x) \\frac{t^n}{n!} - \\sum_{n=1}^{\\infty} \\text{He}_{n-1}(x) \\frac{t^{n}}{(n-1)!} \\\\\n\\end{align}\n\\] 所以我們有另一個遞迴關係式： \\[\n\\text{He}_{n+1}(x) = x \\text{He}_n(x) - n \\text{He}_{n-1}(x) \\tag{4}\\label{eq:hermite-recursion-2}\n\\] 我們再用剛才的的遞迴式 \\(\\eqref{eq:hermite-recursion}\\) 把 \\(n\\text{He}_{n-1}(x)\\) 換成微分： \\[\n\\text{He}_{n+1}(x) = x \\text{He}_n(x) - \\text{He}_n'(x) = \\left( x - \\frac{d}{dx} \\right) \\text{He}_n(x) = \\left( x - \\frac{d}{dx} \\right)^{n+1} \\cdot 1  \\tag{5}\\label{eq:hermite-operator}\n\\] 疑? 原本 \\(\\eqref{eq:hermite-recursion}\\) 告訴我們要算 \\(n\\) 階 Hermite 多項式是一直積分，現在變成了一直乘以 \\(x\\) 和微分!\n至少我們可以確定，Hermite 多項式的係數一定是整數，因為每一步運算都不會產生分母。\n既然是整數，是否可以問他的係數的組合數學的意義? 從 \\(\\eqref{eq:hermite-operator}\\) 出發，我們可以展開 \\((x - \\frac{d}{dx})^{n}\\)： \\[\n\\text{He}_n(x) = \\sum_{k=0}^{n} \\binom{n}{k} x^{n-k} \\left(-\\frac{d}{dx}\\right)^{k} \\cdot 1 = \\sum_{k=0}^{n} \\binom{n}{k} x^{n-k} (-1)^k \\frac{d^k}{dx^k} 1  = x^{n}\n\\tag{Wrong!}\n\\] 因為 \\(\\frac{d^k}{dx^k} 1 = 0\\) 當 \\(k \\geq 1\\)，所以只有 \\(k=0\\) 的項會留下來，這似乎跟我們之前的結論矛盾? 當然不是!! 因為乘以 \\(x\\) 和 微分作用是不能交換的，所以不能套用二項式展開式。\n\nP.S. 組合數意義:\n\\(\\text{He}_n(x)\\) 的係數其實對應於 完全圖 (Complete Graph) \\(K_n\\) 的匹配數 (Matchings)。 具體來說： \\[\n\\text{He}_n(x) = \\sum_{k=0}^{\\lfloor n/2 \\rfloor} (-1)^k N(n, k) x^{n-2k}\n\\] 其中 \\(N(n, k)\\) 是 在 \\(n\\) 個頂點的完全圖中，選出 \\(k\\) 條不相鄰邊 (disjoint edges) 的方法數。\n例子 \\(n=4\\): \\(\\text{He}_4(x) = x^4 - 6x^2 + 3\\)\n\n\\(x^4\\) 項 (\\(k=0\\)): 選 0 條邊。方法數 = 1。\n\\(x^2\\) 項 (\\(k=1\\)): 在 4 個點中選 1 條邊。\\(C(4, 2) = 6\\) 種。\n常數項 (\\(k=2\\)): 在 4 個點中選 2 條不相鄰的邊 (也就是完美匹配 Perfect Matching) -&gt; 3 種。\n係數正好是 \\(1, -6, 3\\) (正負號來自公式裡的 \\((-1)^k\\))。這完美的解釋了為什麼係數都是整數！\n\n\n解微分方程:\n我們來思考一下這個微分算符，他在物理學上有重要的意義，叫做 創生-消滅算符 (creation-annihilation operator)： \\[\n\\left( x - \\frac{d}{dx} \\right)\n\\] 但我們先不談物理意義，之後有機會再補充。如果說要解微分方程: \\[\n\\left( x - \\frac{d}{dx} \\right) y = f(x)\n\\] 其中 \\(f(x)\\) 是已知。大家以前在大學可能有學過個技巧，叫做 積分因子 (integrating factor)，我們可以試著同乘以一個函數 \\(u(x)\\) 來讓左邊變成「一個微分」，而非現在這樣的「乘法減微分」。我這邊不寫通式(因為原理是一樣的)，直接給出結果，取 \\(u(x) = e^{-\\frac{x^2}{2}}\\)： \\[\ne^{-\\frac{x^2}{2}} \\left( x - \\frac{d}{dx} \\right) y = e^{-\\frac{x^2}{2}} f(x)\n\\implies \\frac{d}{dx} \\left( e^{-\\frac{x^2}{2}} y \\right) = - e^{-\\frac{x^2}{2}} f(x)\n\\] 因此就可以寫出通解： \\[\ny = e^{\\frac{x^2}{2}} \\left( C - \\int e^{-\\frac{x^2}{2}} f(x) dx \\right)\n\\]\n所以說，從算符的角度來看，其實 \\[\n\\left( x - \\frac{d}{dx} \\right)  = e^{\\frac{x^2}{2}} \\left( -\\frac{d}{dx} \\right) e^{-\\frac{x^2}{2}}  \\tag{6}\\label{eq:hermite-operator-2}\n\\] 因此我們寫出了第三種 Hermite 多項式的定義： \\[\n\\text{He}_n(x) = e^{\\frac{x^2}{2}} \\left( -\\frac{d}{dx} \\right)^{n} e^{-\\frac{x^2}{2}}  \\tag{7}\\label{eq:hermite-definition-3}\n\\]\n其實回到 \\(\\eqref{eq:hermite-operator-2}\\)，這個等式是一個算符的等式，我們目前只是看到他作用在 1上的特例，而這個轉換不覺得很眼熟嗎? 感覺就像在對矩陣做對角化! 前後乘一個轉換矩陣和它的反矩陣，然後中間是簡單的對角矩陣 (這邊是微分算符)。\n\n\n正交性質\n在函數空間中，多項式正交就是指內積為零，而內積的定義通常是相乘後在某個權重函數下積分(也可以想成是期望值)。那這組多項式在什麼權重函數下是正交的呢？\n我們想選個適當的權重函數 \\(w(x)\\) 使得對所有 \\(m \\neq n\\)： \\[\n\\int_{-\\infty}^{\\infty} \\text{He}_m(x) \\text{He}_n(x) w(x) dx = 0\n\\] 利用 Hermite 多項式的定義 \\(\\eqref{eq:hermite-definition-3}\\)，我們有： \\[\n\\begin{align}\n&\\int_{-\\infty}^{\\infty} \\text{He}_m(x) \\text{He}_n(x) w(x) dx \\\\\n=& \\int_{-\\infty}^{\\infty} e^{\\frac{x^2}{2}} \\left( -\\frac{d}{dx} \\right)^{m} e^{-\\frac{x^2}{2}} \\cdot \\text{He}_n(x) w(x) dx \\\\\n=& \\int_{-\\infty}^{\\infty} \\left( -\\frac{d}{dx} \\right)^{m} e^{-\\frac{x^2}{2}} \\cdot \\left( e^{\\frac{x^2}{2}} \\text{He}_n(x) w(x) \\right) dx \\\\\n\\end{align}\n\\] 最直覺的想法是讓 \\(w(x) = e^{-\\frac{x^2}{2}}\\)，這樣就消掉了。然後不斷地做分部積分，直到把 \\(m\\) 次微分都移到 \\(\\text{He}_n(x)\\) 上面： \\[\n\\begin{align}\n&\\int_{-\\infty}^{\\infty} \\left( -\\frac{d}{dx} \\right)^{m} e^{-\\frac{x^2}{2}} \\cdot \\left( \\text{He}_n(x) \\right) dx \\\\\n=& \\int_{-\\infty}^{\\infty} e^{-\\frac{x^2}{2}} \\cdot \\left( \\frac{d}{dx} \\right)^{m} \\text{He}_n(x) dx\n\\end{align}\n\\] 當 \\(m \\neq n\\)，我們不妨假設 \\(m &gt; n\\)，那麼因為 \\(\\text{He}_n(x)\\) 是 \\(n\\) 次多項式，所以經過 \\(m\\) 次微分後會變成 0，因此整個積分結果是 0。\n若 \\(m=n\\)，我們可以利用 \\(\\eqref{eq:hermite-recursion}\\) 計算出 \\(\\left( \\frac{d}{dx} \\right)^{n} \\text{He}_n(x) = n!\\)，所以整個積分結果是 \\(\\displaystyle n! \\int_{-\\infty}^{\\infty} e^{-\\frac{x^2}{2}} dx = n! \\sqrt{2\\pi}\\)。\n因此我們推導出 Hermite 多項式的正交性質： \\[\n\\int_{-\\infty}^{\\infty} \\text{He}_m(x) \\text{He}_n(x) e^{-\\frac{x^2}{2}} dx = n! \\sqrt{2\\pi} \\delta_{mn}\n\\tag{8}\\label{eq:hermite-orthogonality}\n\\] 其中 \\(\\delta_{mn}\\) 是 Kronecker delta。\n\n\n小結\nHermite 多項式會出現在很多地方，他會是 Fourier Transform 的本徵函數 (eigenfunction)! 敬請期待未來的文章。"
  },
  {
    "objectID": "posts/normal-distribution/day_5.html",
    "href": "posts/normal-distribution/day_5.html",
    "title": "一天證明一個 Normal Distribution 的性質 Day5：獨立性的量尺: Cumulant",
    "section": "",
    "text": "今天來講個輕鬆的小主題：Cumulant。\n對於一個隨機變數 \\(X\\)，我們通常想知道平均跟標準差。有時候我們甚至會想知道更高階的資訊，例如偏度 (skewness) 跟峰度 (kurtosis)，這些都可以透過動差 (moment) 來描述。\n\nCumulant Generating Function\n還記得動差生成函數 (Moment Generating Function, MGF) 定義為： \\[  \nM_X(t) = \\mathbb{E}[e^{tX}] = \\sum_{k=0}^{\\infty} \\mathbb{E}[X^{k}] \\frac{t^k}{k!}\n\\] 名符其實，他就是「動差」的生成函數，因為對 \\(M_X(t)\\) 做泰勒展開 (Taylor Expansion) 後，係數正好是各階 動差 (moment)： \\[\n\\mu_k' := \\mathbb{E}[X^k] = M_X^{(k)}(0) = \\left. \\frac{d^k}{dt^k} M_X(t) \\right|_{t=0}  \\tag{1}\\label{eq:moment_def_1}\n\\] 另外一種比較常用的是減掉常數項的中心動差 (central moment)： \\[\n\\mu_k := \\mathbb{E}[(X - \\mathbb{E}[X])^k]\n\\]\n不過呢，有時候因為動差函數不存在 (例如 Cauchy 分佈)，我們會改用一定會存在的特徵函數 (Characteristic Function, CF)： \\[\n\\phi_X(t) = \\mathbb{E}[e^{itX}] = \\sum_{k=0}^{\\infty} \\mathbb{E}[X^{k}] \\frac{(it)^k}{k!}\n\\] 所以也可以用 \\(\\phi_X(t)\\) 來計算動差： \\[\n\\mu_k' = \\mathbb{E}[X^k] = \\frac{1}{i^k} \\phi_X^{(k)}(0) = \\left. \\frac{1}{i^k} \\frac{d^k}{dt^k} \\phi_X(t) \\right|_{t=0}  \\tag{2}\\label{eq:moment_def_2}\n\\] 至於 central moment， \\[\n\\mu_k = \\mathbb{E}[(X - \\mathbb{E}[X])^k] = \\frac{1}{i^k} \\frac{d^k}{dt^k} \\left[ e^{-it \\mu_1'} \\phi_X(t) \\right]_{t=0}\n\\] 這邊要稍微注意一下，CF 是個複數值函數，所以其實是個複變函數的微分!\n舉例說明：\n\n\n\n\n\n\n\n\n\n\n\nDistribution\n\\(\\phi_X(t)\\)\n\\(\\frac{d}{dt}\\phi_X(t)\\)\n\\(\\mathbb{E}[X]\\)\n\\(\\frac{d^2}{dt^2}\\phi_X(t)\\)\n\\(\\mathbb{E}[X^2]\\)\n\n\n\n\nBernoulli(\\(p\\))\n\\(1 - p + p e^{it}\\)\n\\(ip e^{it}\\)\n\\(p\\)\n\\(-p e^{it}\\)\n\\(p\\)\n\n\nPoisson(\\(\\lambda\\))\n\\(e^{\\lambda(e^{it}-1)}\\)\n\\(i\\lambda e^{it} e^{\\lambda(e^{it}-1)}\\)\n\\(\\lambda\\)\n\\(-\\lambda e^{it} e^{\\lambda(e^{it}-1)} + (i\\lambda e^{it})^2 e^{\\lambda(e^{it}-1)}\\)\n\\(\\lambda + \\lambda^2\\)\n\n\nNormal(\\(\\mu, \\sigma^2\\))\n\\(e^{i\\mu t - \\frac{1}{2}\\sigma^2 t^2}\\)\n\\(i\\mu e^{i\\mu t - \\frac{1}{2}\\sigma^2 t^2} - \\sigma^2 t e^{i\\mu t - \\frac{1}{2}\\sigma^2 t^2}\\)\n\\(\\mu\\)\n\\(-\\sigma^2 e^{i\\mu t - \\frac{1}{2}\\sigma^2 t^2} + (i\\mu - \\sigma^2 t)^2 e^{i\\mu t - \\frac{1}{2}\\sigma^2 t^2}\\)\n\\(\\sigma^2 + \\mu^2\\)\n\n\nGamma(\\(\\alpha, \\theta\\))\n\\((1 - i\\theta t)^{-\\alpha}\\)\n\\(i\\alpha \\theta (1 - i\\theta t)^{-\\alpha - 1}\\)\n\\(\\alpha \\theta\\)\n\\(- \\alpha (\\alpha + 1) \\theta^2 (1 - i\\theta t)^{-\\alpha - 2}\\)\n\\(\\alpha (\\alpha + 1) \\theta^2\\)\n\n\n\n而如果只是想算 moment 的話 CF 就很夠用，但他對於捕捉獨立性 (independence) 的效果並不理想。這時候我們就需要用到 Cumulant Generating Function (CGF)： \\[\nK_X(t) = \\log M_X(t) = \\log \\mathbb{E}[e^{tX}]  \\tag{3}\\label{eq:cumulant_gf_def}\n\\]\n因此我們定義 Cumulant 為： \\[\n\\kappa_k := K_X^{(k)}(0) = \\left. \\frac{d^k}{dt^k} K_X(t) \\right|_{t=0}  \\tag{4}\\label{eq:cumulant_def_1}\n\\]\n當然，也有複數版本，通常會叫 Second Characteristic Function： \\[\nH_X(t) = \\log \\phi_X(t) = \\log \\mathbb{E}[e^{itX}]  \\tag{5}\\label{eq:second_char_def}\n\\] 但這有個問題，就是 \\(\\phi_X(t)\\) 可能會是負數或複數，導致 \\(\\log \\phi_X(t)\\) 會有多重值 (multi-valued) 的問題，所以我們通常還是用實數版本的 CGF。\n取了 \\(\\log\\) 的好處就是：如果 \\(X\\) 跟 \\(Y\\) 獨立，那麼 \\(K_{X+Y}(t) = K_X(t) + K_Y(t)\\)。這點跟 MGF 不同，因為 MGF 是乘法關係 (\\(M_{X+Y}(t) = M_X(t) M_Y(t)\\))。\n所以對於 moment 來說，兩個獨立隨機變數相加的第 \\(n\\) 階動差，會是兩個變數各自前 \\(n\\) 階動差的 convolution (加上一些組合係數)。 但對於 cumulant 來說，兩個獨立隨機變數相加的第 \\(n\\) 階 cumulant，會是兩個變數各自第 \\(n\\) 階 cumulant 的直接相加： \\[\n\\kappa_n(X + Y) = \\kappa_n(X) + \\kappa_n(Y) \\quad \\text{if } X \\perp Y  \\tag{6}\\label{eq:cumulant_indep_add}\n\\]\n舉例說明：\n\n\n\n\n\n\n\n\n\n\nDistribution\n\\(\\kappa_1\\)\n\\(\\kappa_2\\)\n\\(\\kappa_3\\)\n\\(\\kappa_4\\)\n\n\n\n\nBernoulli(\\(p\\))\n\\(p\\)\n\\(p(1-p)\\)\n\\(p(1-p)(1-2p)\\)\n\\(p(1-p)(1 - 6p(1-p))\\)\n\n\nPoisson(\\(\\lambda\\))\n\\(\\lambda\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\n\nNormal(\\(\\mu, \\sigma^2\\))\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(0\\)\n\\(0\\)\n\n\nGamma(\\(\\alpha, \\theta\\))\n\\(\\alpha \\theta\\)\n\\(\\alpha \\theta^2\\)\n\\(2 \\alpha \\theta^3\\)\n\\(6 \\alpha \\theta^4\\)\n\n\n\n註: 直覺上因為 distribution 完全被 CF 決定，所以知道了所有 moment (或 cumulant) 似乎也就等於知道了 distribution 本身。但是，其實有可能 CF 在 0 點附近並不解析 (analytic)，常見例子請參考 Log-Normal distribution。所以說，知道所有 moment 只是知道在 0 點的微分，並不保證能還原整個 CF。\n看著這個表，我們會想，是不是任意給 \\(\\kappa_1, \\kappa_2, \\ldots, \\kappa_n\\)，就能找到一個對應的分佈？答案是否定的。比方說，若要使 cumulant 只有有限項非零，那麼這個分佈只能是 Normal 分佈，這是個非常有名的結果：\n\n大定理(Marcinkiewicz Theorem): 如果一個隨機變數的 cumulant generating function 是個(有限階)多項式，那麼這個多項式的階數最多是 2。\n\n為了保證 CGF 的存在性，該定理本來的敘述是說，若 CF 可以寫成 \\(\\phi_X(t) = e^{P(t)}\\)，其中 \\(P(t)\\) 是個(複係數)多項式，那麼 \\(P(t)\\) 的階數最多是 2。\n我們先跳過證明，提供一些直覺上的理解：主要是因為 CF 是 PDF 的傅立葉變換，而 PDF 必須非負 (non-negative)，這使得 CF 的形式受到很大限制。Lukacs (1970, Theorem 7.3.3) 有詳細的證明。\n\n\nCumulant vs Moment\n在一階、二階和三階，cumulant 跟 central moment 是一樣的: \\[\n\\frac{d}{dt} K_X(t) = \\frac{M_X'(t)}{M_X(t)} \\implies \\kappa_1 = K_X'(0) = M_X'(0) = \\mu_1' = \\mathbb{E}[X]\n\\] \\[\n\\begin{align}\n&\\frac{d^2}{dt^2} K_X(t) = \\frac{M_X''(t)}{M_X(t)} - \\frac{M_X'(t)^2}{(M_X(t))^2} \\\\\n\\implies &\\kappa_2 = K_X''(0) = M_X''(0) - (M_X'(0))^2 = \\mu_2' - \\mu_1'^2 = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mu_2\n\\end{align}\n\\] 三階 cumulant 是： \\[\n\\begin{align}\n&\\frac{d^3}{dt^3} K_X(t) = \\frac{M_X'''(t)}{M_X(t)} - 3 \\frac{M_X''(t) M_X'(t)}{(M_X(t))^2} + 2 \\frac{(M_X'(t))^3}{(M_X(t))^3} \\\\\n\\implies &\\kappa_3 = K_X'''(0) = M_X'''(0) - 3 M_X''(0) M_X'(0) + 2 (M_X'(0))^3 \\\\\n&\\quad = \\mu_3' - 3 \\mu_2' \\mu_1' + 2 (\\mu_1')^3 = \\mathbb{E}[(X - \\mathbb{E}[X])^3] = \\mu_3\n\\end{align}\n\\] 但是從四階開始，cumulant 跟 central moment 就不一樣了： \\[\n\\frac{d^4}{dt^4} K_X(t) = \\frac{M_X^{(4)}(t)}{M_X(t)} - 4 \\frac{M_X^{(3)}(t) M_X'(t)}{(M_X(t))^2} - 3 \\frac{(M_X''(t))^2}{(M_X(t))^2} + 12 \\frac{M_X''(t) (M_X'(t))^2}{(M_X(t))^3} - 6 \\frac{(M_X'(t))^4}{(M_X(t))^4}\n\\] 所以我們有： \\[\n\\begin{align}\n\\implies &\\kappa_4 = K_X^{(4)}(0) = M_X^{(4)}(0) - 4 M_X^{(3)}(0) M_X'(0) - 3 (M_X''(0))^2 + 12 M_X''(0) (M_X'(0))^2 - 6 (M_X'(0))^4 \\\\\n&\\quad = \\mu_4' - 4 \\mu_3' \\mu_1' - 3 (\\mu_2')^2 + 12 \\mu_2' (\\mu_1')^2 - 6 (\\mu_1')^4 \\\\\n&\\quad = \\mathbb{E}[(X - \\mathbb{E}[X])^4] - 3 (\\mathbb{E}[(X - \\mathbb{E}[X])^2])^2 = \\mu_4 - 3 \\mu_2^2\n\\end{align}\n\\]\n由 cumulant 對於獨立變數相加的可加性質，我們可以由此看出，兩個獨立的隨機變數相加，平均值、變異數、偏度 都會相加，但峰度卻不會相加，因為峰度差了 \\(3 \\mu_2^2\\)。而一般峰度的定義是 \\(\\text{Kurt}[X] = \\frac{\\mu_4}{\\mu_2^2}\\)，所以峰度減去 3 (稱之為 excess kurtosis)，再搭配上適當的 variance scaling 就會有可加性。\n一般式的推導\n徒手算到這邊也差不多了。對於更高階的 cumulant，我們應該系統化地研究他們的係數。 因為 \\(M_X(t) = e^{K_X(t)}\\)，所以 \\(\\frac{dM_X(t)}{dt} = K_X'(t) e^{K_X(t)} = K_X'(t) M_X(t)\\)。\n\n乘法的微分 \\(n\\) 階就會跟 二項式係數 有關: \\[\n\\frac{d^n}{dt^n} [f(t) g(t)] = \\sum_{k=0}^{n} \\binom{n}{k} f^{(k)}(t) g^{(n-k)}(t)\n\\]\n\n所以， \\[\n\\begin{align}\nM_X^{(n)}(t) &= \\frac{d^{n-1}}{dt^{n-1}} [K_X'(t) M_X(t)] \\\\\n&= \\sum_{m=0}^{n-1} \\binom{n-1}{m} K_X^{(n-m)}(t) M_X^{(m)}(t)\n\\end{align}\n\\] 將 \\(t=0\\) 代入，因為 \\(M_X^{(0)}(0) = 1\\) ，而 \\(M_X(t)\\) 的 \\(n\\) 階導數在 \\(0\\) 就是動差 \\(\\mu_n'\\)，我們得到： \\[\n\\mu_n' = \\kappa_{n} + \\sum_{m=1}^{n-1} \\binom{n-1}{m} \\kappa_{n-m} \\mu_{m}'  \\tag{7}\\label{eq:moment_cumulant_relation}\n\\] 寫成矩陣形式會比較清楚： \\[\n\\begin{bmatrix}\n\\mu_1' \\\\\n\\mu_2' \\\\\n\\mu_3' \\\\\n\\mu_4' \\\\\n\\mu_5' \\\\\n\\vdots\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n\\binom{1}{1} \\mu_1' & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\binom{2}{2} \\mu_2' & \\binom{2}{1} \\mu_1' & 1 & 0 & 0 & \\cdots \\\\\n\\binom{3}{3} \\mu_3' & \\binom{3}{2} \\mu_2' & \\binom{3}{1} \\mu_1' & 1 & 0 & \\cdots \\\\\n\\binom{4}{4} \\mu_4' & \\binom{4}{3} \\mu_3' & \\binom{4}{2} \\mu_2' & \\binom{4}{1} \\mu_1' & 1 & \\cdots \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n\\end{bmatrix}\n\\begin{bmatrix}\n\\kappa_1 \\\\\n\\kappa_2 \\\\\n\\kappa_3 \\\\\n\\kappa_4 \\\\\n\\kappa_5 \\\\\n\\vdots\n\\end{bmatrix}\n\\] 還是把 \\(\\mu_n'\\) 全部寫在一起好了： \\[\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 & 0 & \\cdots \\\\\n\\mu_1' & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n\\mu_2' & \\binom{1}{1} \\mu_1' & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\mu_3' & \\binom{2}{2} \\mu_2' & \\binom{2}{1} \\mu_1' & 1 & 0 & 0 & \\cdots \\\\\n\\mu_4' & \\binom{3}{3} \\mu_3' & \\binom{3}{2} \\mu_2' & \\binom{3}{1} \\mu_1' & 1 & 0 & \\cdots \\\\\n\\mu_5' & \\binom{4}{4} \\mu_4' & \\binom{4}{3} \\mu_3' & \\binom{4}{2} \\mu_2' & \\binom{4}{1} \\mu_1' & 1 & \\cdots \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n\\end{bmatrix}\n\\begin{bmatrix}\n-1 \\\\\n\\kappa_1 \\\\\n\\kappa_2 \\\\     \n\\kappa_3 \\\\\n\\kappa_4 \\\\\n\\kappa_5 \\\\\n\\vdots  \n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-1 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\vdots\n\\end{bmatrix}\n\\] 所以說要算任意的 \\(\\kappa_n\\)，我們就看這個矩陣的前 \\((n+1) \\times (n+1)\\) 子矩陣，然後套克拉瑪公式 (Cramer’s Rule)。 而且因為左邊矩陣行列式為 1，所以分母總是1。比方說來算 \\(\\kappa_5\\)，(最後一列是代右邊)： \\[\n\\kappa_5 =\n\\begin{vmatrix}\n1 & 0 & 0 & 0 & 0 & \\textcolor{blue}{-1} \\\\\n\\mu_1' & 1 & 0 & 0 & 0 & \\textcolor{blue}{0} \\\\\n\\mu_2' & \\binom{2}{1} \\mu_1' & 1 & 0 & 0 & \\textcolor{blue}{0} \\\\\n\\mu_3' & \\binom{3}{2} \\mu_2' & \\binom{3}{1} \\mu_1' & 1 & 0 & \\textcolor{blue}{0} \\\\\n\\mu_4' & \\binom{4}{3} \\mu_3' & \\binom{4}{2} \\mu_2' & \\binom{4}{1} \\mu_1' & 1 & \\textcolor{blue}{0} \\\\\n\\mu_5' & \\binom{5}{4} \\mu_4' & \\binom{5}{3} \\mu_3' & \\binom{5}{2} \\mu_2' & \\binom{5}{1} \\mu_1' & \\textcolor{blue}{0}\n\\end{vmatrix}\n\\]\n\n\n\n\n\nReferences\n\nLukacs, Eugene. 1970. Characteristic Functions. 2nd ed. Griffin."
  },
  {
    "objectID": "posts/normal-distribution/day_1.html",
    "href": "posts/normal-distribution/day_1.html",
    "title": "一天證明一個 Normal Distribution 的性質 Day1：高斯積分與最大化熵(Entropy)",
    "section": "",
    "text": "我們將介紹的第一個分佈，是統計學中最著名的——常態分佈（Normal Distribution）。它通常被稱為高斯分佈 (Gaussian distribution)，以紀念偉大的數學家卡爾·弗里德里希·高斯 (Carl Friedrich Gauss)。\n儘管高斯在 19 世紀初將其發揚光大（用於分析天文觀測的誤差），但這個鐘形曲線的數學形式最早是由亞伯拉罕·德莫佛 (Abraham de Moivre) 在 1733 年發現的，作為二項分佈的近似。\n我們通常看到的常態分佈公式是 \\(\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-(x-\\mu)^2 / (2\\sigma^2)}\\)，看起來有點嚇人。但今天，讓我們從一個更簡潔、更優美的形式開始 \\[\nf(x) = e^{-\\pi x^2}\n\\] 在此參數下，平均值 \\(\\mu=0\\)，變異數 \\(\\sigma^2=\\frac{1}{2\\pi}\\)。\n\n高斯積分\n你可能會問：『等等，你是不是漏掉了前面那個複雜的常數（像是 \\(\\frac{1}{\\sqrt{2\\pi}}\\)）？』答案是：沒有！ 這個形式的巧妙之處在於，它的歸一化常數恰好是 1。換句話說，這個函數在整個實數線上的積分（即曲線下的總面積）不多不少，剛好等於 1。這使它成為一個合法的機率密度函數 (PDF)。讓我們來證明這一點。\n我們要計算的是 \\(I = \\int_{-\\infty}^{\\infty} e^{-\\pi x^2} dx\\)。這裡有一個絕妙的技巧：我們不直接計算 \\(I\\)，而是計算 \\(I^2\\)：\\[I^2 = \\left( \\int_{-\\infty}^{\\infty} e^{-\\pi x^2} dx \\right) \\left( \\int_{-\\infty}^{\\infty} e^{-\\pi y^2} dy \\right)\\]由於 \\(x\\) 和 \\(y\\) 只是虛擬變數 (dummy variables)，我們可以將它們合併為一個二重積分：\\[I^2 = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} e^{-\\pi (x^2 + y^2)} dx dy\\]關鍵一步來了：切換到極座標 (Polar Coordinates)！讓我們設 \\(x = r \\cos \\theta\\) 和 \\(y = r \\sin \\theta\\)。\\(x^2 + y^2 = r^2\\)面積元素 \\(dx dy\\) 變為 \\(r dr d\\theta\\)積分範圍：\\(r\\) 從 \\(0\\) 到 \\(\\infty\\)，\\(\\theta\\) 從 \\(0\\) 到 \\(2\\pi\\)。我們的 \\(I^2\\) 積分變成了：\\[I^2 = \\int_{0}^{2\\pi} \\left( \\int_{0}^{\\infty} e^{-\\pi r^2} \\cdot r dr \\right) d\\theta\\]我們先來解決括號內的 \\(r\\) 積分。我們使用 u-替換 (u-substitution)：設 \\(u = \\pi r^2\\) \\(du = 2\\pi r dr \\implies r dr = \\frac{1}{2\\pi} du\\) 當 \\(r=0\\) 時，\\(u=0\\)。當 \\(r \\to \\infty\\) 時，\\(u \\to \\infty\\)。 \\[\\int_{r=0}^{\\infty} e^{-\\pi r^2} r dr = \\int_{u=0}^{\\infty} e^{-u} \\left( \\frac{1}{2\\pi} du \\right) = \\frac{1}{2\\pi} {(-e^{-u})}\\Big|_{0}^{\\infty} = \\frac{1}{2\\pi}\n\\] 現在，我們把這個結果 \\(\\frac{1}{2\\pi}\\) 放回 \\(I^2\\) 的 \\(\\theta\\) 積分中： \\[I^2 = \\int_{0}^{2\\pi} \\left( \\frac{1}{2\\pi} \\right) d\\theta = 1\n\\] 既然 \\(I^2 = 1\\)，且我們的函數 \\(f(x)\\) 明顯恆為正，所以 \\(I\\) 必定為正。因此，我們證明了： \\[\\int_{-\\infty}^{\\infty} e^{-\\pi x^2} dx = 1\n\\] Q.E.D. (證明完畢)！\n\n\n最大化熵(Entropy)\n如同離散熵的定義，我們對於連續的機率分布可以定義微分熵 (Differential Entropy): \\[\nH(f) = - \\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) dx\n\\]\n這是資訊理論 (Information Theory) 和統計物理 (Statistical Physics) 中的基石。它解釋了為什麼常態分佈在自然界中如此普遍：在給定平均值和變異數（即平均能量和能量波動）的限制下，常態分佈是系統「最混亂」或「最不確定」的狀態。\n不同於使用傳統的變分法，我們將使用一個非常強大（且更簡潔）的工具來證明這一點: KL 散度 (Kullback-Leibler Divergence)。 \\(D_{KL}(f || g)\\) 衡量「機率分佈 \\(f\\) 與 \\(g\\) 的差異程度」。 \\[\nD_{KL}(f || g) = \\int_{-\\infty}^{\\infty} f(x) \\ln\\left(\\frac{f(x)}{g(x)}\\right) dx\n\\]\n由於 \\(\\ln(x)\\) 是一個凹函數 (concave function)（其二階導數恆負），根據琴生不等式 (Jensen’s inequality)，我們可以得到著名的吉布斯不等式 (Gibbs’ Inequality): \\[\n-D_{KL}(f || g) = \\int_{-\\infty}^{\\infty} f(x) \\ln\\left(\\frac{g(x)}{f(x)}\\right) dx\n    \\le \\ln \\left( \\int_{-\\infty}^{\\infty} f(x) \\frac{g(x)}{f(x)} dx \\right)\n    = \\ln(1) = 0\n\\]\n將兩邊同乘 \\(-1\\)，我們就證明了 \\(D_{KL}(f || g) \\ge 0\\) 恆成立。\n利用這個強大的不等式，我們將 \\(g(x)\\) 設為我們的目標分佈 \\[\ne^{-\\pi x^2} \\sim \\mathcal{N}(0, \\frac{1}{2\\pi})\n\\] 我們已知這是一個合法的 PDF，其均值 \\(\\mu_g = 0\\)，變異數 \\(\\sigma_g^2 = \\frac{1}{2\\pi}\\)。\n接著，我們來分析 \\(\\int f(x) \\ln(g(x)) dx\\) 這一項： \\[\n\\int_{-\\infty}^{\\infty} f(x) \\ln\\left( g(x) \\right) dx\n= -\\pi \\int_{-\\infty}^{\\infty} f(x) x^2 dx\n\\]\n上式即為在 \\(f(x)\\) 分佈下，\\(x^2\\) 的期望值，記為 \\(\\mathbb{E}_f[x^2]\\)。 現在，我們施加約束：我們要求 \\(f(x)\\) 必須和 \\(g(x)\\) 具有相同的均值與變異數。 也就是說，我們假設 \\(f(x)\\) 也滿足：\n\n\\(\\text{Mean}(f) = \\mu_f = 0\\)\n\\(\\text{Var}(f) = \\sigma_f^2 = \\frac{1}{2\\pi}\\)\n\n最後，我們來串聯這一切： \\[\n\\begin{align}\nH(f) + D_{KL}(f || g)  &= - \\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) dx + D_{KL}(f || g)  \\\\\n    &= - \\int_{-\\infty}^{\\infty} f(x) \\ln\\left( g(x) \\right) dx  \\\\\n    &= \\pi \\cdot \\mathbb{E}_f(x^2)   \\qquad \\text{(帶入我們剛剛的計算)} \\\\\n    &= \\pi \\left( \\text{Var}(f)+\\text{Mean}(f)^2 \\right)  \\qquad \\text{(變異數的定義)} \\\\\n    &= \\pi \\left( \\frac{1}{2\\pi} + 0^2 \\right)   \\qquad \\text{(}f(x)\\text{的約束)} \\\\\n    &= \\frac{1}{2} \\\\\n\\end{align}\n\\]\n我們得到了 \\(H(f) + D_{KL}(f || g) = \\frac{1}{2}\\)。那麼 \\(g(x)\\) 本身的熵 \\(H(g)\\) 是多少呢？我們可以用完全相同的計算（因為 \\(g(x)\\) 也滿足均值為 0、變異數為 \\(\\frac{1}{2\\pi}\\) 的約束）： \\[\nH(g) = \\pi \\cdot \\mathbb{E}_g[x^2] = \\pi \\left( \\text{Var}(g) + \\text{Mean}(g)^2 \\right) = \\pi \\left( \\frac{1}{2\\pi} + 0^2 \\right) = \\frac{1}{2}\n\\] 因此，我們證明了： \\[\nH(f) + D_{KL}(f || g) = H(g)\n\\] 所以 \\(H(g) - H(f) = D_{KL}(f || g) \\ge 0 \\implies H(g) \\ge H(f)\\)。\nQ.E.D. (證明完畢)！這證明了在所有均值為 \\(0\\)、變異數為 \\(\\frac{1}{2\\pi}\\) 的分佈中，常態分佈 \\(g(x)\\) 的熵是最大的。"
  },
  {
    "objectID": "posts/normal-distribution/day_2.html",
    "href": "posts/normal-distribution/day_2.html",
    "title": "一天證明一個 Normal Distribution 的性質 Day2：特徵函數(CF)與傅立葉變換",
    "section": "",
    "text": "今天來講一下 moment generation function 跟 characteristic function。\n對一個隨機變數 \\(X\\)，我們可以定義動差生成函數(Moment Generating Function, MGF): \\[\nM_X(t) = E[e^{tX}]\n\\] 對於 \\(X \\sim N(\\mu, \\sigma^2)\\)，它的 MGF 為： \\[\nM_X(t) = e^{\\mu t + \\frac{1}{2}\\sigma^2 t^2}\n\\]\n讓我們假設隨機變數 \\(X\\) 服從常態分佈，即 \\(X \\sim N(\\mu, \\sigma^2)\\)，其機率密度函數 (PDF) 為： \\[f(x; \\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}, \\quad x \\in \\mathbb{R}\n\\]\n其中 \\(\\mu\\) 是平均數（mean），\\(\\sigma^2\\) 是變異數（variance）。\n帶入定義 \\[\n\\begin{aligned}\nM_X(t)\n&= \\int_{-\\infty}^{\\infty} \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} e^{tx}  dx  \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-\\frac{1}{2}u^2 + t(\\sigma u+\\mu) }dx\n\\qquad(\\text{replace:}\\quad x=\\sigma u + \\mu) \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} e^{\\mu t + \\frac{1}{2}\\sigma^2 t^2} \\int_{-\\infty}^{\\infty} e^{-\\frac{1}{2}(u - t\\sigma )^2} du \\\\\n&= e^{\\mu t + \\frac{1}{2}\\sigma^2 t^2} \\quad(\\text{因為高斯積分的結果為 } \\sqrt{2\\pi}) \\\\\n\\end{aligned}\n\\]\nCharacteristic function (CF) 定義為： \\[  \n\\phi_X(t) = E[e^{itX}]\n\\] 對於 \\(X \\sim N(\\mu, \\sigma^2)\\)，它的 CF 為： \\[\n\\phi_X(t) = e^{i\\mu t - \\frac{1}{2}\\sigma^2 t^2} \\tag{Gauss-CF}\\label{eq:gauss}\n\\]\n計算跟上面 MGF 類似，只是將 \\(t\\) 換成 \\(it\\)。\n\nCharacteristic 的唯一性\nCF 有一個重要的性質：它總是存在，因為 \\(|e^{itX}| = 1\\)。 此外，CF 可以用來證明隨機變數的分佈唯一性：如果兩個隨機變數的 CF 相同，則它們的分佈也相同。這也將是我們今天的重點。讓我們來細細品味這個結果背後的意義與應用。\n我們來看一個隨機變數 \\(X\\) 的 CF： \\[\n\\phi_X(t) = E[e^{itX}] = \\int_{-\\infty}^{\\infty} e^{itx} f_X(x) dx  \\tag{1}\\label{eq:fourier}\n\\] 其中 \\(f_X(x)\\) 是 \\(X\\) 的機率密度函數 (PDF)。\n我們來看看能不能從 CF 回推 PDF。我們可以把上式\\(\\eqref{eq:fourier}\\)看成是一個傅立葉變換 (Fourier Transform)。 \\[\n\\mathcal{F}[f_X](t) \\coloneqq \\int_{-\\infty}^{\\infty} e^{itx} f_X(x) dx = \\phi_X(t)\n\\] 這邊的正負號和係數跟一般傅立葉變換的定義可能不太一樣，但本質上是一樣的。\n我們想做個傅立葉「反」變換，大致上是：\n\\[\n\\begin{align}\n\\mathcal{F}^{-1}[\\phi_X](x)\n&\\coloneqq \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-itx} \\phi_X(t) dt   \\tag{2}\\label{eq:fourier_inv} \\\\\n&= \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-itx} \\left( \\int_{-\\infty}^{\\infty} e^{it y} f_X(y) dy \\right) dt    \\\\\n&= \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty}   \\int_{-\\infty}^{\\infty} e^{-itx} e^{it y} f_X(y) dy dt    \\\\\n&= \\int_{-\\infty}^{\\infty} f_X(y) \\left( \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{it(y - x)} dt \\right) dy  \\tag{3}\\label{eq:swap} \\\\\n&= \\int_{-\\infty}^{\\infty} f_X(y) \\delta(y - x) dy \\qquad\\text{(Why?)} \\tag{4}\\label{eq:delta}  \\\\\n&= f_X(x)  \\\\\n\\end{align}\n\\]\n大致上的感覺是這樣，但中間的步驟有些含糊，特別是涉及到狄拉克 delta 函數的部分。我們需要一些條件來保證這些積分的交換是合法的。確切來說\\(\\eqref{eq:swap}\\) 使用的積分順序的調換，是Fubini 定理的應用，而\\(\\eqref{eq:delta}\\) 則是利用了狄拉克 delta 函數的定義。\n而Fubini定理要求的條件是被積分函數必須是絕對可積的 (absolutely integrable)，來檢查一下 \\(|e^{-itx}e^{ity}| = 1\\)，所以 \\[\n\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} |e^{-itx} e^{ity} f_X(y)| dy dt\n= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} |f_X(y)| dy dt\n= \\int_{-\\infty}^{\\infty} dt = \\infty\n\\]\nBoom! 這個條件不成立。\n但這不代表結論是錯的，只是這個證明是錯的!直接調換順序是行不通的。\n我們來欣賞一下大數學家是怎麼解決這個問題的。如果在積分內有個函數 \\(g(t)\\)，使得 \\(g(t)\\) 是絕對可積的 (absolutely integrable)，那麼我們就可以使用Fubini定理來調換積分順序。\n讓我們退回到第一條式子，假設有某個 \\(g(t)\\in L^1\\)，也就是說 \\(\\int_{-\\infty}^{\\infty} |g(t)| dt &lt; \\infty\\)，那麼我們有：\n\\[\n\\begin{align}\n&\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-itx} \\textcolor{red}{g(t)} \\mathcal{F}[f_X](t) dt \\tag{5}\\label{eq:ee1}  \\\\\n= &\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-itx} \\textcolor{red}{g(t)} \\left( \\int_{-\\infty}^{\\infty} e^{it y} f_X(y) dy \\right) dt   \\\\\n= &\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty}   \\int_{-\\infty}^{\\infty} e^{-itx} e^{it y} f_X(y) \\textcolor{red}{g(t)} dy dt  \\\\\n= &\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty}   \\int_{-\\infty}^{\\infty} e^{-itx} e^{it y} f_X(y) \\textcolor{red}{g(t)} dt dy  \\qquad\\text{(可以交換了!)} \\\\\n= & \\int_{-\\infty}^{\\infty} f_X(y) \\left( \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-it(x - y)} \\textcolor{red}{g(t)} dt \\right) dy  \\\\\n= & \\int_{-\\infty}^{\\infty} f_X(y) \\cdot \\textcolor{red}{\\mathcal{F}^{-1}[g]}(x - y) dy  \\qquad\\text{(剛好是}g\\text{的傅立葉反轉換)} \\tag{6}\\label{eq:ee2}  \\\\\n\\end{align}\n\\] 積分順序可以交換是因為 \\(g(t)\\in L^1\\)。這時候分析的大絕招來了，取極限!我們讓 \\(g(t)\\) 慢慢逼近常數函數 \\(1\\)，然後看看右邊會變成什麼。\n但我們要取個已知傅立葉反變換的 \\(g(t)\\)，例如我們剛剛算的高斯分布PDF: \\(g(t) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-t^2/2\\sigma^2}\\)，其傅立葉反變換也是高斯函數： \\[\n\\begin{align}\n&\\mathcal{F}^{-1}[g](x)  \\\\\n&= \\frac{1}{2\\pi} \\mathcal{F}[g](-x)  \\qquad\\text{(這是根據定義)} \\\\\n&= \\frac{1}{2\\pi} e^{-\\frac{1}{2}\\sigma^2 (-x)^2}  \\qquad\\text{(根據\\eqref{eq:gauss})} \\\\\n&= \\frac{1}{2\\pi} e^{-\\frac{1}{2}\\sigma^2 x^2}  \\tag{7}\\label{eq:gauss_inv} \\\\\n\\end{align}\n\\]\n但這個 \\(g\\) 隨著 \\(\\sigma \\to \\infty\\)，會趨近於常數函數 \\(0\\)。我們要把常數乘回去(傅立葉變換是線性的)，所以我們其實是要定義 \\[\ng_{\\sigma}(t) \\coloneqq e^{-t^2/2\\sigma^2}\n\\] 根據\\(\\eqref{eq:gauss_inv}\\)，其傅立葉反變換為 \\[\n\\mathcal{F}^{-1}[g_{\\sigma}](x) = \\frac{\\sigma}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\sigma^2 x^2}\n\\] 我們代回\\(\\eqref{eq:ee1}\\)和\\(\\eqref{eq:ee2}\\)，得到 \\[\n\\begin{align}\n\\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-itx} \\textcolor{red}{g_{\\sigma}(t)} \\mathcal{F}[f_X](t) dt\n= & \\int_{-\\infty}^{\\infty} f_X(y) \\cdot \\textcolor{red}{\\mathcal{F}^{-1}[g_{\\sigma}]}(x - y) dy  \\\\\n= & \\int_{-\\infty}^{\\infty} f_X(y) \\cdot \\frac{\\sigma}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\sigma^2 (x - y)^2} dy  \\\\\n\\end{align}\n\\] 我們 \\(\\sigma\\) 趨近於無窮大後等式就成立了，這需要兩個極限的等式： \\[\n\\begin{align}\n\\mathcal{F}^{-1}[\\mathcal{F}[f_X]](x)\n&= \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-itx} \\mathcal{F}[f_X](t) dt  \\qquad\\text{(這是用傅立葉反變換的定義)}  \\\\\n&= \\lim_{\\sigma \\to \\infty} \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-itx} g_{\\sigma}(t) \\mathcal{F}[f_X](t) dt  \\tag{8}\\label{eq:lim1}  \\\\\n&= \\lim_{\\sigma \\to \\infty} \\int_{-\\infty}^{\\infty} f_X(y) \\cdot \\frac{\\sigma}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\sigma^2 (x - y)^2} dy  = f_X(x)  \\qquad\\text{(根據上面的推導)} \\\\\n&= \\lim_{\\epsilon \\to 0^{+}} \\int_{-\\infty}^{\\infty} f_X(y) \\cdot \\frac{1}{\\sqrt{2\\pi \\epsilon^2}} e^{-\\frac{(x - y)^2}{2\\epsilon^2}} dy  = f_X(x)  \\tag{9}\\label{eq:lim2} \\\\\n\\end{align}\n\\] 而\\(\\eqref{eq:lim1}\\)是對的因為可以用 dominated convergence theorem。\n而\\(\\eqref{eq:lim2}\\)仔細一看，他就是 \\(f_X\\) 跟一個高斯核函數 (Gaussian Kernel) 的捲積 (convolution)。而這個高斯核函數的變異數趨近於 \\(0\\)。\n這可以用非常基礎的古典論證，我就簡單寫寫: 我們將這個積分切分成兩個區間，一個是 \\(|x - y| &lt; \\delta\\)，另一個是 \\(|x - y| \\ge \\delta\\)。那個 \\(|x - y| &lt; \\delta\\) 的部分核函數總面積會趨近於1，而另一個部分因為核函數在 \\(|x - y| \\ge \\delta\\) 的地方會趨近於0，所以整個積分就會趨近於 \\(f_X(x)\\) 在 \\(y=x\\) 附近的平均值。若 \\(f_X\\) 在 \\(x\\) 點連續，那麼這個平均值就會趨近於 \\(f_X(x)\\)。若 \\(f_X\\) 在 \\(x\\) 點不連續，那麼這個極限會趨近於 \\(f_X\\) 在 \\(x\\) 點的連續化 (continuous version)。\n\n\n至於 Levy’s Continuity Theorem\n上面的證明解釋了一個分布的 characteristic function 唯一決定了該分布的概率密度函數 (PDF)，這是Levy’s Continuity Theorem 的一個重要部分。更完整的Levy’s Continuity Theorem 除了說明 CF 唯一決定分布外，還說明了如果一列隨機變數的 CF 收斂到某個函數，且該函數是某個分布的 CF，那麼這列隨機變數的分布也會收斂到該分布。\n\n定理 1.1 (Lévy 連續性定理) 設 \\(\\mu, \\mu_n, n \\in \\mathbb{N},\\) 是定義在 \\((\\mathbb{R}^d, \\mathcal{B}(\\mathbb{R}^d))\\) 上的概率測度，其對應的特徵函數分別為 \\(\\chi\\) 和 \\(\\chi_n, n \\in \\mathbb{N}\\)。則以下條件是等價的：\n\n序列 \\((\\mu_n)_{n \\in \\mathbb{N}}\\) 弱收斂於 \\(\\mu\\)。\n\\(\\lim_{n \\to \\infty} \\chi_n(t) = \\chi(t)\\) 對於所有 \\(t \\in \\mathbb{R}^d\\) 成立。\n\n\n\n\n\nInteractive 常態分佈與其傅立葉變換\n這是一個互動式視覺化，展示 Normal Distribution \\(N(\\mu, \\sigma^2)\\) 及其 Fourier Transform。 特別注意 3D 圖中的螺旋結構：當 \\(\\mu \\neq 0\\) 時，頻域會產生旋轉（Phase Shift）。\n\n  Mean (μ): \n  0.0\n  Variance (σ²): \n  1.0"
  },
  {
    "objectID": "posts/normal-distribution/day_6.html",
    "href": "posts/normal-distribution/day_6.html",
    "title": "一天證明一個 Normal Distribution 的性質 Day6：Chi-squared Test",
    "section": "",
    "text": "Chi-squared Test 複習\n假設我們有個 contingency table (列聯表) 如下：\n\n\n\n\nCategory 1\nCategory 2\nCategory 3\nTotal\n\n\n\n\nGroup A\n10\n20\n30\n60\n\n\nGroup B\n10\n15\n15\n40\n\n\nTotal\n20\n35\n45\n100\n\n\n\n我們想知道 Group A 跟 Group B 在這三個 category 上是否有顯著差異 (independence)。我們可以使用 Chi-squared test 來檢驗這個假設。\n步驟一、計算期望值 (Expected Counts)： 根據獨立性的假設，也就是這個 contingency table 是一個 rank 1 矩陣，我們可以計算每個 cell 的期望值：\n\n\n\n\n\n\n\n\n\n\nCategory 1\nCategory 2\nCategory 3\n\n\n\n\nGroup A\n(60*20)/100 = 12\n(60*35)/100 = 21\n(60*45)/100 = 27\n\n\nGroup B\n(40*20)/100 = 8\n(40*35)/100 = 14\n(40*45)/100 = 18\n\n\n\n步驟二、計算 Chi-squared 統計量： \\[\n\\chi^2 = \\sum \\frac{(O - E)^2}{E}\n\\] 其中 \\(O\\) 是觀察值 (Observed Counts)，\\(E\\) 是期望值 (Expected Counts)。 計算如下： \\[\n\\chi^2 = \\frac{(10-12)^2}{12} + \\frac{(20-21)^2}{21} + \\frac{(30-27)^2}{27} + \\frac{(10-8)^2}{8} + \\frac{(15-14)^2}{14} + \\frac{(15-18)^2}{18} \\approx 2.38\n\\]\n步驟三、決定自由度 (Degrees of Freedom)： 自由度計算公式為： \\[\ndf = (r - 1)(c - 1)\n\\] 其中 \\(r\\) 是列數，\\(c\\) 是行數。在這個例子中，\\(r=2\\)，\\(c=3\\)，所以 \\(df = (2-1)(3-1) = 2\\)。\n步驟四、查表或計算 p-value： 我們可以使用 Chi-squared 分布表或計算 p-value 來判斷。我們查表發現，當 \\(\\chi^2 \\approx 2.38\\) 且 \\(df=2\\) 時，p-value 約為 0.3。\n由於 p-value 大於常見的顯著水準 (如 0.05)，我們無法拒絕獨立性的假設，表示 Group A 跟 Group B 在這三個 category 上沒有顯著差異。\nChi-squared Test 是說，當樣本數很大(超過30)，這個統計量會趨近於 Chi-squared 分布 (degrees of freedom = (r-1)(c-1))。本質上他本來就永遠不會真正等於 Chi-squared 分布，因為他是離散的。所以這邊談的是個趨近的概念。\n其實第一次看到這個公式覺得很不舒服，為什麼要這樣算? 更直觀的算法應該是某種 statistic distance，比方說 \\[\n\\sum |p_i - q_i|\n\\] 其中 \\(p_i\\) 是觀察到的比例，\\(q_i\\) 是期望的比例。\n身為數學家想要最優化的靈魂開始作祟，這樣為甚麼是最好的? 我們通常想找一個「最有效力的檢定方法 (most powerful test)」，而實在看不出來這個是。\n\n\nIntuition 解釋版本1\n我們直觀地理解 Chi-squared test，應該這樣解讀公式 \\[\n\\sum \\frac{(O-E)^2}{E} = \\sum \\left(\\frac{O-E}{\\sqrt{E}}\\right)^2\n\\] 還記得如果是 binomial 分佈(或者看成 multinomial 分布的其中一項)，期望值是 \\(np\\)，標準差是 \\(\\sqrt{np(1-p)}\\)，所以若 \\(E\\) 是期望值，那 \\(\\sqrt{E}\\) 大概是標準差 (忽略掉 \\(1-p\\) 部分)。所以我們就是在算 實際值 \\(O\\) 減去 期望值 \\(E\\) 再除以標準差 \\(\\sqrt{E}\\)，這就是標準化 (standardization) z-score的概念。然後我們把每個 category 的標準化結果平方後加總起來，這就是 Chi-squared 統計量。\n這個解釋對於公式來說是最直觀的，但也忽略了很多細節，比方說這忽略的 \\((1-p)\\) 感覺不會趨近於 0，而且為甚麼自由度是 \\((r-1)(c-1)\\)，這些不同z-score之間並沒有獨立，怎麼可以說是Chi-squared?\n\n\nIntution 解釋版本2\n另一個角度，我們證明這其實是個 likelihood ratio test 的近似，而 likelihood ratio test 本身就是最有效力的檢定方法 (Neyman-Pearson lemma)。\n\n\\(H_0\\): Contigency table 是 rank 1 矩陣 (independent)。用 \\(r+c-2\\) 個參數描述。\n\\(H_1\\): Contigency table 是 任意的矩陣。用 \\(rc -1\\) 個參數描述。\n\n我們可以計算在 \\(H_0\\) 跟 \\(H_1\\) 下的 likelihood ratio。\n先算 \\(H_0\\):\n假設我們實驗得到的表是 \\(O_{ij}\\)，\\(N = \\sum_{i,j} O_{ij}\\)。用 rank 1 矩陣去算 likelihood 就是待定 \\(a_i\\) 跟 \\(b_j\\) (滿足 \\(\\sum_i a_i = 1\\) 且 \\(\\sum_j b_j = 1\\))，把 \\(a_i b_j N\\) 當成期望值。\nlikelihood function 是 multinomial distribution: \\[\nL(H_0) := \\mathbb{P}_0(O \\mid a_i, b_j) = \\binom{N}{O_{11}, O_{12}, \\ldots, O_{rc}} \\prod_{i,j} (a_i b_j)^{O_{ij}}\n\\] 這邊我們要選 \\(a_i, b_j\\) 使得 likelihood 最大化。不防忽略常數項，然後取 log: \\[\n\\log L(H_0) = \\sum_{i,j} O_{ij} (\\log a_i + \\log b_j)\n\\] 因為還有約束條件 \\(\\sum_i a_i = 1\\)，\\(\\sum_j b_j = 1\\)，我們用拉格朗日乘數法 (Lagrange multipliers)，引入 \\(\\lambda, \\mu\\)，考慮 \\[\n\\mathcal{L}(a_i, b_j, \\lambda, \\mu) = \\sum_{i,j} O_{ij} (\\log a_i + \\log b_j) + \\lambda (1 - \\sum_i a_i) + \\mu (1 - \\sum_j b_j)\n\\] 分別對 \\(a_i\\), \\(b_j\\) 求導數並設為 0: \\[\n\\begin{align}\n\\frac{\\partial \\mathcal{L}}{\\partial a_i} &= \\sum_j \\frac{O_{ij}}{a_i} - \\lambda = 0 \\implies a_i = \\frac{\\sum_j O_{ij}}{\\lambda} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial b_j} &= \\sum_i \\frac{O_{ij}}{b_j} - \\mu = 0 \\implies b_j = \\frac{\\sum_i O_{ij}}{\\mu}\n\\end{align}\n\\] 利用約束條件，我們可以解出 \\(\\lambda=\\mu=\\sum_{i,j} O_{ij} = N\\)，所以期望值 \\[\na_i b_j N = \\frac{\\sum_j O_{ij} \\sum_i O_{ij}}{N} = E_{ij}\n\\] 這就是我們在 Chi-squared test 裡面計算的期望值。 代回去 likelihood function: \\[\nL(H_0) = \\binom{N}{O_{11}, O_{12}, \\ldots, O_{rc}} \\prod_{i,j} \\left(\\frac{E_{ij}}{N}\\right)^{O_{ij}}\n= \\binom{N}{O_{11}, O_{12}, \\ldots, O_{rc}} \\frac{1}{N^N}\\prod_{i,j} E_{ij}^{O_{ij}}\n\\]\n接著來算 \\(H_1\\):\n過程很類似，但現在有 \\(rc-1\\) 個參數 \\(p_{ij}\\)，且約束條件是 \\(\\sum_{i,j} p_{ij} = 1\\)。直接算 Lagrangian: \\[\n\\mathcal{L}(p_{ij}, \\lambda) = \\sum_{i,j} O_{ij} \\log p_{ij} + \\lambda (1 - \\sum_{i,j} p_{ij})\n\\] 取偏導並設為 0: \\[\n\\frac{\\partial \\mathcal{L}}{\\partial p_{ij}} = \\frac{O_{ij}}{p_{ij}} - \\lambda = 0 \\implies p_{ij} = \\frac{O_{ij}}{\\lambda} = \\frac{O_{ij}}{N}\n\\] 所以期望值就是觀察值本身 \\(O_{ij}\\)，非常符合直覺。代回去 likelihood function: \\[\nL(H_1) = \\binom{N}{O_{11}, O_{12}, \\ldots, O_{rc}} \\prod_{i,j} \\left(\\frac{O_{ij}}{N}\\right)^{O_{ij}}\n= \\binom{N}{O_{11}, O_{12}, \\ldots, O_{rc}} \\frac{1}{N^N}\\prod_{i,j} O_{ij}^{O_{ij}}\n\\]\n最後計算 likelihood ratio: \\[\n\\begin{align}\n\\Lambda &= \\frac{L(H_0)}{L(H_1)} \\\\\n&= \\frac{\\prod_{i,j} E_{ij}^{O_{ij}}}{\\prod_{i,j} O_{ij}^{O_{ij}}} \\\\\n&= \\prod_{i,j} \\left(\\frac{E_{ij}}{O_{ij}}\\right)^{O_{ij}}\n\\end{align}\n\\]\n\n\nInteractive: Finite Chi-Square CDF (Log Scale)\nAdjust \\(n\\) to see how the discrete CDF steps approximate the smooth curve.\n\nviewof n = Inputs.range([2, 50], {value: 5, step: 1, label: \"Sample Size (n)\"})\n\n\n\n\n\n\n\n\n\nprobs = [0.2, 0.5, 0.3] // Change probabilities here\nk = probs.length\nexpected = probs.map(p =&gt; p * n)\n\n// --- 2. MATH LOGIC ---\n\n// Recursive function to get partitions (compositions of integer n)\nfunction getCompositions(target, bins) {\n  if (bins === 1) return [[target]];\n  const results = [];\n  for (let i = 0; i &lt;= target; i++) {\n    const sub = getCompositions(target - i, bins - 1);\n    sub.forEach(s =&gt; results.push([i, ...s]));\n  }\n  return results;\n}\n\n// Generate Raw Data\noutcomes = getCompositions(n, k)\n\nrawData = outcomes.map(counts =&gt; {\n  // Factorial helper\n  const fact = (num) =&gt; {\n    if (num &lt;= 1) return 1;\n    let r = 1; \n    for(let i=2; i&lt;=num; i++) r *= i; \n    return r;\n  }\n  \n  // Multinomial Prob\n  let denom = 1;\n  let probTerm = 1;\n  counts.forEach((c, i) =&gt; {\n    denom *= fact(c);\n    probTerm *= Math.pow(probs[i], c);\n  });\n  const p = (fact(n) / denom) * probTerm;\n  \n  // Chi Sq Statistic\n  let q = 0;\n  counts.forEach((c, i) =&gt; {\n    q += Math.pow(c - expected[i], 2) / expected[i];\n  });\n  \n  return {q: q, p: p};\n})\n\n// --- 3. AGGREGATE & CALCULATE CDF ---\n\ngroupedCDF = {\n  // A. Group by Q (sum probabilities for identical Q statistics)\n  const map = new Map();\n  rawData.forEach(d =&gt; {\n    const key = d.q.toFixed(6); \n    const existing = map.get(key) || 0;\n    map.set(key, existing + d.p);\n  });\n  \n  // B. Sort by Q\n  let sorted = Array.from(map, ([q, p]) =&gt; ({q: +q, p: p})).sort((a,b) =&gt; a.q - b.q);\n  \n  // C. Calculate Cumulative Sum\n  let cumSum = 0;\n  return sorted.map(d =&gt; {\n    cumSum += d.p;\n    // Log scale fix: If Q is exactly 0, bump it to 0.01 so it shows up on log axis\n    const plotQ = d.q === 0 ? 0.01 : d.q;\n    return {\n      realQ: d.q,\n      plotQ: plotQ, \n      cdf: cumSum\n    };\n  });\n}\n\n// --- 4. PLOTTING ---\n\nPlot.plot({\n  title: `CDF for n=${n} (Log Scale)`,\n  grid: true,\n  x: {\n    type: \"log\", \n    label: \"Chi-Square Statistic (Q)\",\n    domain: [0.01, d3.max(groupedCDF, d =&gt; d.realQ) * 1.2] // Ensure plot fits\n  },\n  y: {\n    label: \"Cumulative Probability\", \n    domain: [0, 1.05]\n  },\n  marks: [\n    // The \"Sticks\" (Discrete CDF)\n    Plot.ruleY(groupedCDF, {x: \"plotQ\", y: \"cdf\", stroke: \"#2563eb\", strokeWidth: 2}),\n    Plot.dot(groupedCDF, {x: \"plotQ\", y: \"cdf\", fill: \"#2563eb\", r: 3, title: d =&gt; `Q: ${d.realQ.toFixed(2)}\\nCDF: ${d.cdf.toFixed(4)}`}),\n    \n    // The Continuous Chi-Square CDF Curve (df = 2)\n    // Formula: 1 - exp(-x/2)\n    Plot.line(\n       d3.range(0.01, d3.max(groupedCDF, d =&gt; d.realQ) + 5, 0.1).map(x =&gt; ({x: x, y: 1 - Math.exp(-x/2)})),\n      {x: \"x\", y: \"y\", stroke: \"#dc2626\", strokeWidth: 2}\n    ),\n    \n    // Add a text annotation explaining the log clamp if needed\n    Plot.text([{x: 0.01, y: 0.1, text: \"← Q=0 (Clamped)\"}], {x: \"x\", y: \"y\", textAnchor: \"start\", fontSize: 10, fill: \"gray\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy this works:\n\nviewof n = Inputs.range(...): Creates the HTML slider.\n{ojs} block: This JavaScript code runs in the user’s browser, not on your server.\nReactivity: When the user drags the slider, n updates, the data array recalculates, and Plot.plot re-renders automatically.\n\nNote: I hardcoded the Chi-Square PDF for \\(k=3\\) (df=2) as \\(0.5e^{-x/2}\\) to avoid needing a complex Gamma function library in JavaScript. If you change the number of probabilities (\\(k\\)), you will need to update that formula."
  },
  {
    "objectID": "posts/poker/lesson_1_pot_odds.html",
    "href": "posts/poker/lesson_1_pot_odds.html",
    "title": "Poker Lesson 1 - 池底賠率 (Pot Odds)",
    "section": "",
    "text": "先來談談賭博中最小的一步，要 繼續遊戲 (call) 還是 忍痛放棄 (fold)。\n所謂的 Pot Odds，指的是你為了繼續參與遊戲 (call 或 raise) 所需要投入的金額，和你可能贏得的彩池 (pot) 之間的比例。\n舉例來說，假設彩池中有 100 美元，而你需要投入 20 美元來繼續參與遊戲，那麼你的 Pot Odds 就是 20:100，或者簡化為 1:5。\n那我們該如何判斷是否該繼續參與呢? 假設我們目前評估勝率為 \\(p\\)，那麼我們可以計算出期望值 (Expected Value, EV)： \\[\nEV = p \\times \\text{Pot Size} - (1 - p) \\times \\text{Cost to Call} \\tag{1}\\label{eq:ev-pot-odds}\n\\] 若 \\(EV &gt; 0\\)，表示長期下來這是一個有利可圖的決策，反之則不然。也就是我們希望 \\[\np &gt; \\frac{\\text{Cost to Call}}{\\text{Pot Size} + \\text{Cost to Call}} \\tag{2}\\label{eq:pot-odds-threshold}\n\\]\n在上面例子中，就是 \\(20 / (100 + 20) = 1/6 \\approx 16.67\\%\\)。也就是說，如果我們認為自己的勝率超過 16.67%，那麼繼續參與遊戲是有利可圖的。\n\n\n\nFigure 1: Poker Pot Odds\n\n\n\nOdds vs Probability\n在撲克中，Odds 和 Probability 是兩個相關但不同的概念。 Probability 是指某事件發生的可能性，通常以百分比表示 (例如 25% 的機率)。而 Odds 則是指某事件發生與不發生的比率。\n\n1:3 的 Odds 意味著該事件發生的機率是 1/ (1+3) = 25%。\n3:1 的 Odds 意味著該事件發生的機率是 3/ (3+1) = 75%。\n2:5 的 Odds 意味著該事件發生的機率是 2/ (2+5) ≈ 28.57%。"
  },
  {
    "objectID": "posts/poker/lesson_4_bluff_value_ratio.html",
    "href": "posts/poker/lesson_4_bluff_value_ratio.html",
    "title": "Poker Lesson 4 - 如何詐唬",
    "section": "",
    "text": "在德州撲克中，有三種不確定性來源:\n由於不確定性太多，甚至可能有多名玩家，狀況難以分析。\n讓我們來考慮最簡單的版本之一: 只有兩名玩家 A 跟 B，而且 B 有完全的資訊(B知道自己輸還贏)，但 A 沒有任何資訊。"
  },
  {
    "objectID": "posts/poker/lesson_4_bluff_value_ratio.html#seven-card-stud",
    "href": "posts/poker/lesson_4_bluff_value_ratio.html#seven-card-stud",
    "title": "Poker Lesson 4 - 如何詐唬",
    "section": "Seven-card stud",
    "text": "Seven-card stud\n假設 A 跟 B 玩 Seven-card stud， \\(40-80\\) 元有限注，池底有 \\(P = 655\\) 元。\n現在來到最後一輪下注(已經發了最後一張牌)，前六張都是開的，最後一張只有自己知道。\n\nA 的手牌是: A♠ A♣ K♣ 9♦ 7♦ 6♥ [?]\nB 的手牌是: K♠ 9♠ 8♠ 7♠ 4♦ 2♦ [?]\n\n目前 A 有一對 A，B 有高牌 K。若 B 的最後一張牌沒能組成同花 (flush)，那麼 A 就會贏。若 B 可以組成同花，那麼 A 也不可以組出更強的牌，所以 B 會贏。\n也就是說，B 知道自己有沒有贏，但是 A 完全不知道。\n現在這輪下注由 A 先行，他可以選擇下注 \\(B = 80\\) 元 (Bet size) 或是蓋牌 (fold)。\n從旁觀者的角度: B 拿到同花的機率是 \\(\\frac{8}{40} = \\frac{1}{5}\\)，因為剩下的 40 張牌中，有 8 張黑桃可以讓 B 組成同花。\nA 的策略:\nA 應該先過 (check)。因為 A 沒有任何 B 不知道的資訊。\n這無關 A 的勝率 (B拿到同花的機率)，因為只要 B 用誠實的策略 (若贏就下注，若輸就蓋牌)，A 最多只贏 \\(655\\) 元，但下注的話多了輸 \\(80\\) 元的風險。\nB 的策略:\n現在輪到 B。 B 已經知道自己是否會贏。\n\n如果 B 有同花(必贏)，那麼 B 當然要下注 \\(B = 80\\) 元。\n如果 B 沒有同花(必輸)，B 應該下注嗎? 有那麼一點可能是下注可以騙到 A 蓋牌(fold)，那麼 B 就成功騙贏 \\(655\\) 元。若欺騙失敗的話則多花 \\(80\\) 元。\n\nB 的最佳策略是什麼呢?\n\n引入機率模型\n在真實世界中，我們當然都是進行有限次的比賽，這一輪的決定很可能跟前幾輪有關，也就是心理戰。但我們先不談有限次比賽，我們使用比較好分析的 無限次比賽 (infinite repeated game) 模型，也就是機率模型。根據中央極限定理，獨立地在一樣情況下進行無限次的抽樣，樣本平均值會趨近於期望值 (expected value)。所以我們只要最大化期望值就可以了。\n所以假設有個機率 \\(\\beta\\)，代表 B 在沒有同花的情況下，會以機率 \\(\\beta\\) 詐唬 (下注 \\(B = 80\\) 元)。\n我們就變成問，\\(\\beta\\) 要取多少，才可以使得 B 的期望值最大化?\n怎麼算期望值呢?\nB bet 之後不知道 A 會 fold 還是 call，所以我們也要把 A 也拉入我們的機率模型，假設 A 會以 機率 \\(\\alpha\\) 跟注 (call)。\nB 的期望值計算:\n假設 Bet size  \\(B = 80\\) 元，池底大小 \\(P = 655\\) 元。\n使用全概率公式 (law of total expectation)，我們可以把 B 的期望值拆成以下幾項:\n\\[\n\\begin{align}\n\\mathbb{E}(\\text{Payoff}_B)\n&= \\mathbb{P}[\\text{B 有同花, A call}] \\cdot \\mathbb{E}(\\text{Payoff}_B | \\text{B 有同花, A call}) \\\\\n&\\quad + \\mathbb{P}[\\text{B 有同花, A fold}] \\cdot \\mathbb{E}(\\text{Payoff}_B | \\text{B 有同花, A fold}) \\\\\n&\\quad + \\mathbb{P}[\\text{B 無同花, B bet, A call}] \\cdot \\mathbb{E}(\\text{Payoff}_B | \\text{B 無同花, B bet, A call}) \\\\\n&\\quad + \\mathbb{P}[\\text{B 無同花, B bet, A fold}] \\cdot \\mathbb{E}(\\text{Payoff}_B | \\text{B 無同花, B bet, A fold}) \\\\\n&\\quad + \\mathbb{P}[\\text{B 無同花, B check}] \\cdot \\mathbb{E}(\\text{Payoff}_B | \\text{B 無同花, B check}) \\\\\n&=  \\frac{1}{5} \\cdot \\left[ \\alpha \\cdot (P+B) + (1-\\alpha) \\cdot P \\right ]\n  + \\frac{4}{5} \\cdot \\left[ \\beta \\cdot \\left( \\alpha \\cdot (-B) + (1-\\alpha) \\cdot P \\right) + (1-\\beta) \\cdot 0 \\right] \\\\\n&= \\frac{1}{5} \\cdot \\left[ \\alpha B + P \\right ] + \\frac{4}{5} \\cdot \\beta \\cdot \\left( -\\alpha B + P - \\alpha P \\right)  \\\\\n&= \\frac{1}{5} \\cdot \\left( \\alpha B + P \\right) + \\beta \\left( \\frac{4}{5} \\left( -\\alpha (B+P) + P \\right) \\right)  \\tag{1}\\label{eq:E_for_beta} \\\\\n&= \\frac{1}{5} P \\left( 1 + 4\\beta \\right) + \\alpha \\left( \\frac{B}{5} - \\frac{4}{5} \\beta (B + P) \\right)  \\tag{2}\\label{eq:E_for_alpha}  \\\\\n\\end{align}\n\\]\n上面的推導中用到了 A 選擇 call 或 fold 的機率，跟 B 的狀態是獨立的，機率是相乘，因為 A 沒有任何資訊。\n在真實世界中，雙方是不會知道對方的策略的。所以 A 不知道 B 的 \\(\\beta\\)，B 也不知道 A 的 \\(\\alpha\\)。\n在這樣的情況下，A 要選一個 \\(\\alpha\\) 來最小化 B 的期望值，B 也要選一個 \\(\\beta\\) 來最大化自己的期望值。\n也就是說 B 的最佳 \\(\\beta\\) 是多少，跟 \\(\\alpha\\) 有關；A 的最佳 \\(\\alpha\\) 也跟 \\(\\beta\\) 有關。\n剪刀石頭布\n回想一下，這感覺跟在玩剪刀石頭布很像。我要出甚麼 (使用甚麼策略)，非常依賴於對方會出甚麼 (使用甚麼策略)。\n假設我發現對方很常出 布，那我多出一些 剪刀 就可以贏更多。\n但是對方發現我很常出 剪刀，那他多出一些 石頭 就可以贏我更多。\n如果用各 1/3 的機率玩，那是最安全的，但是也完全佔不到便宜。(比方說對方就算永遠出布，長期而言期望值還是0)。\n而當我發現對方的弱點，想要調整策略去佔便宜時，同時也會暴露出弱點。\n這邊隱隱約約感覺到有兩種「最好 (optimal)」 的概念:\n\n第一種是最好的防守策略，對方就算知道我的策略，也無法剝削我。\n第二種是最好的進攻策略，假設我已知對方策略時，可以最大化剝削對方。\n\n由以上討論，我們發現當剪刀石頭布對手不是出均勻分布時，我們可以進攻，但是無法防守；若要繼續維持最佳防守，我們就無法進攻。\n\n\n納許均衡 (Nash Equilibrium)\n我們一般來說的「最佳策略 (optimal strategy)」是指 最佳防守策略，也就是無法被對方剝削的策略。\n假設用 \\(\\sigma_A\\) 跟 \\(\\sigma_B\\) 分別代表 A 跟 B 的策略 (strategy)，可以用以下 Min-Max 定義出來: \\[\n\\begin{align}\n\\sigma_A^* := \\arg \\max_{\\sigma_A} \\min_{\\sigma_B} \\mathbb{E}(\\text{A's payoff} \\mid \\sigma_A, \\sigma_B)  \\tag{Min-Max Optimal}\\label{eq:define_optimal} \\\\\n\\sigma_B^* := \\arg \\max_{\\sigma_B} \\min_{\\sigma_A} \\mathbb{E}(\\text{B's payoff} \\mid \\sigma_A, \\sigma_B)  \\\\\n\\end{align}\n\\] 值得留意的是，這邊的期望值是跑遍所有可能的隨機性 (包含自己和對方的策略隨機性、以及遊戲本身的隨機性)。我們可以決定我們遇到某種狀況時要以怎樣的機率分布採取某種行動 (action)，但無法決定我們會遇到什麼狀況 (situation)。\n而納許均衡的定義，是使用最佳進攻的概念：\n對於 A 的納許均衡策略 \\(\\sigma_A^*\\)，以及 B 的納許均衡策略 \\(\\sigma_B^*\\)，有： \\[\n\\begin{align}\n\\mathbb{E}(\\text{Payoff}_A | \\sigma_A^*, \\sigma_B^*) &\\ge \\mathbb{E}(\\text{Payoff}_A | \\sigma_A, \\sigma_B^*) \\quad \\forall \\sigma_A  \\tag{Nash Equilibrium}\\label{eq:nash_equilibrium} \\\\  \n\\mathbb{E}(\\text{Payoff}_B | \\sigma_A^*, \\sigma_B^*) &\\ge \\mathbb{E}(\\text{Payoff}_B | \\sigma_A^*, \\sigma_B) \\quad \\forall \\sigma_B \\\\\n\\end{align}\n\\]\n首先先說明幾個重點:\n\n納許均衡是描述一個 strategy pair \\((\\sigma_A^*, \\sigma_B^*)\\)，而不是單一策略。\n納許均衡可能不只有一組，但在有限賽局中引入混合策略 (mixed strategies) 後，一定存在。\n純策略與混合策略：若說 Pure Nash Equilibrium，代表策略是確定性的 (deterministic)；若說 Mixed Nash Equilibrium，代表策略是隨機性的 (stochastic)。\n\n零和賽局 (Zero-Sum Game) 的美好性質:\n\n在零和賽局中，Min-Max 定理 (Minimax Theorem) 告訴我們 納許均衡策略 \\(\\iff\\) 最佳防守策略 (Min-Max Optimal)\n\n這意味著在撲克這種零和遊戲裡，只要我們找到了讓對手「無利可圖」的防守策略，該策略同時也會是對抗高手的最佳策略。且無論這場賽局有多少個均衡點，雙方的期望收益 (Game Value) 都是唯一的。\n雖然我們的賽局不是嚴格的零和賽局，但總和 \\(P\\) 是固定的，所以兩個人都扣掉 \\(P/2\\) 後，即為零和賽局。\n\n\n求詐唬頻率\n理解了賽局的框架，現在我們回到剛剛的例子，來求解 B 的最佳詐唬頻率 \\(\\beta\\)，和 A 的最佳跟注頻率 \\(\\alpha\\)。\nB 的最佳進攻策略:\n假設 A 選擇了一個 \\(\\alpha\\)，輪到 B，這時 B 要選擇一個 \\(\\beta\\) 來最大化自己的期望值。我們使用 \\(\\eqref{eq:E_for_beta}\\)，這是一個關於 \\(\\beta\\) 的線性函數: \\[\ng(\\beta) = C + \\beta \\cdot M\n\\] 其中 \\(M = \\frac{4}{5} \\left( -\\alpha (B+P) + P \\right)\\)。\n\n如果 \\(M &gt; 0\\)，B (想要最大化自己的收益) 就會選擇 \\(\\beta = 1\\) (Always Bluff)。\n如果 \\(M &lt; 0\\)，B 就會選擇 \\(\\beta = 0\\) (Never Bluff)。\n如果 \\(M = 0\\)，B 對於 \\(\\beta\\) 的選擇是無所謂的。\n\n把 \\(M\\) 再換回 \\(\\alpha\\)： \\[\n\\beta = \\beta(\\alpha) =\n\\begin{cases}\n1, & \\text{if } \\alpha &lt; \\frac{P}{P + B} \\\\\n0, & \\text{if } \\alpha &gt; \\frac{P}{P + B} \\\\\n\\text{any value in } [0,1], & \\text{if } \\alpha = \\frac{P}{P + B} \\\\\n\\end{cases}\n\\]\n回想剪刀石頭布的例子，也就是 \\(\\alpha\\) 太小時，B 可以一律詐唬來剝削 A；\\(\\alpha\\) 太大時，B 就永遠不詐唬來避免被剝削。但在 \\(\\alpha = \\frac{P}{P + B}\\) 時。\n直覺上這個「讓 B 怎麼選都無所謂」的點，就是 A 的最佳防守。但我們還是先代回去 \\(\\eqref{eq:E_for_beta}\\) 看看 \\[\n\\max_{\\beta} \\mathbb{E}(\\text{Payoff}_B) = \\frac{1}{5} \\cdot \\left( \\alpha B + P \\right) + \\max \\left(0, \\frac{4}{5} \\left( -\\alpha (B+P) + P \\right)  \\right)\n\\] 這是兩段折線組成的函數，轉折點在 \\(\\alpha = \\frac{P}{P + B}\\)。 \\[\n\\begin{align}\n\\max_{\\beta} \\mathbb{E}(\\text{Payoff}_B)\n&=\n\\begin{cases}\n\\frac{1}{5} \\cdot \\left( \\alpha B + P \\right) + \\frac{4}{5} \\left( -\\alpha (B+P) + P \\right), & \\text{if } \\alpha &lt; \\frac{P}{P + B} \\\\\n\\frac{1}{5} \\cdot \\left( \\alpha B + P \\right), & \\text{if } \\alpha \\ge \\frac{P}{P + B} \\\\\n\\end{cases} \\\\\n&= \\begin{cases}\nP - \\alpha \\frac{3B+4P}{5}, & \\text{if } \\alpha &lt; \\frac{P}{P + B} \\\\\n\\frac{1}{5} \\cdot \\left( \\alpha B + P \\right),  & \\text{if } \\alpha \\ge \\frac{P}{P + B} \\\\  \n\\end{cases} \\\\\n\\end{align}\n\\]\n這個公式的意思是，若 B 針對 A 的策略 \\(\\alpha\\) 做出最佳回應 (也就是 \\(\\beta = \\beta(\\alpha)\\) 會隨著 \\(\\alpha\\) 而改變)，將 B 的 Payoff 期望值看成是 \\(\\alpha\\) 的函數。\n這下輪到 A 選擇。 A 要選一個 \\(\\alpha\\) 來最小化 B 的期望值。\n可以看出，折線的前半段是遞減的，後半段是遞增的，所以最小值會出現在轉折點 \\(\\alpha = \\frac{P}{P + B}\\)。\n而最小值為： \\[\n\\min_{\\alpha} \\max_{\\beta} \\mathbb{E}(\\text{Payoff}_B) = \\frac{1}{5} \\cdot \\left( \\frac{P}{P + B} B + P \\right)\n\\] 這是這個遊戲的 Value。對 B 而言是正的，代表 B 有利可圖。\nA 的最佳進攻策略:\n完全同樣的論述，可以解出 A 的最佳進攻策略。\n來快速看一下，這時就用到 \\(\\eqref{eq:E_for_alpha}\\)，這是一個關於 \\(\\alpha\\) 的線性函數。\n如果 \\(\\beta\\) 太小，A 就會選擇 \\(\\alpha = 1\\) (Always Call)；如果 \\(\\beta\\) 太大，A 就會選擇 \\(\\alpha = 0\\) (Always Fold)。 \\[\n\\alpha = \\alpha(\\beta) =\n\\begin{cases}\n1, & \\text{if } \\beta &lt; \\frac{B}{4(P + B)} \\\\\n0, & \\text{if } \\beta &gt; \\frac{B}{4(P + B)} \\\\\n\\text{any value in } [0,1], & \\text{if } \\beta = \\frac{B}{4(P + B)} \\\\  \n\\end{cases}\n\\] 代回去看看: \\[\n\\min_{\\alpha} \\mathbb{E}(\\text{Payoff}_B) = \\frac{1}{5} P \\left( 1 + 4\\beta \\right) + \\min \\left(0, \\frac{B}{5} - \\frac{4}{5} \\beta (B + P) \\right)\n\\] 這也是兩段折線組成的函數，轉折點在 \\(\\beta = \\frac{B}{4(P + B)}\\)。 \\[\n\\begin{align}\n\\min_{\\alpha} \\mathbb{E}(\\text{Payoff}_B)\n&=\n\\begin{cases}\n\\frac{1}{5} P \\left( 1 + 4\\beta \\right), & \\text{if } \\beta &lt; \\frac{B}{4(P + B)} \\\\\n\\frac{1}{5} P \\left( 1 + 4\\beta \\right) + \\left( \\frac{B}{5} - \\frac{4}{5} \\beta (B + P) \\right), & \\text{if } \\beta \\ge \\frac{B}{4(P + B)} \\\\\n\\end{cases} \\\\\n&= \\begin{cases}\n\\frac{1}{5} P \\left( 1 + 4\\beta \\right), & \\text{if } \\beta &lt; \\frac{B}{4(P + B)} \\\\\n\\frac{1}{5} (P + B) - \\frac{4P}{5} \\beta,  & \\text{if } \\beta \\ge \\frac{B}{4(P + B)} \\\\  \n\\end{cases} \\\\\n\\end{align}\n\\] 現在要找 \\(\\beta\\) 來最大化這個函數。因為前半段是遞增的，後半段是遞減的，所以最大值會出現在轉折點 \\(\\beta = \\frac{B}{4(P + B)}\\)。 \\[\n\\max_{\\beta} \\min_{\\alpha} \\mathbb{E}(\\text{Payoff}_B) = \\frac{1}{5} P \\left( 1 + 4 \\cdot \\frac{B}{4(P + B)} \\right) = \\frac{1}{5} \\cdot \\left( \\frac{P}{P + B} B + P \\right)\n\\] 這結果跟剛剛一樣，代表我們找到了納許均衡點。同時，我們也手動驗證了 Min-Max 定理在這個賽局中成立。\n結論\n由此得出，納許均衡點為: \\[\n\\begin{align}\n\\alpha^* &= \\frac{P}{P + B} \\\\\n\\beta^* &= \\frac{B}{4(P + B)} \\\\\n\\end{align}\n\\]\n這個結論可以細品一下: 如果 \\(\\alpha\\) 固定選在 \\(\\alpha^*\\) 上，B 怎麼選 \\(\\beta\\) 都無所謂；如果 \\(\\beta\\) 固定選在 \\(\\beta^*\\) 上，A 怎麼選 \\(\\alpha\\) 都無所謂。\n但是若有一方偏離這個點，則另一方就可以透過極端策略來進行剝削。但同時也暴露出自己的弱點。\n這個架構跟剪刀石頭布是幾乎一樣的。\n\n\n速算法: 無異原理 (Indifference Principle)\n費了很大一番功夫，我們終於確認納許均衡點。但每次都要列出落落長的全機率公式再微分實在太慢了。\n我們剛才發現一個關鍵性質：「若 A 取 \\(\\alpha = \\frac{P}{P + B}\\) 時，則 B 對於 \\(\\beta\\) 的選擇是無所謂的（期望值相同）。」\n這其實就是微積分中「極值點斜率為零」的賽局版本。既然 B 在均衡點時，對於「詐唬」或「不詐唬」的期望值應該一樣（否則他就會全部倒向其中一邊），我們可以直接針對 B 的決策節點 (Decision Node) 列式。\n怎麼心算 A 的最佳跟注率 (\\(\\alpha\\))?\n我們要讓 B 在「拿到爛牌」時，對於 Check 和 Bluff 感到無差別。 (注意：我們不需要考慮 B 拿到好牌的情況，因為那裡他一定會下注，沒有決策難題)。\n\n若 B 選擇 Check (放棄): 他直接輸掉，收益為 \\(0\\)。\n若 B 選擇 Bluff (下注 \\(B\\)):\n\n有 \\((1-\\alpha)\\) 的機率 A 蓋牌，B 贏得底池 \\(P\\)。\n有 \\(\\alpha\\) 的機率 A 跟注，B 被抓包，輸掉賭注 \\(B\\)。\n\n\n令兩者期望值相等： \\[0 = (1-\\alpha) \\cdot P + \\alpha \\cdot (-B)\\]\n一眼就能看出這就是著名的 Pot Odds 公式變體： \\[\\alpha \\cdot B = (1-\\alpha) \\cdot P \\implies \\alpha = \\frac{P}{P+B}\\]\n這告訴我們一個很反直覺的真理： A 的最佳防守頻率 (\\(\\alpha\\))，完全取決於底池賠率，跟 B 到底有多少機率拿到同花毫無關係！\n\n怎麼心算 B 的最佳詐唬率 (\\(\\beta\\))?\n我們要讓 A 在面對下注時，對於 Call (跟注) 和 Fold (蓋牌) 感到無差別。\n\n若 A 選擇 Fold: 收益為 \\(0\\) (不賠不賺，底池送給 B)。\n若 A 選擇 Call: 這時候 A 贏或輸，取決於 B 到底是真的有牌 (Value Bet) 還是偷雞 (Bluff)。\n\nA 贏 (B 詐唬): 贏得底池 \\(P\\) 加上 B 的下注 \\(B\\)。\nA 輸 (B 有同花): 輸掉跟注金額 \\(B\\)。\n\n\n為了讓 A 無差別 (EV of Call = 0)，A 贏的獲利期望值 必須等於 A 輸的風險期望值。\n\\[\n\\text{Prob(Bluff)} \\times (P + B) = \\text{Prob(Value)} \\times B\n\\]\n移項整理一下，這就是著名的 賠率平衡公式 (bluff-to-value ratio)：\n\\[\n\\frac{\\text{Prob(Bluff)}}{\\text{Prob(Value)}} = \\frac{B}{P + B}\n\\]\n這告訴我們：B 的 詐唬頻率 對比 價值注頻率，必須剛好等於 A 的跟注賠率 (Pot Odds)。\n現在把題目中的機率帶進去： * Prob(Value): B 拿到同花且下注。機率是 \\(\\frac{1}{5} \\times 1\\) (有牌必打)。 * Prob(Bluff): B 沒拿到同花且下注。機率是 \\(\\frac{4}{5} \\times \\beta\\) (沒牌詐唬)。\n代回公式： \\[\n\\frac{\\frac{4}{5}\\beta}{\\frac{1}{5}} = \\frac{B}{P+B}\n\\]\n左邊消掉 5： \\[\n4\\beta = \\frac{B}{P+B}\n\\]\n解得： \\[\n\\beta = \\frac{1}{4} \\cdot \\frac{B}{P+B}\n\\]\n這結果跟我們之前用微積分算出來的一模一樣！\n[Optional] 註記:\n這裡看上去好像真的只要解平衡點就好了，而且 A 的 抓詐唬頻率真的真 B 的反敗為勝機率毫無關係 (在公式上)。\n但我們想像一種情況，假設 B 反敗為勝的機率變得超高 (比方說 4/5)，而且池底相對小，比方說 \\(P = 80\\) 元，\\(B = 80\\) 元。那其實 A 的最佳跟注率並非 \\(\\alpha = \\frac{80}{80+80} = 0.5\\)，而是 0 。這也很符合直覺，因為高機率會輸。這可以用上面代回去論證的方法看出來: 原本是 前半段函數斜率是負的，後半段是正的，但現在兩段都是正的，所以 A 的最佳策略是 \\(\\alpha = 0\\)。\n不過這種情況在實際的撲克中很少見，因為在這種設定下(只有B知道自己是否獲勝)，反敗為勝的機率通常不會太高，也就是池底大部分的牌都可以讓 B 組出好牌。這違反了撲克的常識。"
  }
]