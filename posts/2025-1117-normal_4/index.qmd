---
title: "一天講一個Normal Distribution的性質 Day4"
author: "Tai-Ning Liao"
date: "2025-11-17"
categories: [analysis, statistic, distribution]
format:
    html:
        fig-align: center # 這會讓所有圖片都置中
        fig-cap-location: bottom # 確保圖標在圖片下方
        fig-cap-align: center # 這是控制圖標置中的關鍵
---

今天來講統計學中，參數估計(Parameter Estimation)的一個有趣概念，Sufficient Statistic。

假設我們從一個distribution i.i.d. 取樣了 $n$ 個數據點，
$$
X_1, \ldots, X_n \sim \mathbf{X_\theta}
$$
而這個distribution有個未知的參數 $\theta$，我們原則上是想要估計這個 $\theta$，(例如Maximal Likelihood Estimation就是其中一種方式)。但是在這之前，我們來看看有沒有辦法先把這 $n$ 個數據點「壓縮」成一個更小的統計量 (statistic)，而不損失任何關於 $\theta$ 的資訊。這個概念就叫做 Sufficient Statistic。

比方說最簡單的丟硬幣例子，假設我們有一個未知偏置的硬幣，正面朝上的機率是 $\theta$，我們丟了 $n$ 次硬幣，得到 $X_1, X_2, \ldots, X_n$，每個 $X_i$ 是 $0$ (反面) 或 $1$ (正面)。我們想要估計這個 $\theta$。

來算算看，這 $n$ 個數據點的聯合機率分佈(joint distribution)是：
$$
P(\mathbf{X} = \mathbf{x} \mid \theta) \coloneqq P(X_1=x_1, X_2=x_2, \ldots, X_n=x_n \mid \theta) = \theta^{\sum_{i=1}^n x_i} (1-\theta)^{n - \sum_{i=1}^n x_i}
$$
我們想要用這次的實驗結果 $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ 來估計 $\theta$。但是注意到，這個聯合分佈，跟實驗數據有關的部分只依賴於 $T(\mathbf{x}) = \sum_{i=1}^n x_i$，(以及 $n$ ，但這邊設 $n$ 為固定值)，也就是正面朝上的次數，而不是每一次丟硬幣的具體結果。換句話說，所有這些 $T(\mathbf{x})$ 相同的實驗結果，形成了一個等高線。而在這個等高線上 $x$ 的所有可能的方法數跟 $\theta$ 無關。所以我們有，若 $\mu \coloneqq T(\mathbf{x})$:
$$
\begin{aligned}
P(\mathbf{X} = \mathbf{x} \mid \theta, T(\mathbf{X}) = \mu) 
&= \frac{P(\mathbf{X} = \mathbf{x}, T(\mathbf{X}) = \mu \mid \theta)}{P(T(\mathbf{X}) = \mu \mid \theta)}  \\
&= \frac{P(\mathbf{X} = \mathbf{x} \mid \theta)}{P(T(\mathbf{X}) = \mu \mid \theta)} \\
&= \frac{\theta^{\sum_{i=1}^n x_i} (1-\theta)^{n - \sum_{i=1}^n x_i}}{\sum_{T(\mathbf{x})=\mu}  \theta^{\mu} (1-\theta)^{n - \mu}} \\
&= \frac{\theta^{\mu} (1-\theta)^{n - \mu}}{\sum_{T(\mathbf{x})=\mu}  \theta^{\mu} (1-\theta)^{n - \mu}} \\
&= \frac{1}{\sum_{T(\mathbf{x})=\mu}  1} \\
&= \frac{1}{\binom{n}{\mu}} \\
\end{aligned}
$$

而這跟 $\theta$ 是無關的。這個結論有個很直觀的推導，就是擲 $n$ 次硬幣，給定正面朝上的次數是 $\mu$，則某一種特定的結果出現的機率，就是 $1/\binom{n}{\mu}$。也就是這個簡化的計算偷偷用到了「每一種結果出現機率均等」的條件。

通常記這個結果為
$$
P(\mathbf{X} = \mathbf{x} \mid \theta, T(\mathbf{X}) = \mu) = P(\mathbf{X} = \mathbf{x} \mid T(\mathbf{X}) = \mu)
$$
這些 $P$ 裡面的「等於」記號其實有點冗，我們通常省略，簡記為
$$
P(\mathbf{X} \mid \theta, T(\mathbf{X})) = P(\mathbf{X} \mid T(\mathbf{X}))
\tag{1}\label{eq:suff_stat_def}
$$
我們稱滿足這個條件的統計量 $T(\mathbf{X})$ 為 $\theta$ 的 Sufficient Statistic。

也就是說，conditioning on $T(X)$，$X$ 的分佈不再依賴於 $\theta$。換句話說，給定 $T(X)$ 後，$X$ 不再提供任何關於 $\theta$ 的額外資訊。



### Information Theory 
消息理論是個基於機率論的東西，但他所定義的 mutual information 這個概念完美捕捉了 兩個隨機變數跟獨立性的「差距」。

先定義一個隨機變數的資訊熵(Shannon Entropy)，若是離散: 
$$
H(X) = - \sum_{x} P(X=x) \log P(X=x)
$$
若是連續函數，則改成積分:
$$
H(X) = - \int f_X(x) \log f_X(x) dx
$$
還記得在 [前面文章](/posts/2025-1111-normal_1/index.qmd#sec-max_entropy)，已經證明過常態分佈會最大化熵(給定平均值和變異數的限制下)。

接著，mutual information 定義為:
$$
\begin{aligned}
I(X; Y) 
&= H(X) - H(X|Y)   \\ 
&= H(Y) - H(Y|X)   \\
&= H(X) + H(Y) - H(X, Y)   \\
\end{aligned}
$$
以上這些定義都是等價的。

如果 $X$ 和 $Y$ 是獨立的，那麼 $H(X|Y) = H(X)$，所以 $I(X; Y) = 0$。反過來說，如果 $I(X; Y) = 0$，那麼 $X$ 和 $Y$ 必須是獨立的。所以 mutual information 衡量了兩個隨機變數與獨立性的差距。


### Sufficient Statistic 

還記得我們只關心 likelihood function，就是 $P(\mathbf{X} \mid \theta)$。

如果說 $\theta$ 也是隨機變數，那我們就可以開始討論 $\theta$ 的分布、資訊熵、mutual information 之類的。但這邊的設定 $\theta$ 只是一個待定的參數，若要討論 $\theta$ 的分布，那就是開始對 prior distribution 做假設了，而我們很不喜歡亂假設東西。而這邊很巧妙的是，對於所有prior，我們都必然有的性質: (假設 $\theta$ 有個分布)，假設 $T$ 是個 Sufficient Statistic 滿足 \eqref{eq:suff_stat_def}，那麼我們有:
$$
\begin{aligned}
P(\mathbf{X} \mid \theta, T(\mathbf{X})) 
&= P(\mathbf{X} \mid T(\mathbf{X}))  \qquad \text{(Sufficient Statistic 定義)} \\
\end{aligned}
$$
同乘以 $P(\theta \mid T(\mathbf{X}))$，
$$
P(\mathbf{X}, \theta \mid T(\mathbf{X}))
= P(\mathbf{X} \mid T(\mathbf{X})) P(\theta \mid T(\mathbf{X}))  \tag{2}\label{eq:suff_stat_indep}   
$$

這個式子的解讀就是: 給定 $T(\mathbf{X})$ 後，$\mathbf{X}$ 和 $\theta$ 是條件獨立的 (conditionally independent)。
其實立刻就可以得到Entropy的等式，然後推論mutual information，但我喜歡帶大家走一下這一段。

同時取 $\log$:
$$
\log P(\mathbf{X}, \theta \mid T(\mathbf{X}))
= \log P(\mathbf{X} \mid T(\mathbf{X})) + \log P(\theta \mid T(\mathbf{X}))
$$
將上式對 $\mathbf{X}, \theta$ 取期望值，但固定 $T(\mathbf{X})$:
$$
\begin{aligned}
\int P(\mathbf{X}, \theta \mid T(\mathbf{X})) \log P(\mathbf{X}, \theta \mid T(\mathbf{X})) d\mathbf{X} d\theta
&= \int P(\mathbf{X}, \theta \mid T(\mathbf{X})) \log P(\mathbf{X} \mid T(\mathbf{X})) d\mathbf{X} d\theta \\
&\quad + \int P(\mathbf{X}, \theta \mid T(\mathbf{X})) \log P(\theta \mid T(\mathbf{X})) d\mathbf{X} d\theta \\
&= \int P(\mathbf{X} \mid T(\mathbf{X})) \log P(\mathbf{X} \mid T(\mathbf{X})) d\mathbf{X} \\
&\quad + \int P(\theta \mid T(\mathbf{X})) \log P(\theta \mid T(\mathbf{X})) d\theta \\
\end{aligned}
$$
所以同乘以 $-1$，我們有:
$$
H(\mathbf{X}, \theta \mid T(\mathbf{X}))
= H(\mathbf{X} \mid T(\mathbf{X})) + H(\theta \mid T(\mathbf{X}))  \tag{3}\label{eq:suff_stat_entropy}
$$

這個式子的解讀就是: 給定 $T(\mathbf{X})$ 後，$\mathbf{X}$ 和 $\theta$ 的條件熵是可加的 (additive)。

再進一步，這也是 conditional mutual information 的定義:
$$
\begin{align}
I(\mathbf{X}; \theta \mid T(\mathbf{X})) 
&\coloneqq H(\mathbf{X} \mid T(\mathbf{X})) + H(\theta \mid T(\mathbf{X})) - H(\mathbf{X}, \theta \mid T(\mathbf{X}))   \\
&= 0  \tag{4}\label{eq:suff_stat_mutual_info}
\end{align}
$$

因為這個結構的特殊性 ( $\theta \rightarrow \mathbf{X} \rightarrow T(\mathbf{X})$ 是個馬可夫鏈)，我們本來就會有:
$$
I(\theta; T(\mathbf{X}) \mid \mathbf{X}) = 0  \tag{Markov-chain}\label{eq:markov_chain}
$$
根據

> Mutual information 的 chain rule: 
> $$
> I(X; Y, Z) = I(X; Z) + I(X; Y \mid Z)
> $$

$$
I(\theta; T(\mathbf{X}) \mid \mathbf{X}) + I(\theta; \mathbf{X}) 
= I(\theta; \mathbf{X} \mid T(\mathbf{X})) + I(\theta; T(\mathbf{X}))
$$
因為是 \eqref{eq:markov_chain}，所以左邊第一項是 $0$，因此我們有:
$$
I(\theta; \mathbf{X}) - I(\theta; T(\mathbf{X}))
= I(\theta; \mathbf{X} \mid T(\mathbf{X})) \ge 0 \tag{Data-Processing Inequality}\label{eq:data_processing_ineq}
$$
而等號成立當且僅當 $\mathbf{X}$ 和 $\theta$ 在給定 $T(\mathbf{X})$ 後是條件獨立的，也就是說 $T(\mathbf{X})$ 是個 Sufficient Statistic。

所以如果 $T(\mathbf{X})$ 是個 Sufficient Statistic，那麼
$$
I(\theta; \mathbf{X}) = I(\theta; T(\mathbf{X}))  \tag{5}\label{eq:suff_stat_info_eq}
$$

### Normal Distribution 的 Sufficient Statistic 很簡單，就是樣本均值和樣本變異數:
假設我們有 $X_1, X_2, \ldots, X_n$ 是來自常態分佈 $\mathcal{N}(\mu, \sigma^2)$ 的 i.i.d. 樣本，則 $\theta = (\mu, \sigma^2)$ 的 Sufficient Statistic 就是樣本均值和樣本變異數:
$$
T(\mathbf{X}) = \left( \bar{X}, S^2 \right) = \left( \frac{1}{n} \sum_{i=1}^{n} X_i, \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2 \right)
$$
也就是說，給定樣本均值和樣本變異數後，原本的樣本數據對於 $\mu, \sigma^2$ 不再提供任何額外資訊。
















