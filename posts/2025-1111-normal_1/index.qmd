---
title: "一天講一個Normal Distribution的性質 Day1"
author: "Tai-Ning Liao"
date: "2025-11-11"
categories: [analysis, statistic, distribution]
---

我們將介紹的第一個分佈，是統計學中最著名的——常態分佈（Normal Distribution）。它通常被稱為高斯分佈 (Gaussian distribution)，以紀念偉大的數學家卡爾·弗里德里希·高斯 (Carl Friedrich Gauss)。

儘管高斯在 19 世紀初將其發揚光大（用於分析天文觀測的誤差），但這個鐘形曲線的數學形式最早是由亞伯拉罕·德莫佛 (Abraham de Moivre) 在 1733 年發現的，作為二項分佈的近似。


我們通常看到的常態分佈公式是 $\frac{1}{\sigma\sqrt{2\pi}} e^{-(x-\mu)^2 / (2\sigma^2)}$，看起來有點嚇人。但今天，讓我們從一個更簡潔、更優美的形式開始
$$
f(x) = e^{-\pi x^2}
$$
在此參數下，平均值 $\mu=0$，變異數 $\sigma^2=\frac{1}{2\pi}$。

### 高斯積分

你可能會問：『等等，你是不是漏掉了前面那個複雜的常數（像是 $\frac{1}{\sqrt{2\pi}}$）？』答案是：沒有！ 這個形式的巧妙之處在於，它的歸一化常數恰好是 1。換句話說，這個函數在整個實數線上的積分（即曲線下的總面積）不多不少，剛好等於 1。這使它成為一個合法的機率密度函數 (PDF)。讓我們來證明這一點。

我們要計算的是 $I = \int_{-\infty}^{\infty} e^{-\pi x^2} dx$。這裡有一個絕妙的技巧：我們不直接計算 $I$，而是計算 $I^2$：$$I^2 = \left( \int_{-\infty}^{\infty} e^{-\pi x^2} dx \right) \left( \int_{-\infty}^{\infty} e^{-\pi y^2} dy \right)$$由於 $x$ 和 $y$ 只是虛擬變數 (dummy variables)，我們可以將它們合併為一個二重積分：$$I^2 = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-\pi (x^2 + y^2)} dx dy$$關鍵一步來了：切換到極座標 (Polar Coordinates)！讓我們設 $x = r \cos \theta$ 和 $y = r \sin \theta$。$x^2 + y^2 = r^2$面積元素 $dx dy$ 變為 $r dr d\theta$積分範圍：$r$ 從 $0$ 到 $\infty$，$\theta$ 從 $0$ 到 $2\pi$。我們的 $I^2$ 積分變成了：$$I^2 = \int_{0}^{2\pi} \left( \int_{0}^{\infty} e^{-\pi r^2} \cdot r dr \right) d\theta$$我們先來解決括號內的 $r$ 積分。我們使用 u-替換 (u-substitution)：設 $u = \pi r^2$ $du = 2\pi r dr \implies r dr = \frac{1}{2\pi} du$ 當 $r=0$ 時，$u=0$。當 $r \to \infty$ 時，$u \to \infty$。
$$\int_{r=0}^{\infty} e^{-\pi r^2} r dr = \int_{u=0}^{\infty} e^{-u} \left( \frac{1}{2\pi} du \right) = \frac{1}{2\pi} {(-e^{-u})}\Big|_{0}^{\infty} = \frac{1}{2\pi}
$$
現在，我們把這個結果 $\frac{1}{2\pi}$ 放回 $I^2$ 的 $\theta$ 積分中：
$$I^2 = \int_{0}^{2\pi} \left( \frac{1}{2\pi} \right) d\theta = 1
$$
既然 $I^2 = 1$，且我們的函數 $f(x)$ 明顯恆為正，所以 $I$ 必定為正。因此，我們證明了：
$$\int_{-\infty}^{\infty} e^{-\pi x^2} dx = 1
$$
Q.E.D. (證明完畢)！

### 最大化熵(Entropy) {#sec-max_entropy}

如同離散熵的定義，我們對於連續的機率分布可以定義微分熵 (Differential Entropy): 
$$
H(f) = - \int_{-\infty}^{\infty} f(x) \ln(f(x)) dx
$$

這是資訊理論 (Information Theory) 和統計物理 (Statistical Physics) 中的基石。它解釋了為什麼常態分佈在自然界中如此普遍：在給定平均值和變異數（即平均能量和能量波動）的限制下，常態分佈是系統「最混亂」或「最不確定」的狀態。

不同於使用傳統的變分法，我們將使用一個非常強大（且更簡潔）的工具來證明這一點: KL 散度 (Kullback-Leibler Divergence)。 $D_{KL}(f || g)$ 衡量「機率分佈 $f$ 與 $g$ 的差異程度」。
$$
D_{KL}(f || g) = \int_{-\infty}^{\infty} f(x) \ln\left(\frac{f(x)}{g(x)}\right) dx
$$

由於 $\ln(x)$ 是一個凹函數 (concave function)（其二階導數恆負），根據琴生不等式 (Jensen's inequality)，我們可以得到著名的吉布斯不等式 (Gibbs' Inequality):
$$
-D_{KL}(f || g) = \int_{-\infty}^{\infty} f(x) \ln\left(\frac{g(x)}{f(x)}\right) dx
    \le \ln \left( \int_{-\infty}^{\infty} f(x) \frac{g(x)}{f(x)} dx \right)
    = \ln(1) = 0
$$

將兩邊同乘 $-1$，我們就證明了 $D_{KL}(f || g) \ge 0$ 恆成立。

利用這個強大的不等式，我們將 $g(x)$ 設為我們的目標分佈
$$ 
e^{-\pi x^2} \sim \mathcal{N}(0, \frac{1}{2\pi})
$$
我們已知這是一個合法的 PDF，其均值 $\mu_g = 0$，變異數 $\sigma_g^2 = \frac{1}{2\pi}$。

接著，我們來分析 $\int f(x) \ln(g(x)) dx$ 這一項：
$$
\int_{-\infty}^{\infty} f(x) \ln\left( g(x) \right) dx
= -\pi \int_{-\infty}^{\infty} f(x) x^2 dx
$$

上式即為在 $f(x)$ 分佈下，$x^2$ 的期望值，記為 $\mathbb{E}_f[x^2]$。
現在，我們施加約束：我們要求 $f(x)$ 必須和 $g(x)$ 具有相同的均值與變異數。
也就是說，我們假設 $f(x)$ 也滿足：

 * $\text{Mean}(f) = \mu_f = 0$
 * $\text{Var}(f) = \sigma_f^2 = \frac{1}{2\pi}$ 

最後，我們來串聯這一切：
$$
\begin{align}
H(f) + D_{KL}(f || g)  &= - \int_{-\infty}^{\infty} f(x) \ln(f(x)) dx + D_{KL}(f || g)  \\
    &= - \int_{-\infty}^{\infty} f(x) \ln\left( g(x) \right) dx  \\
    &= \pi \cdot \mathbb{E}_f(x^2)   \qquad \text{(帶入我們剛剛的計算)} \\
    &= \pi \left( \text{Var}(f)+\text{Mean}(f)^2 \right)  \qquad \text{(變異數的定義)} \\
    &= \pi \left( \frac{1}{2\pi} + 0^2 \right)   \qquad \text{(}f(x)\text{的約束)} \\
    &= \frac{1}{2} \\
\end{align}
$$

我們得到了 $H(f) + D_{KL}(f || g) = \frac{1}{2}$。那麼 $g(x)$ 本身的熵 $H(g)$ 是多少呢？我們可以用完全相同的計算（因為 $g(x)$ 也滿足均值為 0、變異數為 $\frac{1}{2\pi}$ 的約束）：
$$
H(g) = \pi \cdot \mathbb{E}_g[x^2] = \pi \left( \text{Var}(g) + \text{Mean}(g)^2 \right) = \pi \left( \frac{1}{2\pi} + 0^2 \right) = \frac{1}{2}
$$
因此，我們證明了：
$$ 
H(f) + D_{KL}(f || g) = H(g) 
$$
所以 $H(g) - H(f) = D_{KL}(f || g) \ge 0 \implies H(g) \ge H(f)$。

Q.E.D. (證明完畢)！這證明了在所有均值為 $0$、變異數為 $\frac{1}{2\pi}$ 的分佈中，常態分佈 $g(x)$ 的熵是最大的。


-----


