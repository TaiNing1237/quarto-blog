---
title: "一天證明一個 Normal Distribution 的性質 Day4：充分統計量與消息理論"
author: "Tai-Ning Liao"
date: "2025-11-17"
categories: [Normal Distribution]
format:
    html:
        fig-align: center # 這會讓所有圖片都置中
        fig-cap-location: bottom # 確保圖標在圖片下方
        fig-cap-align: center # 這是控制圖標置中的關鍵
---

今天來講統計學中「參數估計」(Parameter Estimation) 一個非常優雅的概念：**充分統計量 (Sufficient Statistic)**。

假設我們從一個分佈 i.i.d. 取樣了 $n$ 個數據點：
$$
X_1, \ldots, X_n \sim P_\theta
$$
這個分佈包含一個未知的參數 $\theta$。原則上我們想估計這個 $\theta$（例如使用 Maximum Likelihood Estimation）。但在進行複雜估計之前，我們先思考一個問題：**有沒有辦法把這 $n$ 個數據點「壓縮」成一個更小的統計量 (statistic)，同時完全不損失任何關於 $\theta$ 的資訊？** 這就是 Sufficient Statistic 的核心精神。

### 從丟硬幣開始：直觀的推導

以最簡單的丟硬幣為例。假設硬幣正面朝上的機率是未知的 $\theta$，$(0 < \theta < 1)$。我們丟了 $n$ 次，得到結果 $X_1, \ldots, X_n$，其中 $X_i \in \{0, 1\}$。

這 $n$ 個數據點的聯合機率分佈 (Joint Distribution) 為：
$$
P(\mathbf{X} = \mathbf{x} \mid \theta) 
:= P(X_1=x_1, X_2=x_2, \ldots, X_n=x_n \mid \theta)
= \theta^{\sum x_i} (1-\theta)^{n - \sum x_i}
$$
觀察這個式子，你會發現它只依賴於 $\sum x_i$（也就是正面朝上的總次數），而不在乎 $0$ 和 $1$ 出現的具體順序。

令 $T(\mathbf{x}) = \sum_{i=1}^n x_i$ 為統計量。若我們固定 $T(\mathbf{x}) = \mu$，也就是已知正面出現了 $\mu$ 次，那麼原本實驗結果 $\mathbf{x}$ 的條件機率分佈為何？

$$
\begin{aligned}
P(\mathbf{X} = \mathbf{x} \mid \theta, T(\mathbf{X}) = \mu) 
&= \frac{P(\mathbf{X} = \mathbf{x} \mid \theta)}{P(T(\mathbf{X}) = \mu \mid \theta)} \\
&= \frac{\theta^{\mu} (1-\theta)^{n - \mu}}{\binom{n}{\mu} \theta^{\mu} (1-\theta)^{n - \mu}} \\
&= \frac{1}{\binom{n}{\mu}} \qquad \textbf{(跟 $\theta$ 無關！)} \\
\end{aligned}
$$

**注意到最後的結果完全不包含 $\theta$！**

這意味著：一旦我們知道正面出現了幾次（$T(\mathbf{X})$），具體是「正反正」還是「反正正」出現的機率都是 $1/\binom{n}{\mu}$，這純粹是排列組合問題，與硬幣本身的性質 $\theta$ 無關。

因此，我們定義：如果滿足下式，則 $T(\mathbf{X})$ 是 $\theta$ 的 **Sufficient Statistic**：
$$
P(\mathbf{X} \mid \theta, T(\mathbf{X})) = P(\mathbf{X} \mid T(\mathbf{X})) \tag{1}\label{eq:suff_stat_def}
$$
換句話說，給定 $T(\mathbf{X})$ 後，原始數據 $\mathbf{X}$ 分佈不再依賴於參數 $\theta$。


### 引進 Information Theory 的語言
消息理論完全是基於機率論的東西，但他所定義的各種概念，似乎捕捉到了甚麼「資訊」的本質。可以從今天這個角度來欣賞一下。

先定義一個隨機變數的資訊熵(Shannon Entropy)，若是離散: 
$$
H(X) = - \sum_{x} P(X=x) \log P(X=x)
$$
若是連續函數，則改成積分(稱之為 differential entropy，可能取值為負):
$$
H(X) = - \int f_X(x) \log f_X(x) dx
$$
還記得在 [前面文章](/posts/normal-distribution/day_1.qmd#sec-max_entropy)，已經證明過常態分佈會最大化熵(給定平均值和變異數的限制下)。

接著，mutual information 定義為:
$$
\begin{aligned}
I(X; Y) 
&= H(X) - H(X|Y)   \\ 
&= H(Y) - H(Y|X)   \\
&= H(X) + H(Y) - H(X, Y)   \\
\end{aligned}
$$
以上這些定義都是等價的。

如果 $X$ 和 $Y$ 是獨立的，那麼 $H(X|Y) = H(X)$，所以 $I(X; Y) = 0$。反過來說，如果 $I(X; Y) = 0$，那麼 $X$ 和 $Y$ 必須是獨立的。所以 mutual information 衡量了兩個隨機變數與獨立性的差距。


### Sufficient Statistic 的各種等價定義

還記得我們只關心 likelihood function，就是 $P(\mathbf{X} \mid \theta)$。

如果說 $\theta$ 也是隨機變數，那我們就可以開始討論 $\theta$ 的分布、資訊熵、mutual information 之類的。但這邊的設定 $\theta$ 只是一個待定的參數，若要討論 $\theta$ 的分布，那就是開始對 prior distribution 做假設了。而這邊很巧妙的是，我們可以推導出一些性質，是不論 $\theta$ 的prior是怎樣，都會成立的性質! 所以說

> 消息理論假設「有prior」，但不在乎prior是什麼。

以下假設 $\theta$ 有個 prior 分布，並假設 $T$ 是個 Sufficient Statistic 滿足 \eqref{eq:suff_stat_def}，那麼我們有:
$$
\begin{aligned}
P(\mathbf{X} \mid \theta, T(\mathbf{X})) 
&= P(\mathbf{X} \mid T(\mathbf{X}))  \qquad \text{(Sufficient Statistic 定義)} \\
\end{aligned}
$$
同乘以 $P(\theta \mid T(\mathbf{X}))$，
$$
P(\mathbf{X}, \theta \mid T(\mathbf{X}))
= P(\mathbf{X} \mid T(\mathbf{X})) P(\theta \mid T(\mathbf{X}))  \tag{2}\label{eq:suff_stat_indep}   
$$

這個式子的解讀就是: 給定 $T(\mathbf{X})$ 後，$\mathbf{X}$ 和 $\theta$ 是條件獨立的 (conditionally independent)。
一般的推導很可能會直接跳結論: 

> Conditioning on $T(X)$, $X$ and $\theta$ have mutual information equals $0$。 

但我想帶大家走一下這段推導。

同時取 $\log$:
$$
\log P(\mathbf{X}, \theta \mid T(\mathbf{X}))
= \log P(\mathbf{X} \mid T(\mathbf{X})) + \log P(\theta \mid T(\mathbf{X}))
$$
將上式對 $\mathbf{X}, \theta$ 取條件期望值(限制在 $T(\mathbf{X})$):
$$
\begin{aligned}
\int P(\mathbf{X}, \theta \mid T(\mathbf{X})) \log P(\mathbf{X}, \theta \mid T(\mathbf{X})) d\mathbf{X} d\theta
&= \int P(\mathbf{X}, \theta \mid T(\mathbf{X})) \log P(\mathbf{X} \mid T(\mathbf{X})) d\mathbf{X} d\theta \\
&\quad + \int P(\mathbf{X}, \theta \mid T(\mathbf{X})) \log P(\theta \mid T(\mathbf{X})) d\mathbf{X} d\theta \\
&= \int P(\mathbf{X} \mid T(\mathbf{X})) \log P(\mathbf{X} \mid T(\mathbf{X})) d\mathbf{X} \\
&\quad + \int P(\theta \mid T(\mathbf{X})) \log P(\theta \mid T(\mathbf{X})) d\theta \\
\end{aligned}
$$
所以同乘以 $-1$，我們有:
$$
H(\mathbf{X}, \theta \mid T(\mathbf{X}))
= H(\mathbf{X} \mid T(\mathbf{X})) + H(\theta \mid T(\mathbf{X}))  \tag{3}\label{eq:suff_stat_entropy}
$$

這個式子的解讀就是: 給定 $T(\mathbf{X})$ 後，$\mathbf{X}$ 和 $\theta$ 的條件熵是可加的 (additive)。

再進一步，這也是 conditional mutual information 的定義:
$$
\begin{align}
I(\mathbf{X}; \theta \mid T(\mathbf{X})) 
&\coloneqq H(\mathbf{X} \mid T(\mathbf{X})) + H(\theta \mid T(\mathbf{X})) - H(\mathbf{X}, \theta \mid T(\mathbf{X}))   \\
&= 0  \tag{4}\label{eq:suff_stat_mutual_info}
\end{align}
$$

因為這個結構的特殊性 ( $\theta \rightarrow \mathbf{X} \rightarrow T(\mathbf{X})$ 是個馬可夫鏈)，我們本來就會有:
$$
I(\theta; T(\mathbf{X}) \mid \mathbf{X}) = 0  \tag{Markov-chain}\label{eq:markov_chain}
$$
根據

> Mutual information 的 chain rule: 
> $$
> I(X; Y, Z) = I(X; Z) + I(X; Y \mid Z)
> $$

$$
I(\theta; T(\mathbf{X}) \mid \mathbf{X}) + I(\theta; \mathbf{X}) 
= I(\theta; \mathbf{X} \mid T(\mathbf{X})) + I(\theta; T(\mathbf{X}))
$$
因為是 \eqref{eq:markov_chain}，所以左邊第一項是 $0$，因此我們有:
$$
I(\theta; \mathbf{X}) - I(\theta; T(\mathbf{X}))
= I(\theta; \mathbf{X} \mid T(\mathbf{X})) \ge 0 \tag{Data-Processing Inequality}\label{eq:data_processing_ineq}
$$
而等號成立當且僅當 $\mathbf{X}$ 和 $\theta$ 在給定 $T(\mathbf{X})$ 後是條件獨立的，也就是說 $T(\mathbf{X})$ 是個 Sufficient Statistic。

所以如果 $T(\mathbf{X})$ 是個 Sufficient Statistic，那麼
$$
I(\theta; \mathbf{X}) = I(\theta; T(\mathbf{X}))  \tag{5}\label{eq:suff_stat_info_eq}
$$


**綜合以上**:  \eqref{eq:suff_stat_def}、\eqref{eq:suff_stat_indep}、\eqref{eq:suff_stat_entropy}、\eqref{eq:suff_stat_mutual_info}、\eqref{eq:suff_stat_info_eq}，都是等價的定義。


### Normal Distribution 的 Sufficient Statistic 很簡單，就是樣本均值和樣本變異數:
假設我們有 $X_1, X_2, \ldots, X_n$ 是來自常態分佈 $\mathcal{N}(\mu, \sigma^2)$ 的 i.i.d. 樣本，則 $\theta = (\mu, \sigma^2)$ 的 Sufficient Statistic 就是樣本均值和樣本變異數:
$$
T(\mathbf{X}) = \left( \bar{X}, S^2 \right) = \left( \frac{1}{n} \sum_{i=1}^{n} X_i, \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2 \right)
$$
也就是說，給定樣本均值和樣本變異數後，原本的樣本數據對於 $\mu, \sigma^2$ 不再提供任何額外資訊。

為什麼哩?
因為常態分佈的 likelihood function 只依賴於樣本均值和樣本變異數:
$$
\begin{align}
P(\mathbf{X} \mid \mu, \sigma^2) 
&= \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left( -\frac{(X_i - \mu)^2}{2 \sigma^2} \right)  \\
&= \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^n \exp\left( -\frac{1}{2 \sigma^2} \sum_{i=1}^{n} (X_i - \mu)^2 \right) \\
&= \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^n \exp\left( -\frac{1}{2 \sigma^2} \left[ (n-1) S^2 + n (\bar{X} - \mu)^2 \right] \right) \\ 
\end{align}
$$  

直觀上來說，這個 likelihood function 跟 $X$ 有關的部分只剩下 $\bar{X}$ 和 $S^2$，所以這兩個統計量已經「充分」地捕捉了關於 $\mu$ 和 $\sigma^2$ 的所有資訊。

但根據定義 \eqref{eq:suff_stat_def}，我們需要計算 $P(\mathbf{X} \mid \mu, \sigma^2, T(\mathbf{X}))$，並驗證它不依賴於 $\mu$ 和 $\sigma^2$。

嘿! 但這裡出現了小麻煩，因為是連續的機率密函數，所以不知道怎麼處理 OAO (請參見: 硬核系列-測度論)。

### 直覺是對的: Fisher-Neyman Factorization Theorem
我們直觀上覺得應該可以從 likelihood function 看出來 $T(\mathbf{X})$ 是 Sufficient Statistic。這也就是以下定理:

> **(Fisher-Neyman Factorization Theorem)** 若 $X_1, \ldots, X_n$ 的聯合機率密度函數 (joint pdf/pmf) 為 $f(\mathbf{x} \mid \theta)$。則 $T(\mathbf{X})$ 是 $\theta$ 的 Sufficient Statistic 若且唯若 存在兩個非負函數 $g$ 和 $h$，使得：
$$
f(\mathbf{x} \mid \theta) = g(T(\mathbf{x}), \theta) \cdot h(\mathbf{x})
$$
> 
> * $g(T(\mathbf{x}), \theta)$：這部分包含了 $\theta$，但它只透過 $T(\mathbf{x})$ 來依賴數據 $\mathbf{x}$。
> 
> * $h(\mathbf{x})$：這部分可以依賴所有的數據 $\mathbf{x}$，但絕對不能包含 $\theta$。



















